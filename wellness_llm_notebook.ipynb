{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "id": "XhktVA1mfUHv",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "NgkWb_jzfUHv"
      },
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "8F3xVTTbsoCX",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid LLM Integration Notebook\n",
        "\n",
        "This notebook demonstrates the LLM functionality used in the WellnessGrid app using:\n",
        "- **Med42 (Llama3-Med42-8B)** for medical text generation\n",
        "- **BioBERT** for embeddings\n",
        "- **Flask API with ngrok tunneling**\n",
        "\n",
        "## Setup Instructions\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order to test the functionality\n",
        "3. Enter your ngrok auth token when prompted\n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "97xQThaysoCa",
        "outputId": "490c081b-3a7b-4b41-87e2-7d3e022d8932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-43219183.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Install required packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers flask flask-cors pyngrok --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sentence-transformers scikit-learn --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install sacremoses --quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1073\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     return {\n\u001b[1;32m   1086\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\".py\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     }\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ignore_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pathlib.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mdoes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \"\"\"\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers flask flask-cors pyngrok --quiet\n",
        "!pip install sentence-transformers scikit-learn --quiet\n",
        "!pip install sacremoses --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-q-NKKhtYgf"
      },
      "source": [
        "# Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGQkAFxLszSy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load Med42 - Llama3 based medical model (better than BioGPT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"m42-health/Llama3-Med42-8B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"m42-health/Llama3-Med42-8B\", \n",
        "    torch_dtype=\"auto\", \n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gks2AfwyS0Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Med42 model already uses device_map=\"auto\", so no need to move manually\n",
        "print(f\"üîß Using device: {device}\")\n",
        "print(f\"ü§ñ Med42 model loaded with automatic device mapping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8--dIof6x2Yt"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "biobert = SentenceTransformer('pritamdeka/S-BioBert-snli-multinli-stsb', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtG1OmfszljK"
      },
      "source": [
        "# Document Embeddings & Retriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFaTjEbTwKvT"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "doc_embeddings = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smA7-I0X0UbL"
      },
      "outputs": [],
      "source": [
        "def add_doc(text):\n",
        "    # Embed the incoming doc using BioBERT\n",
        "    embedding = biobert.encode([text], convert_to_tensor=True)[0].cpu().numpy()\n",
        "\n",
        "    # Store in memory\n",
        "    docs.append(text)\n",
        "    doc_embeddings.append(embedding)\n",
        "\n",
        "    print(f\"‚úÖ Added document. Total docs: {len(docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quT8mlBWydld"
      },
      "outputs": [],
      "source": [
        "def find_most_similar_doc(query, top_k=1):\n",
        "    if not doc_embeddings:\n",
        "        return \"‚ùå No documents in memory.\"\n",
        "\n",
        "    # Embed the user query\n",
        "    query_embedding = biobert.encode([query], convert_to_tensor=True)[0].cpu().numpy()\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "\n",
        "    # Get the top-k most similar docs\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"doc\": docs[idx],\n",
        "            \"similarity\": float(similarities[idx])\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3jIU2h_0q6S"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EXzwa7BuraE"
      },
      "outputs": [],
      "source": [
        "!pip install flask flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCJnHIxIwKlL"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "@app.route('/embed-doc', methods=['POST'])\n",
        "def embed_doc():\n",
        "    data = request.get_json()\n",
        "    text = data.get(\"text\", \"\")\n",
        "    if not text:\n",
        "        return jsonify({\"error\": \"Missing 'text' field.\"}), 400\n",
        "\n",
        "    add_doc(text)\n",
        "    return jsonify({\"message\": \"‚úÖ Document embedded.\", \"total_docs\": len(docs)})\n",
        "\n",
        "@app.route('/query', methods=['POST'])\n",
        "def query():\n",
        "    data = request.get_json()\n",
        "    query = data.get(\"query\", \"\")\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "\n",
        "    results = find_most_similar_doc(query, top_k=1)\n",
        "    return jsonify(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scHbG0Zz08UL"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-onMfMsFfUH1"
      },
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è CRITICAL: Make sure you ran cells 2-8 BEFORE running this cell!\n",
        "\n",
        "# Check if required variables exist\n",
        "required_vars = ['tokenizer', 'model', 'biobert', 'docs', 'doc_embeddings', 'add_doc', 'find_most_similar_doc']\n",
        "missing_vars = []\n",
        "for var in required_vars:\n",
        "    if var not in globals():\n",
        "        missing_vars.append(var)\n",
        "\n",
        "if missing_vars:\n",
        "    print(\"‚ùå ERROR: Missing required variables:\", missing_vars)\n",
        "    print(\"üîÑ SOLUTION: Please run cells 2-8 in order first!\")\n",
        "    raise Exception(f\"Missing variables: {missing_vars}. Run cells 2-8 first!\")\n",
        "\n",
        "print(\"‚úÖ All required variables found! Starting Flask app...\")\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Add some sample health documents for testing\n",
        "sample_docs = [\n",
        "    \"Blood pressure is typically measured in millimeters of mercury (mmHg). Normal blood pressure for adults is usually around 120/80 mmHg. High blood pressure (hypertension) is defined as 140/90 mmHg or higher.\",\n",
        "    \"Diabetes is a chronic condition that affects how your body processes blood sugar (glucose). Type 1 diabetes occurs when the pancreas produces little or no insulin. Type 2 diabetes occurs when your body becomes resistant to insulin.\",\n",
        "    \"Regular exercise is essential for maintaining good health. Adults should aim for at least 150 minutes of moderate-intensity aerobic activity per week, plus muscle-strengthening activities twice a week.\",\n",
        "    \"Sleep is crucial for physical and mental health. Adults typically need 7-9 hours of sleep per night. Poor sleep can affect immune function, mood, and cognitive performance.\"\n",
        "]\n",
        "\n",
        "# Add sample documents to our embedding system\n",
        "for doc in sample_docs:\n",
        "    add_doc(doc)\n",
        "\n",
        "def generate_med42_response(query, context=\"\", max_tokens=200):\n",
        "    \"\"\"Generate response using Med42 (Llama3-based medical model)\"\"\"\n",
        "    try:\n",
        "        # Create medical prompt optimized for Llama3 format\n",
        "        if context:\n",
        "            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful medical AI assistant. Use the provided context to answer medical questions accurately and safely.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContext: {context}\\n\\nQuestion: {query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        else:\n",
        "            prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful medical AI assistant. Provide accurate and safe medical information.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{query}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract only the assistant response\n",
        "        if \"<|start_header_id|>assistant<|end_header_id|>\" in generated_text:\n",
        "            answer = generated_text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1].strip()\n",
        "        else:\n",
        "            answer = generated_text[len(prompt):].strip()\n",
        "        \n",
        "        # Clean up any remaining special tokens\n",
        "        answer = answer.replace(\"<|eot_id|>\", \"\").strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return f\"I apologize, but I encountered an error while processing your question about {query}. Please try rephrasing your question.\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"WellnessGrid AI Flask API is running!\"\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    print(\"üì• Received /ask request\")\n",
        "    try:\n",
        "        data = request.json\n",
        "        query = data.get(\"query\", \"\") or data.get(\"question\", \"\")\n",
        "        user_context = data.get(\"userContext\", {})\n",
        "\n",
        "        print(f\"Processing query: {query}\")\n",
        "        print(f\"User context: {user_context}\")\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\n",
        "                \"response\": \"Please provide a question.\",\n",
        "                \"sources\": [],\n",
        "                \"mockMode\": False\n",
        "            }), 400\n",
        "\n",
        "        # Find relevant documents using BioBERT\n",
        "        similar_docs = find_most_similar_doc(query, top_k=3)\n",
        "\n",
        "        # Prepare context from similar documents\n",
        "        context = \"\"\n",
        "        sources = []\n",
        "\n",
        "        if isinstance(similar_docs, list) and similar_docs:\n",
        "            for i, doc_info in enumerate(similar_docs):\n",
        "                if isinstance(doc_info, dict):\n",
        "                    context += f\"Document {i+1}: {doc_info['doc']}\\n\\n\"\n",
        "                    sources.append({\n",
        "                        \"title\": f\"Health Document {i+1}\",\n",
        "                        \"content\": doc_info['doc'][:200] + \"...\",\n",
        "                        \"similarity\": f\"{doc_info['similarity']:.3f}\"\n",
        "                    })\n",
        "\n",
        "        # Add user health conditions to context if available\n",
        "        if user_context.get(\"healthConditions\"):\n",
        "            conditions = \", \".join(user_context[\"healthConditions\"])\n",
        "            context = f\"Patient has the following health conditions: {conditions}\\n\\n{context}\"\n",
        "\n",
        "        # Generate AI response using Med42\n",
        "        ai_response = generate_med42_response(query, context, max_tokens=150)\n",
        "\n",
        "        # Format response for the app\n",
        "        response = {\n",
        "            \"response\": ai_response,\n",
        "            \"sources\": sources,\n",
        "            \"mockMode\": False\n",
        "        }\n",
        "\n",
        "        print(f\"Generated response: {ai_response[:100]}...\")\n",
        "        print(f\"Sources count: {len(sources)}\")\n",
        "\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in /ask endpoint: {e}\")\n",
        "        return jsonify({\n",
        "            \"response\": \"I apologize, but I'm experiencing technical difficulties. Please try again later.\",\n",
        "            \"sources\": [],\n",
        "            \"mockMode\": True,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"models\": {\n",
        "            \"med42\": \"loaded\",\n",
        "            \"biobert\": \"loaded\"\n",
        "        },\n",
        "        \"documents\": len(docs)\n",
        "    })\n",
        "\n",
        "# Start Flask app in a thread\n",
        "def run_app():\n",
        "    app.run(port=5000, debug=False)\n",
        "\n",
        "# Start the Flask app\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Setup ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"üåê Public URL:\", public_url)\n",
        "print(\"ü§ñ Med42 (Llama3-Med42-8B) and BioBERT models loaded and ready!\")\n",
        "print(\"üìö Sample documents embedded:\", len(docs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zumlr_GW16kv"
      },
      "outputs": [],
      "source": [
        "token = input('Enter ngrok auth token: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0JinrBL0_B4"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace with your token from ngrok.com\n",
        "ngrok.set_auth_token(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ19knyE1C8B"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Add some sample health documents for testing\n",
        "sample_docs = [\n",
        "    \"Blood pressure is typically measured in millimeters of mercury (mmHg). Normal blood pressure for adults is usually around 120/80 mmHg. High blood pressure (hypertension) is defined as 140/90 mmHg or higher.\",\n",
        "    \"Diabetes is a chronic condition that affects how your body processes blood sugar (glucose). Type 1 diabetes occurs when the pancreas produces little or no insulin. Type 2 diabetes occurs when your body becomes resistant to insulin.\",\n",
        "    \"Regular exercise is essential for maintaining good health. Adults should aim for at least 150 minutes of moderate-intensity aerobic activity per week, plus muscle-strengthening activities twice a week.\",\n",
        "    \"Sleep is crucial for physical and mental health. Adults typically need 7-9 hours of sleep per night. Poor sleep can affect immune function, mood, and cognitive performance.\"\n",
        "]\n",
        "\n",
        "# Add sample documents to our embedding system\n",
        "for doc in sample_docs:\n",
        "    add_doc(doc)\n",
        "\n",
        "def generate_biogpt_response(query, context=\"\", max_tokens=200):\n",
        "    \"\"\"Generate response using BioGPT model\"\"\"\n",
        "    try:\n",
        "        # Create prompt with context\n",
        "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "\n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_tokens,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the answer part\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return f\"I apologize, but I encountered an error while processing your question about {query}. Please try rephrasing your question.\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"WellnessGrid AI Flask API is running!\"\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    print(\"üì• Received /ask request\")\n",
        "    try:\n",
        "        data = request.json\n",
        "        query = data.get(\"query\", \"\") or data.get(\"question\", \"\")\n",
        "        user_context = data.get(\"userContext\", {})\n",
        "\n",
        "        print(f\"Processing query: {query}\")\n",
        "        print(f\"User context: {user_context}\")\n",
        "\n",
        "        if not query:\n",
        "            return jsonify({\n",
        "                \"response\": \"Please provide a question.\",\n",
        "                \"sources\": [],\n",
        "                \"mockMode\": false\n",
        "            }), 400\n",
        "\n",
        "        # Find relevant documents using BioBERT\n",
        "        similar_docs = find_most_similar_doc(query, top_k=3)\n",
        "\n",
        "        # Prepare context from similar documents\n",
        "        context = \"\"\n",
        "        sources = []\n",
        "\n",
        "        if isinstance(similar_docs, list) and similar_docs:\n",
        "            for i, doc_info in enumerate(similar_docs):\n",
        "                if isinstance(doc_info, dict):\n",
        "                    context += f\"Document {i+1}: {doc_info['doc']}\\n\\n\"\n",
        "                    sources.append({\n",
        "                        \"title\": f\"Health Document {i+1}\",\n",
        "                        \"content\": doc_info['doc'][:200] + \"...\",\n",
        "                        \"similarity\": f\"{doc_info['similarity']:.3f}\"\n",
        "                    })\n",
        "\n",
        "        # Add user health conditions to context if available\n",
        "        if user_context.get(\"healthConditions\"):\n",
        "            conditions = \", \".join(user_context[\"healthConditions\"])\n",
        "            context = f\"Patient has the following health conditions: {conditions}\\n\\n{context}\"\n",
        "\n",
        "        # Generate AI response using BioGPT\n",
        "        ai_response = generate_biogpt_response(query, context, max_tokens=150)\n",
        "\n",
        "        # Format response for the app\n",
        "        response = {\n",
        "            \"response\": ai_response,\n",
        "            \"sources\": sources,\n",
        "            \"mockMode\": false\n",
        "        }\n",
        "\n",
        "        print(f\"Generated response: {ai_response[:100]}...\")\n",
        "        print(f\"Sources count: {len(sources)}\")\n",
        "\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in /ask endpoint: {e}\")\n",
        "        return jsonify({\n",
        "            \"response\": \"I apologize, but I'm experiencing technical difficulties. Please try again later.\",\n",
        "            \"sources\": [],\n",
        "            \"mockMode\": true,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"models\": {\n",
        "            \"biogpt\": \"loaded\",\n",
        "            \"biobert\": \"loaded\"\n",
        "        },\n",
        "        \"documents\": len(docs)\n",
        "    })\n",
        "\n",
        "# Start Flask app in a thread\n",
        "def run_app():\n",
        "    app.run(port=5000, debug=False)\n",
        "\n",
        "# Start the Flask app\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Setup ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"üåê Public URL:\", public_url)\n",
        "print(\"ü§ñ AI Models loaded and ready!\")\n",
        "print(\"üìö Sample documents embedded:\", len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kPiGzdl5GTH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill ngrok\n",
        "ngrok.kill()\n",
        "\n",
        "# Kill the Flask thread by killing the whole Colab process (if needed)\n",
        "os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "print('Killed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0_LHRrAIbcz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
