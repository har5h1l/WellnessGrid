{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "8F3xVTTbsoCX",
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid LLM Integration Notebook\n",
        "\n",
        "This notebook demonstrates the LLM functionality used in the WellnessGrid app using:\n",
        "- **BioGPT-Large** for text generation\n",
        "- **BioBERT** for embeddings\n",
        "- **Hugging Face Inference API**\n",
        "\n",
        "## Setup Instructions\n",
        "1. Run this notebook in Google Colab\n",
        "2. Set your Hugging Face API token in the first cell\n",
        "3. Execute cells in order to test the functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "97xQThaysoCa"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers flask flask-cors pyngrok --quiet\n",
        "!pip install sentence-transformers scikit-learn --quiet\n",
        "!pip install sacremoses --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-q-NKKhtYgf"
      },
      "source": [
        "# Load Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGQkAFxLszSy",
        "outputId": "db913082-a286-4715-83cb-eb9bf9570f30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/BioGPT-Large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8Gks2AfwyS0Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8--dIof6x2Yt"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "biobert = SentenceTransformer('pritamdeka/S-BioBert-snli-multinli-stsb', device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtG1OmfszljK"
      },
      "source": [
        "# Document Embeddings & Retriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uFaTjEbTwKvT"
      },
      "outputs": [],
      "source": [
        "docs = []\n",
        "doc_embeddings = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "smA7-I0X0UbL"
      },
      "outputs": [],
      "source": [
        "def add_doc(text):\n",
        "    # Embed the incoming doc using BioBERT\n",
        "    embedding = biobert.encode([text], convert_to_tensor=True)[0].cpu().numpy()\n",
        "\n",
        "    # Store in memory\n",
        "    docs.append(text)\n",
        "    doc_embeddings.append(embedding)\n",
        "\n",
        "    print(f\"‚úÖ Added document. Total docs: {len(docs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "quT8mlBWydld"
      },
      "outputs": [],
      "source": [
        "def find_most_similar_doc(query, top_k=1):\n",
        "    if not doc_embeddings:\n",
        "        return \"‚ùå No documents in memory.\"\n",
        "\n",
        "    # Embed the user query\n",
        "    query_embedding = biobert.encode([query], convert_to_tensor=True)[0].cpu().numpy()\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "\n",
        "    # Get the top-k most similar docs\n",
        "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
        "\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        results.append({\n",
        "            \"doc\": docs[idx],\n",
        "            \"similarity\": float(similarities[idx])\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3jIU2h_0q6S"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EXzwa7BuraE",
        "outputId": "3cb8d49a-f0ae-4c10-b0b7-65ff7826bd97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (6.0.1)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install flask flask-cors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YCJnHIxIwKlL"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "@app.route('/embed-doc', methods=['POST'])\n",
        "def embed_doc():\n",
        "    data = request.get_json()\n",
        "    text = data.get(\"text\", \"\")\n",
        "    if not text:\n",
        "        return jsonify({\"error\": \"Missing 'text' field.\"}), 400\n",
        "\n",
        "    add_doc(text)\n",
        "    return jsonify({\"message\": \"‚úÖ Document embedded.\", \"total_docs\": len(docs)})\n",
        "\n",
        "@app.route('/query', methods=['POST'])\n",
        "def query():\n",
        "    data = request.get_json()\n",
        "    query = data.get(\"query\", \"\")\n",
        "    if not query:\n",
        "        return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "\n",
        "    results = find_most_similar_doc(query, top_k=1)\n",
        "    return jsonify(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scHbG0Zz08UL",
        "outputId": "633d521f-5082-41e2-887e-15a8c9c212ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è CRITICAL: Make sure you ran cells 2-8 BEFORE running this cell!\n",
        "\n",
        "# Check if required variables exist\n",
        "required_vars = ['tokenizer', 'model', 'biobert', 'docs', 'doc_embeddings', 'add_doc', 'find_most_similar_doc']\n",
        "missing_vars = []\n",
        "for var in required_vars:\n",
        "    if var not in globals():\n",
        "        missing_vars.append(var)\n",
        "\n",
        "if missing_vars:\n",
        "    print(\"‚ùå ERROR: Missing required variables:\", missing_vars)\n",
        "    print(\"üîÑ SOLUTION: Please run cells 2-8 in order first!\")\n",
        "    raise Exception(f\"Missing variables: {missing_vars}. Run cells 2-8 first!\")\n",
        "\n",
        "print(\"‚úÖ All required variables found! Starting Flask app...\")\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Add some sample health documents for testing\n",
        "sample_docs = [\n",
        "    \"Blood pressure is typically measured in millimeters of mercury (mmHg). Normal blood pressure for adults is usually around 120/80 mmHg. High blood pressure (hypertension) is defined as 140/90 mmHg or higher.\",\n",
        "    \"Diabetes is a chronic condition that affects how your body processes blood sugar (glucose). Type 1 diabetes occurs when the pancreas produces little or no insulin. Type 2 diabetes occurs when your body becomes resistant to insulin.\",\n",
        "    \"Regular exercise is essential for maintaining good health. Adults should aim for at least 150 minutes of moderate-intensity aerobic activity per week, plus muscle-strengthening activities twice a week.\",\n",
        "    \"Sleep is crucial for physical and mental health. Adults typically need 7-9 hours of sleep per night. Poor sleep can affect immune function, mood, and cognitive performance.\"\n",
        "]\n",
        "\n",
        "# Add sample documents to our embedding system\n",
        "for doc in sample_docs:\n",
        "    add_doc(doc)\n",
        "\n",
        "def generate_biogpt_response(query, context=\"\", max_tokens=200):\n",
        "    \"\"\"Generate response using BioGPT model\"\"\"\n",
        "    try:\n",
        "        # Create prompt with context\n",
        "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "        \n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_tokens,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the answer part\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "        \n",
        "        return answer\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return f\"I apologize, but I encountered an error while processing your question about {query}. Please try rephrasing your question.\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"WellnessGrid AI Flask API is running!\"\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    print(\"üì• Received /ask request\")\n",
        "    try:\n",
        "        data = request.json\n",
        "        query = data.get(\"query\", \"\") or data.get(\"question\", \"\")\n",
        "        user_context = data.get(\"userContext\", {})\n",
        "        \n",
        "        print(f\"Processing query: {query}\")\n",
        "        print(f\"User context: {user_context}\")\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\n",
        "                \"response\": \"Please provide a question.\",\n",
        "                \"sources\": [],\n",
        "                \"mockMode\": False\n",
        "            }), 400\n",
        "        \n",
        "        # Find relevant documents using BioBERT\n",
        "        similar_docs = find_most_similar_doc(query, top_k=3)\n",
        "        \n",
        "        # Prepare context from similar documents\n",
        "        context = \"\"\n",
        "        sources = []\n",
        "        \n",
        "        if isinstance(similar_docs, list) and similar_docs:\n",
        "            for i, doc_info in enumerate(similar_docs):\n",
        "                if isinstance(doc_info, dict):\n",
        "                    context += f\"Document {i+1}: {doc_info['doc']}\\n\\n\"\n",
        "                    sources.append({\n",
        "                        \"title\": f\"Health Document {i+1}\",\n",
        "                        \"content\": doc_info['doc'][:200] + \"...\",\n",
        "                        \"similarity\": f\"{doc_info['similarity']:.3f}\"\n",
        "                    })\n",
        "        \n",
        "        # Add user health conditions to context if available\n",
        "        if user_context.get(\"healthConditions\"):\n",
        "            conditions = \", \".join(user_context[\"healthConditions\"])\n",
        "            context = f\"Patient has the following health conditions: {conditions}\\n\\n{context}\"\n",
        "        \n",
        "        # Generate AI response using BioGPT\n",
        "        ai_response = generate_biogpt_response(query, context, max_tokens=150)\n",
        "        \n",
        "        # Format response for the app\n",
        "        response = {\n",
        "            \"response\": ai_response,\n",
        "            \"sources\": sources,\n",
        "            \"mockMode\": False\n",
        "        }\n",
        "        \n",
        "        print(f\"Generated response: {ai_response[:100]}...\")\n",
        "        print(f\"Sources count: {len(sources)}\")\n",
        "        \n",
        "        return jsonify(response)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in /ask endpoint: {e}\")\n",
        "        return jsonify({\n",
        "            \"response\": \"I apologize, but I'm experiencing technical difficulties. Please try again later.\",\n",
        "            \"sources\": [],\n",
        "            \"mockMode\": True,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"models\": {\n",
        "            \"biogpt\": \"loaded\",\n",
        "            \"biobert\": \"loaded\"\n",
        "        },\n",
        "        \"documents\": len(docs)\n",
        "    })\n",
        "\n",
        "# Start Flask app in a thread\n",
        "def run_app():\n",
        "    app.run(port=5000, debug=False)\n",
        "\n",
        "# Start the Flask app\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Setup ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"üåê Public URL:\", public_url)\n",
        "print(\"ü§ñ AI Models loaded and ready!\")\n",
        "print(\"üìö Sample documents embedded:\", len(docs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zumlr_GW16kv"
      },
      "outputs": [],
      "source": [
        "token = input('Enter ngrok auth token: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-0JinrBL0_B4"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace with your token from ngrok.com\n",
        "ngrok.set_auth_token(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ19knyE1C8B",
        "outputId": "5176035f-599b-4563-b0ed-7aa4629a27b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåê Public URL: NgrokTunnel: \"https://3afd-34-87-65-143.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Add some sample health documents for testing\n",
        "sample_docs = [\n",
        "    \"Blood pressure is typically measured in millimeters of mercury (mmHg). Normal blood pressure for adults is usually around 120/80 mmHg. High blood pressure (hypertension) is defined as 140/90 mmHg or higher.\",\n",
        "    \"Diabetes is a chronic condition that affects how your body processes blood sugar (glucose). Type 1 diabetes occurs when the pancreas produces little or no insulin. Type 2 diabetes occurs when your body becomes resistant to insulin.\",\n",
        "    \"Regular exercise is essential for maintaining good health. Adults should aim for at least 150 minutes of moderate-intensity aerobic activity per week, plus muscle-strengthening activities twice a week.\",\n",
        "    \"Sleep is crucial for physical and mental health. Adults typically need 7-9 hours of sleep per night. Poor sleep can affect immune function, mood, and cognitive performance.\"\n",
        "]\n",
        "\n",
        "# Add sample documents to our embedding system\n",
        "for doc in sample_docs:\n",
        "    add_doc(doc)\n",
        "\n",
        "def generate_biogpt_response(query, context=\"\", max_tokens=200):\n",
        "    \"\"\"Generate response using BioGPT model\"\"\"\n",
        "    try:\n",
        "        # Create prompt with context\n",
        "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "        \n",
        "        # Tokenize and generate\n",
        "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_length=inputs.shape[1] + max_tokens,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode response\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the answer part\n",
        "        answer = generated_text.split(\"Answer:\")[-1].strip()\n",
        "        \n",
        "        return answer\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return f\"I apologize, but I encountered an error while processing your question about {query}. Please try rephrasing your question.\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"WellnessGrid AI Flask API is running!\"\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask():\n",
        "    print(\"üì• Received /ask request\")\n",
        "    try:\n",
        "        data = request.json\n",
        "        query = data.get(\"query\", \"\") or data.get(\"question\", \"\")\n",
        "        user_context = data.get(\"userContext\", {})\n",
        "        \n",
        "        print(f\"Processing query: {query}\")\n",
        "        print(f\"User context: {user_context}\")\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\n",
        "                \"response\": \"Please provide a question.\",\n",
        "                \"sources\": [],\n",
        "                \"mockMode\": false\n",
        "            }), 400\n",
        "        \n",
        "        # Find relevant documents using BioBERT\n",
        "        similar_docs = find_most_similar_doc(query, top_k=3)\n",
        "        \n",
        "        # Prepare context from similar documents\n",
        "        context = \"\"\n",
        "        sources = []\n",
        "        \n",
        "        if isinstance(similar_docs, list) and similar_docs:\n",
        "            for i, doc_info in enumerate(similar_docs):\n",
        "                if isinstance(doc_info, dict):\n",
        "                    context += f\"Document {i+1}: {doc_info['doc']}\\n\\n\"\n",
        "                    sources.append({\n",
        "                        \"title\": f\"Health Document {i+1}\",\n",
        "                        \"content\": doc_info['doc'][:200] + \"...\",\n",
        "                        \"similarity\": f\"{doc_info['similarity']:.3f}\"\n",
        "                    })\n",
        "        \n",
        "        # Add user health conditions to context if available\n",
        "        if user_context.get(\"healthConditions\"):\n",
        "            conditions = \", \".join(user_context[\"healthConditions\"])\n",
        "            context = f\"Patient has the following health conditions: {conditions}\\n\\n{context}\"\n",
        "        \n",
        "        # Generate AI response using BioGPT\n",
        "        ai_response = generate_biogpt_response(query, context, max_tokens=150)\n",
        "        \n",
        "        # Format response for the app\n",
        "        response = {\n",
        "            \"response\": ai_response,\n",
        "            \"sources\": sources,\n",
        "            \"mockMode\": false\n",
        "        }\n",
        "        \n",
        "        print(f\"Generated response: {ai_response[:100]}...\")\n",
        "        print(f\"Sources count: {len(sources)}\")\n",
        "        \n",
        "        return jsonify(response)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in /ask endpoint: {e}\")\n",
        "        return jsonify({\n",
        "            \"response\": \"I apologize, but I'm experiencing technical difficulties. Please try again later.\",\n",
        "            \"sources\": [],\n",
        "            \"mockMode\": true,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    return jsonify({\n",
        "        \"status\": \"healthy\",\n",
        "        \"models\": {\n",
        "            \"biogpt\": \"loaded\",\n",
        "            \"biobert\": \"loaded\"\n",
        "        },\n",
        "        \"documents\": len(docs)\n",
        "    })\n",
        "\n",
        "# Start Flask app in a thread\n",
        "def run_app():\n",
        "    app.run(port=5000, debug=False)\n",
        "\n",
        "# Start the Flask app\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "# Setup ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(\"üåê Public URL:\", public_url)\n",
        "print(\"ü§ñ AI Models loaded and ready!\")\n",
        "print(\"üìö Sample documents embedded:\", len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kPiGzdl5GTH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import signal\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Kill ngrok\n",
        "ngrok.kill()\n",
        "\n",
        "# Kill the Flask thread by killing the whole Colab process (if needed)\n",
        "os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "print('Killed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0_LHRrAIbcz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
