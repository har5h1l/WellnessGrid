{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid RAG System - MedGemma Edition\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) system for medical questions using:\n",
        "\n",
        "1. ü§ñ **MedGemma-4B** - Google's multimodal medical model specialized for healthcare\n",
        "2. üóÑÔ∏è **Supabase + pgvector** for document retrieval  \n",
        "3. üåê **Flask API with ngrok** for external access\n",
        "4. üîç **Pre-embedded medical documents** from your database\n",
        "\n",
        "## About MedGemma-4B:\n",
        "- **Model**: Google's medgemma-4b-it (4B parameter multimodal medical model)\n",
        "- **Architecture**: Built on Gemma with multimodal capabilities (text + images)\n",
        "- **Specialty**: Medical and healthcare domain knowledge\n",
        "- **Usage**: This notebook uses MedGemma in text-only mode for RAG\n",
        "\n",
        "## Key Differences from Standard LLMs:\n",
        "- **Processor vs Tokenizer**: Uses AutoProcessor for multimodal support\n",
        "- **Model Type**: AutoModelForImageTextToText instead of CausalLM\n",
        "- **Chat Templates**: Requires special formatting via apply_chat_template()\n",
        "- **Input Handling**: Different tensor management and device placement\n",
        "- **Generation**: Specialized parameter handling for multimodal models\n",
        "\n",
        "## Features:\n",
        "- **MedGemma Integration**: Optimized for MedGemma's unique architecture\n",
        "- **Vector Search**: Supabase pgvector with existing embeddings\n",
        "- **Flask API**: Compatible with WellnessGrid frontend\n",
        "- **Google Colab**: GPU-accelerated inference with CUDA debugging\n",
        "- **ngrok**: Public URL for external access\n",
        "- **Error Recovery**: Robust error handling with CPU fallback\n",
        "\n",
        "## Technical Optimizations for MedGemma:\n",
        "- ‚úÖ **CUDA Launch Blocking**: Enhanced error traceability\n",
        "- ‚úÖ **AutoProcessor**: Proper multimodal tokenization\n",
        "- ‚úÖ **Chat Templates**: Correct prompt formatting for MedGemma\n",
        "- ‚úÖ **Device Management**: float16 precision with fallback support\n",
        "- ‚úÖ **Debug Logging**: Comprehensive input/output validation\n",
        "- ‚úÖ **Modular Design**: Easy integration with other pipelines\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order \n",
        "3. Enter your Supabase credentials and ngrok auth token when prompted  \n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n",
        "\n",
        "## Prerequisites:\n",
        "- Supabase database with `medical_documents` and `document_embeddings` tables\n",
        "- Documents embedded using `embed_documents.py` script\n",
        "- RPC function `search_embeddings` deployed in Supabase\n",
        "- **IMPORTANT**: Use `service_role` API key (NOT `anon` key)\n",
        "\n",
        "## üîë How to Get Your Supabase Credentials:\n",
        "1. **Go to your Supabase project dashboard**\n",
        "2. **Click \"Settings\" ‚Üí \"API\"**\n",
        "3. **Copy \"Project URL\"** (starts with `https://`)\n",
        "4. **Copy \"service_role\" key** (NOT the anon key!)\n",
        "   - ‚úÖ `service_role`: Long key starting with `eyJ...` (200+ chars)\n",
        "   - ‚ùå `anon`: Shorter public key (WRONG for this notebook)\n",
        "\n",
        "**‚ö° RAG system using MedGemma-4B and your existing Supabase embeddings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable CUDA launch blocking for better error traceability\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "# Install required packages for Google Colab\n",
        "%pip install transformers torch sentence-transformers --quiet\n",
        "%pip install flask flask-cors pyngrok --quiet\n",
        "%pip install supabase python-dotenv --quiet\n",
        "%pip install sacremoses --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import torch\n",
        "from getpass import getpass\n",
        "\n",
        "# MedGemma-specific imports - Key difference from standard LLMs\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client\n",
        "\n",
        "# Flask API\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup logging with enhanced verbosity for MedGemma debugging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê RAG session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üîß Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "\n",
        "# MedGemma model configuration - Specialized for multimodal medical tasks\n",
        "MEDGEMMA_CONFIG = {\n",
        "    \"name\": \"MedGemma-4B\",\n",
        "    \"path\": \"google/medgemma-4b-it\",\n",
        "    \"description\": \"4B parameter multimodal medical model from Google\",\n",
        "    \"type\": \"multimodal\",\n",
        "    \"architecture\": \"ImageTextToText\",\n",
        "    \"specialty\": \"Medical and healthcare domain\"\n",
        "}\n",
        "\n",
        "print(f\"\\nü§ñ Selected Medical Model: {MEDGEMMA_CONFIG['name']}\")\n",
        "print(f\"üì¶ Model path: {MEDGEMMA_CONFIG['path']}\")\n",
        "print(f\"üîß Architecture: {MEDGEMMA_CONFIG['architecture']}\")\n",
        "print(f\"üè• Specialty: {MEDGEMMA_CONFIG['specialty']}\")\n",
        "print(f\"üí° Note: MedGemma supports both text and image inputs, but this RAG system uses text-only mode\")\n",
        "\n",
        "# Check GPU compatibility for MedGemma\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    compute_capability = torch.cuda.get_device_capability(0)\n",
        "    print(f\"\\nüéØ GPU: {device_name}\")\n",
        "    print(f\"üî¢ Compute capability: {compute_capability}\")\n",
        "    \n",
        "    # Check if GPU supports bfloat16 (for future optimization)\n",
        "    if compute_capability[0] >= 8:  # A100, H100 series\n",
        "        print(\"‚úÖ GPU supports bfloat16 (will use float16 for compatibility)\")\n",
        "    else:  # T4, V100 series\n",
        "        print(\"‚úÖ GPU supports float16 (optimal for this setup)\")\n",
        "else:\n",
        "    print(\"üì± No GPU available - will use CPU with reduced performance\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MedGemma Model Loading - Specialized for Multimodal Medical Tasks\n",
        "print(\"üß† Loading MedGemma-4B for medical text generation...\")\n",
        "print(\"üìñ Loading processor (AutoProcessor for multimodal support)...\")\n",
        "\n",
        "# Check GPU memory before loading\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîç GPU Memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "\n",
        "try:\n",
        "    # Key difference: Use AutoProcessor instead of AutoTokenizer for MedGemma\n",
        "    # This enables proper handling of both text and potential image inputs\n",
        "    print(\"üî§ Loading AutoProcessor for MedGemma (supports multimodal inputs)...\")\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        MEDGEMMA_CONFIG[\"path\"], \n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ AutoProcessor loaded successfully\")\n",
        "    \n",
        "    # Key difference: Use AutoModelForImageTextToText instead of AutoModelForCausalLM\n",
        "    # MedGemma is built for text + image inputs, even when used in text-only mode\n",
        "    print(\"üß† Loading MedGemma multimodal model (this may take a few minutes)...\")\n",
        "    print(\"üí° Note: Using AutoModelForImageTextToText for MedGemma's multimodal architecture\")\n",
        "    \n",
        "    medgemma_model = AutoModelForImageTextToText.from_pretrained(\n",
        "        MEDGEMMA_CONFIG[\"path\"],\n",
        "        torch_dtype=torch.float16,  # Use float16 instead of bfloat16 for better GPU compatibility\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"‚úÖ MedGemma model loaded successfully\")\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"üéØ Model device: {device}\")\n",
        "    \n",
        "    # Check if processor has special tokens (MedGemma might not have traditional pad/eos tokens)\n",
        "    print(\"üîç Checking MedGemma processor tokens...\")\n",
        "    if hasattr(processor, 'tokenizer'):\n",
        "        tokenizer = processor.tokenizer\n",
        "        print(f\"‚úÖ Tokenizer accessible via processor.tokenizer\")\n",
        "        \n",
        "        # Check for special tokens\n",
        "        if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token:\n",
        "            print(f\"üîö EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No EOS token found - this is normal for some multimodal models\")\n",
        "            \n",
        "        if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token:\n",
        "            print(f\"üìù PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No PAD token found - will set if needed during generation\")\n",
        "            # Set pad token if needed (some multimodal models don't have one by default)\n",
        "            if hasattr(tokenizer, 'eos_token') and tokenizer.eos_token:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "                print(f\"‚úÖ Set PAD token to EOS token: {tokenizer.pad_token}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Processor doesn't expose tokenizer directly - will handle during generation\")\n",
        "    \n",
        "    # Store model info for global access\n",
        "    MODEL_INFO = {\n",
        "        \"name\": MEDGEMMA_CONFIG[\"name\"],\n",
        "        \"path\": MEDGEMMA_CONFIG[\"path\"],\n",
        "        \"type\": MEDGEMMA_CONFIG[\"type\"],\n",
        "        \"architecture\": MEDGEMMA_CONFIG[\"architecture\"],\n",
        "        \"loaded\": True\n",
        "    }\n",
        "    \n",
        "    # Check GPU memory after loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üîç GPU Memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "    \n",
        "    print(f\"‚úÖ {MEDGEMMA_CONFIG['name']} loaded and ready on {device}\")\n",
        "    print(f\"üéØ Architecture: {MEDGEMMA_CONFIG['architecture']}\")\n",
        "    print(f\"üí° Ready for text-only medical RAG tasks\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading MedGemma: {str(e)}\")\n",
        "    print(f\"üîç Full error trace:\")\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(\"üîÑ Attempting fallback to CPU loading...\")\n",
        "    try:\n",
        "        # Try CPU loading as fallback\n",
        "        print(\"üì± Loading MedGemma on CPU...\")\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            MEDGEMMA_CONFIG[\"path\"], \n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        medgemma_model = AutoModelForImageTextToText.from_pretrained(\n",
        "            MEDGEMMA_CONFIG[\"path\"],\n",
        "            torch_dtype=torch.float32,  # Use float32 for CPU\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        \n",
        "        device = torch.device(\"cpu\")\n",
        "        MODEL_INFO = {\n",
        "            \"name\": f\"{MEDGEMMA_CONFIG['name']} (CPU)\",\n",
        "            \"path\": MEDGEMMA_CONFIG[\"path\"],\n",
        "            \"type\": MEDGEMMA_CONFIG[\"type\"],\n",
        "            \"architecture\": MEDGEMMA_CONFIG[\"architecture\"],\n",
        "            \"loaded\": True,\n",
        "            \"device\": \"cpu\"\n",
        "        }\n",
        "        \n",
        "        print(\"‚úÖ MedGemma loaded successfully on CPU\")\n",
        "        \n",
        "    except Exception as cpu_error:\n",
        "        print(f\"‚ùå CPU fallback also failed: {str(cpu_error)}\")\n",
        "        print(\"üîÑ Using emergency fallback model...\")\n",
        "        \n",
        "        # Emergency fallback to a simpler model\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "        \n",
        "        fallback_path = \"microsoft/DialoGPT-medium\"\n",
        "        print(f\"üîÑ Loading emergency fallback: {fallback_path}\")\n",
        "        \n",
        "        processor = AutoTokenizer.from_pretrained(fallback_path)\n",
        "        medgemma_model = AutoModelForCausalLM.from_pretrained(\n",
        "            fallback_path,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "        \n",
        "        if processor.pad_token is None:\n",
        "            processor.pad_token = processor.eos_token\n",
        "        \n",
        "        MODEL_INFO = {\n",
        "            \"name\": \"DialoGPT-medium (Emergency Fallback)\",\n",
        "            \"path\": fallback_path,\n",
        "            \"type\": \"causal\",\n",
        "            \"architecture\": \"CausalLM\",\n",
        "            \"loaded\": True,\n",
        "            \"fallback\": True\n",
        "        }\n",
        "        \n",
        "        print(f\"‚úÖ Emergency fallback model loaded\")\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pre-load the embedding model to avoid CUDA conflicts with MedGemma\n",
        "print(\"üîç Pre-loading embedding model to avoid CUDA memory conflicts with MedGemma...\")\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    # Check if there's enough GPU memory for the embedding model alongside MedGemma\n",
        "    if torch.cuda.is_available():\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "        allocated_memory = torch.cuda.memory_allocated()\n",
        "        free_memory = total_memory - allocated_memory\n",
        "        \n",
        "        print(f\"üîç Available GPU memory: {free_memory/1024**3:.2f} GB\")\n",
        "        \n",
        "        # If less than 2GB free, use CPU for embeddings (MedGemma takes priority)\n",
        "        if free_memory < 2 * 1024**3:  # 2GB threshold\n",
        "            print(\"‚ö†Ô∏è Limited GPU memory - using CPU for embedding model (MedGemma takes priority)\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            EMBEDDING_DEVICE = 'cpu'\n",
        "        else:\n",
        "            print(\"‚úÖ Sufficient GPU memory - using GPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "            EMBEDDING_DEVICE = 'cuda'\n",
        "    else:\n",
        "        print(\"üì± No GPU available - using CPU for embedding model\")\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "        EMBEDDING_DEVICE = 'cpu'\n",
        "    \n",
        "    print(f\"‚úÖ Embedding model loaded on {EMBEDDING_DEVICE}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error pre-loading embedding model: {str(e)}\")\n",
        "    print(\"üîÑ Will load embedding model on-demand with CPU fallback\")\n",
        "    embedding_model = None\n",
        "    EMBEDDING_DEVICE = 'cpu'\n",
        "\n",
        "# Setup Supabase connection using Colab secrets\n",
        "print(\"üóÑÔ∏è Setting up Supabase connection using Colab secrets...\")\n",
        "print(\"üìã Required Colab secrets:\")\n",
        "print(\"   1. SUPABASE_URL - Your project URL (e.g., https://abc123.supabase.co)\")\n",
        "print(\"   2. SUPABASE_SERVICE_ROLE_KEY - Your service role key (NOT anon key)\")\n",
        "print(\"   3. NGROK_AUTH_TOKEN - Your ngrok authentication token\")\n",
        "print(\"\")\n",
        "print(\"üîë To set these secrets:\")\n",
        "print(\"   1. Click the üîë key icon in the left sidebar\")\n",
        "print(\"   2. Add the three secrets listed above\")\n",
        "print(\"   3. Re-run this cell\")\n",
        "print(\"\")\n",
        "\n",
        "try:\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    \n",
        "    print(\"‚úÖ Successfully retrieved secrets from Colab\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving secrets: {str(e)}\")\n",
        "    print(\"üîß Make sure you've added the required secrets in Colab:\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "    print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    raise\n",
        "\n",
        "# Validate the inputs\n",
        "if not supabase_url or not supabase_key:\n",
        "    raise ValueError(\"‚ùå Both Supabase URL and Service Role Key are required!\")\n",
        "\n",
        "if not supabase_url.startswith('https://'):\n",
        "    raise ValueError(\"‚ùå Supabase URL should start with 'https://'\")\n",
        "\n",
        "if not supabase_key.startswith('eyJ'):\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys typically start with 'eyJ'\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "    \n",
        "if len(supabase_key) < 100:\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys are typically very long (200+ characters)\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "\n",
        "try:\n",
        "    supabase = create_client(supabase_url, supabase_key)\n",
        "    print(\"‚úÖ Supabase client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Supabase client: {str(e)}\")\n",
        "    print(\"üîß Common issues:\")\n",
        "    print(\"   ‚Ä¢ Wrong API key type (use service_role, not anon)\")\n",
        "    print(\"   ‚Ä¢ Typo in URL or key\")\n",
        "    print(\"   ‚Ä¢ Key might be expired or regenerated\")\n",
        "    raise\n",
        "\n",
        "# Configuration optimized for MedGemma\n",
        "CONFIG = {\n",
        "    \"top_k\": 5,\n",
        "    \"similarity_threshold\": 0.5,\n",
        "    \"max_context_length\": 2000,\n",
        "    \"max_response_length\": 150,\n",
        "    \"medgemma_max_tokens\": 512,  # MedGemma-specific limit\n",
        "    \"use_chat_template\": True    # Enable chat template for MedGemma\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è MedGemma RAG Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")\n",
        "print(f\"   ü§ñ MedGemma max tokens: {CONFIG['medgemma_max_tokens']}\")\n",
        "print(f\"   üí¨ Chat template enabled: {CONFIG['use_chat_template']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supabase document retrieval functions - Same as main notebook but with enhanced error handling\n",
        "def query_supabase_documents(query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Query Supabase for similar documents using vector search\"\"\"\n",
        "    try:\n",
        "        global embedding_model, EMBEDDING_DEVICE\n",
        "        \n",
        "        # Use pre-loaded embedding model or load with fallback\n",
        "        if embedding_model is None:\n",
        "            print(f\"üîç Loading embedding model on-demand for query: {query[:50]}...\")\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                \n",
        "                # Always use CPU for on-demand loading to avoid CUDA errors with MedGemma\n",
        "                embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "                EMBEDDING_DEVICE = 'cpu'\n",
        "                print(f\"‚úÖ Embedding model loaded on CPU (fallback)\")\n",
        "            except Exception as load_error:\n",
        "                print(f\"‚ùå Failed to load embedding model: {str(load_error)}\")\n",
        "                raise\n",
        "        \n",
        "        top_k = top_k or CONFIG['top_k']\n",
        "        \n",
        "        # Generate embedding for the query\n",
        "        print(f\"üß† Generating embedding vector on {EMBEDDING_DEVICE}...\")\n",
        "        query_embedding = embedding_model.encode([query])[0].tolist()\n",
        "        \n",
        "        # Use the correct RPC function from schema.sql: search_embeddings\n",
        "        print(f\"üîç Searching embeddings with threshold {CONFIG['similarity_threshold']}...\")\n",
        "        result = supabase.rpc('search_embeddings', {\n",
        "            'query_embedding': query_embedding,\n",
        "            'match_threshold': CONFIG['similarity_threshold'],\n",
        "            'match_count': top_k\n",
        "        }).execute()\n",
        "        \n",
        "        if result.data:\n",
        "            documents = []\n",
        "            for i, doc in enumerate(result.data):\n",
        "                documents.append({\n",
        "                    'content': doc.get('chunk_content', ''),  # Correct field name from RPC\n",
        "                    'similarity_score': doc.get('similarity', 0.0),\n",
        "                    'metadata': {\n",
        "                        'title': doc.get('title', 'Medical Document'),\n",
        "                        'source': doc.get('source', 'unknown'),\n",
        "                        'topic': doc.get('topic', 'general'),\n",
        "                        'document_type': doc.get('document_type', 'unknown'),\n",
        "                        'document_id': doc.get('document_id', '')\n",
        "                    },\n",
        "                    'rank': i + 1,\n",
        "                    'doc_id': doc.get('document_id', '')\n",
        "                })\n",
        "            \n",
        "            print(f\"üìä Found {len(documents)} similar documents from Supabase\")\n",
        "            return documents\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No similar documents found in Supabase\")\n",
        "            return []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error querying Supabase: {str(e)}\")\n",
        "        # Fallback: try direct table query if RPC function doesn't exist\n",
        "        try:\n",
        "            print(\"üîÑ Trying fallback query method...\")\n",
        "            result = supabase.table('medical_documents').select('*').limit(top_k).execute()\n",
        "            \n",
        "            if result.data:\n",
        "                documents = []\n",
        "                for i, doc in enumerate(result.data[:top_k]):\n",
        "                    documents.append({\n",
        "                        'content': doc.get('content', ''),\n",
        "                        'similarity_score': 0.8,  # Default similarity\n",
        "                        'metadata': {\n",
        "                            'title': doc.get('title', 'Medical Document'),\n",
        "                            'source': doc.get('source', 'unknown'),\n",
        "                            'topic': doc.get('topic', 'general'),\n",
        "                            'document_type': doc.get('document_type', 'unknown'),\n",
        "                            'document_id': doc.get('id', '')\n",
        "                        },\n",
        "                        'rank': i + 1,\n",
        "                        'doc_id': doc.get('id', '')\n",
        "                    })\n",
        "                \n",
        "                print(f\"üìä Fallback: Retrieved {len(documents)} documents from Supabase\")\n",
        "                return documents\n",
        "            \n",
        "        except Exception as fallback_error:\n",
        "            print(f\"‚ùå Fallback query also failed: {str(fallback_error)}\")\n",
        "            return []\n",
        "\n",
        "# Test Supabase connection and RPC functions\n",
        "print(\"üß™ Testing Supabase connection...\")\n",
        "try:\n",
        "    # Test basic connection\n",
        "    test_result = supabase.table('medical_documents').select('count').execute()\n",
        "    doc_count = len(test_result.data) if test_result.data else 0\n",
        "    print(f\"‚úÖ Supabase connected - Found {doc_count} documents in database\")\n",
        "    \n",
        "    # Test RPC function availability\n",
        "    print(\"üß™ Testing RPC functions...\")\n",
        "    try:\n",
        "        stats_result = supabase.rpc('get_document_stats').execute()\n",
        "        if stats_result.data:\n",
        "            print(\"‚úÖ RPC functions working\")\n",
        "            for stat in stats_result.data[:3]:  # Show first 3 document sources\n",
        "                print(f\"   üìÑ {stat['source']}: {stat['count']} documents\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è RPC function exists but returned no data\")\n",
        "    except Exception as rpc_error:\n",
        "        print(f\"‚ö†Ô∏è RPC function test failed: {str(rpc_error)}\")\n",
        "        print(\"   Vector search will use fallback method\")\n",
        "        \n",
        "except Exception as e:\n",
        "    error_str = str(e)\n",
        "    print(f\"‚ö†Ô∏è Supabase connection test failed: {error_str}\")\n",
        "    \n",
        "    # Provide specific guidance based on error type\n",
        "    if '401' in error_str or 'Invalid API key' in error_str:\n",
        "        print(\"üîß AUTHENTICATION ERROR - Invalid API Key:\")\n",
        "        print(\"   ‚ùå You're using the wrong API key!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Go to your Supabase project dashboard\")\n",
        "        print(\"   2. Settings ‚Üí API\")\n",
        "        print(\"   3. Copy the 'service_role' key (NOT anon key)\")\n",
        "        print(\"   4. The service_role key is much longer and starts with 'eyJ'\")\n",
        "        print(\"   5. Re-run Cell 4 with the correct key\")\n",
        "        print(\"\")\n",
        "        print(\"   üîç Key differences:\")\n",
        "        print(\"   ‚Ä¢ anon key: Used for client-side apps (WRONG for this notebook)\")\n",
        "        print(\"   ‚Ä¢ service_role key: Used for server-side/admin access (CORRECT)\")\n",
        "    elif '404' in error_str:\n",
        "        print(\"üîß TABLE NOT FOUND:\")\n",
        "        print(\"   ‚ùå The 'medical_documents' table doesn't exist!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Run the schema.sql in your Supabase SQL editor\")\n",
        "        print(\"   2. Or run the embed_documents.py script to create tables\")\n",
        "    elif 'timeout' in error_str.lower():\n",
        "        print(\"üîß CONNECTION TIMEOUT:\")\n",
        "        print(\"   ‚ùå Can't reach Supabase servers\")\n",
        "        print(\"   üìã Check your internet connection and Supabase URL\")\n",
        "    else:\n",
        "        print(\"üîß GENERAL CONNECTION ERROR:\")\n",
        "        print(\"   üìã Common fixes:\")\n",
        "        print(\"   ‚Ä¢ Double-check your Supabase URL\")\n",
        "        print(\"   ‚Ä¢ Verify you're using service_role key (not anon)\")\n",
        "        print(\"   ‚Ä¢ Check if your project is paused/suspended\")\n",
        "        print(\"   ‚Ä¢ Ensure database tables exist\")\n",
        "    \n",
        "    print(\"\\n   ‚ö†Ô∏è The system will continue but may have limited document retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MedGemma-Specific Response Generation Function\n",
        "def generate_medgemma_response(prompt: str, max_new_tokens: int = 150) -> str:\n",
        "    \"\"\"\n",
        "    Generate medical response using MedGemma-4B with specialized handling\n",
        "    \n",
        "    Key differences from standard LLMs:\n",
        "    1. Uses processor.apply_chat_template() for proper formatting\n",
        "    2. Handles multimodal inputs (text-only mode in this case)\n",
        "    3. Explicit token management and debugging\n",
        "    4. Enhanced error handling with CUDA debugging\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"ü§ñ Generating response using {MODEL_INFO['name']}...\")\n",
        "        \n",
        "        # Check GPU memory before generation\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated()/1024**3\n",
        "            reserved = torch.cuda.memory_reserved()/1024**3\n",
        "            print(f\"üîç GPU Memory before generation: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
        "        \n",
        "        # MedGemma-specific: Format input using chat template\n",
        "        # This is crucial for MedGemma's multimodal architecture\n",
        "        if CONFIG['use_chat_template'] and hasattr(processor, 'apply_chat_template'):\n",
        "            print(\"üí¨ Using chat template formatting for MedGemma...\")\n",
        "            \n",
        "            # Format messages for MedGemma's chat template\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful medical assistant. Provide accurate medical information based on the given context.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\", \n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            print(\"üî§ Applying chat template...\")\n",
        "            try:\n",
        "                # Key difference: Use processor.apply_chat_template() for MedGemma\n",
        "                inputs = processor.apply_chat_template(\n",
        "                    messages, \n",
        "                    add_generation_prompt=True,  # Critical for proper generation\n",
        "                    tokenize=True,\n",
        "                    return_dict=True, \n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "                \n",
        "                # Move to device with float16 precision (avoiding bfloat16 issues)\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {k: v.to(device, dtype=torch.float16 if k == 'input_ids' else v.dtype) \n",
        "                             for k, v in inputs.items()}\n",
        "                \n",
        "                print(f\"‚úÖ Chat template applied successfully\")\n",
        "                input_len = inputs[\"input_ids\"].shape[-1]\n",
        "                \n",
        "                # Debug: Print input_ids for validation\n",
        "                print(f\"üîç Input shape: {inputs['input_ids'].shape}\")\n",
        "                print(f\"üîç Input length: {input_len} tokens\")\n",
        "                print(f\"üîç Input IDs sample: {inputs['input_ids'][0][:10].tolist()}...\")\n",
        "                \n",
        "            except Exception as template_error:\n",
        "                print(f\"‚ö†Ô∏è Chat template failed: {str(template_error)}\")\n",
        "                print(\"üîÑ Falling back to direct tokenization...\")\n",
        "                \n",
        "                # Fallback to direct tokenization\n",
        "                inputs = processor(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "                if torch.cuda.is_available():\n",
        "                    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "                input_len = inputs['input_ids'].shape[1]\n",
        "                \n",
        "        else:\n",
        "            # Fallback: Direct tokenization without chat template\n",
        "            print(\"üî§ Using direct tokenization (fallback mode)...\")\n",
        "            inputs = processor(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        # Generation parameters optimized for MedGemma\n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": min(max_new_tokens, CONFIG['medgemma_max_tokens']),\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        # Explicitly set tokens for MedGemma (critical for proper generation)\n",
        "        if hasattr(processor, 'tokenizer'):\n",
        "            tokenizer = processor.tokenizer\n",
        "            if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:\n",
        "                generation_params[\"eos_token_id\"] = tokenizer.eos_token_id\n",
        "                print(f\"üîö EOS token ID: {tokenizer.eos_token_id}\")\n",
        "            if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
        "                generation_params[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "                print(f\"üìù PAD token ID: {tokenizer.pad_token_id}\")\n",
        "        \n",
        "        print(f\"‚öôÔ∏è Generation params: {generation_params}\")\n",
        "        \n",
        "        # Generate with enhanced error handling and debugging\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                print(\"üîÑ Starting generation...\")\n",
        "                outputs = medgemma_model.generate(\n",
        "                    **inputs,\n",
        "                    **generation_params\n",
        "                )\n",
        "                print(\"‚úÖ Generation completed successfully\")\n",
        "                \n",
        "            except RuntimeError as cuda_error:\n",
        "                print(f\"‚ùå CUDA error during generation: {str(cuda_error)}\")\n",
        "                print(f\"üîç Error details: {traceback.format_exc()}\")\n",
        "                \n",
        "                if \"out of memory\" in str(cuda_error).lower():\n",
        "                    print(\"üßπ GPU out of memory - clearing cache and retrying...\")\n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    \n",
        "                    # Retry with smaller parameters\n",
        "                    generation_params[\"max_new_tokens\"] = min(generation_params[\"max_new_tokens\"], 50)\n",
        "                    print(f\"üîÑ Retrying with reduced tokens: {generation_params['max_new_tokens']}\")\n",
        "                    \n",
        "                    outputs = medgemma_model.generate(\n",
        "                        **inputs,\n",
        "                        **generation_params\n",
        "                    )\n",
        "                    \n",
        "                elif \"cuda\" in str(cuda_error).lower():\n",
        "                    print(\"üîÑ CUDA error - trying CPU fallback...\")\n",
        "                    \n",
        "                    # Move to CPU for generation\n",
        "                    if 'medgemma_model' in globals():\n",
        "                        medgemma_model.cpu()\n",
        "                    inputs = {k: v.cpu() for k, v in inputs.items()}\n",
        "                    \n",
        "                    generation_params[\"max_new_tokens\"] = min(generation_params[\"max_new_tokens\"], 100)\n",
        "                    \n",
        "                    outputs = medgemma_model.generate(\n",
        "                        **inputs,\n",
        "                        **generation_params\n",
        "                    )\n",
        "                    \n",
        "                    print(\"‚úÖ CPU fallback generation completed\")\n",
        "                else:\n",
        "                    raise cuda_error\n",
        "                \n",
        "            except Exception as gen_error:\n",
        "                print(f\"‚ùå General generation error: {str(gen_error)}\")\n",
        "                print(f\"üîç Full traceback: {traceback.format_exc()}\")\n",
        "                raise gen_error\n",
        "        \n",
        "        # Decode response - MedGemma specific handling\n",
        "        print(\"üî§ Decoding response...\")\n",
        "        \n",
        "        try:\n",
        "            # Decode only the newly generated tokens\n",
        "            generated_tokens = outputs[0][input_len:]\n",
        "            \n",
        "            # Debug: Print generated token info\n",
        "            print(f\"üîç Generated tokens shape: {generated_tokens.shape}\")\n",
        "            print(f\"üîç Generated tokens sample: {generated_tokens[:10].tolist()}...\")\n",
        "            \n",
        "            if hasattr(processor, 'decode'):\n",
        "                response = processor.decode(generated_tokens, skip_special_tokens=True)\n",
        "            elif hasattr(processor, 'tokenizer'):\n",
        "                response = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "            else:\n",
        "                # Emergency fallback\n",
        "                response = str(generated_tokens.tolist())\n",
        "                \n",
        "        except Exception as decode_error:\n",
        "            print(f\"‚ö†Ô∏è Decoding error: {str(decode_error)}\")\n",
        "            # Fallback decoding\n",
        "            try:\n",
        "                response = processor.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                # Extract only the response part (remove input)\n",
        "                if prompt in response:\n",
        "                    response = response.split(prompt)[-1]\n",
        "            except:\n",
        "                response = \"Error: Unable to decode response properly\"\n",
        "        \n",
        "        response = response.strip()\n",
        "        \n",
        "        # Clean up MedGemma-specific artifacts\n",
        "        response = response.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
        "        response = response.replace(\"<|im_end|>\", \"\").replace(\"<|im_start|>\", \"\").strip()\n",
        "        \n",
        "        # Basic quality check\n",
        "        if not response or len(response) < 10:\n",
        "            response = \"I understand your question about health. Please consult with a healthcare professional for personalized medical advice.\"\n",
        "        \n",
        "        print(f\"‚úÖ Response generated successfully ({len(response)} characters)\")\n",
        "        print(f\"üìÑ Response preview: {response[:100]}...\")\n",
        "        \n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå MedGemma generation error: {str(e)}\")\n",
        "        print(f\"üîç Full error trace: {traceback.format_exc()}\")\n",
        "        return f\"I apologize, but I encountered an error processing your question with MedGemma. Please try rephrasing your question or consult with a healthcare professional.\"\n",
        "\n",
        "\n",
        "# Modular MedGemma RAG System Class\n",
        "class MedGemmaRAGSystem:\n",
        "    \"\"\"RAG system specifically optimized for MedGemma and medical/wellness queries\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        print(f\"üè• Initializing MedGemma RAG System...\")\n",
        "        print(f\"üéØ Model: {MODEL_INFO['name']}\")\n",
        "        print(f\"üîß Architecture: {MODEL_INFO['architecture']}\")\n",
        "    \n",
        "    def retrieve_context(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Retrieve relevant document chunks from Supabase\"\"\"\n",
        "        retrieved_docs = query_supabase_documents(query, self.config['top_k'])\n",
        "        \n",
        "        context_parts = []\n",
        "        total_chars = 0\n",
        "        \n",
        "        for doc in retrieved_docs:\n",
        "            if total_chars + len(doc['content']) <= self.config['max_context_length']:\n",
        "                context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                total_chars += len(doc['content'])\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'context': context,\n",
        "            'retrieved_documents': retrieved_docs,\n",
        "            'total_documents_found': len(retrieved_docs),\n",
        "            'documents_used': len(retrieved_docs),\n",
        "            'context_length': len(context)\n",
        "        }\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG query: retrieve context and generate response using MedGemma\"\"\"\n",
        "        print(f\"üîç Processing MedGemma RAG query: {question}\")\n",
        "        \n",
        "        context_result = self.retrieve_context(question)\n",
        "        \n",
        "        print(f\"üìä Found {context_result['total_documents_found']} similar documents\")\n",
        "        print(f\"üìÑ Using {context_result['documents_used']} documents for context\")\n",
        "        \n",
        "        print(f\"ü§ñ Generating response using {MODEL_INFO['name']}...\")\n",
        "        # Medical prompt optimized for MedGemma\n",
        "        medical_prompt = f\"\"\"You are a helpful and accurate medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context_result['context']}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (based only on the context, provide helpful medical information):\"\"\"\n",
        "        \n",
        "        generated_response = generate_medgemma_response(medical_prompt, self.config['max_response_length'])\n",
        "        print(\"‚úÖ MedGemma response generated successfully\")\n",
        "        \n",
        "        result = {\n",
        "            'query': question,\n",
        "            'response': generated_response,\n",
        "            'sources': [\n",
        "                {\n",
        "                    'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                    'source': doc['metadata']['source'],\n",
        "                    'topic': doc['metadata']['topic'],\n",
        "                    'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                    'rank': doc['rank'],\n",
        "                    'content_preview': doc['content'][:150] + \"...\"\n",
        "                }\n",
        "                for doc in context_result['retrieved_documents']\n",
        "            ],\n",
        "            'metadata': {\n",
        "                'documentsUsed': context_result['documents_used'],\n",
        "                'totalFound': context_result['total_documents_found'],\n",
        "                'contextLength': context_result['context_length'],\n",
        "                'model': MODEL_INFO['name'],\n",
        "                'model_path': MODEL_INFO['path'],\n",
        "                'architecture': MODEL_INFO['architecture'],\n",
        "                'embeddings': 'Supabase pgvector',\n",
        "                'processingTime': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Initialize the MedGemma RAG system\n",
        "medgemma_rag = MedGemmaRAGSystem(config=CONFIG)\n",
        "print(\"‚úÖ MedGemma RAG system initialized and ready!\")\n",
        "print(\"üè• Specialized for medical queries with multimodal model architecture\")\n",
        "print(\"üí° System uses text-only mode for document-based RAG tasks\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the MedGemma RAG system\n",
        "print(\"üß™ Testing MedGemma RAG system with sample question...\")\n",
        "\n",
        "test_question = \"What are the symptoms of diabetes?\"\n",
        "try:\n",
        "    print(f\"üîç Testing query: {test_question}\")\n",
        "    result = medgemma_rag.query(test_question)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚ùì QUESTION: {result['query']}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nü§ñ MEDGEMMA RESPONSE:\")\n",
        "    print(f\"{result['response']}\")\n",
        "    \n",
        "    print(f\"\\nüìö SOURCES ({result['metadata']['documentsUsed']} documents):\")\n",
        "    if result['sources']:\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"   {i}. {source['title']} - {source['source']}\")\n",
        "            print(f\"      üìä Similarity: {source['similarity']}\")\n",
        "            print(f\"      üìÑ Preview: {source['content_preview']}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è No sources found - this could indicate:\")\n",
        "        print(\"   ‚Ä¢ No documents in database yet\")\n",
        "        print(\"   ‚Ä¢ Similarity threshold too high\")\n",
        "        print(\"   ‚Ä¢ RPC function needs adjustment\")\n",
        "    \n",
        "    print(f\"\\nüìä Metadata:\")\n",
        "    print(f\"   üîß Model: {result['metadata']['model']}\")\n",
        "    print(f\"   üèóÔ∏è Architecture: {result['metadata']['architecture']}\")\n",
        "    print(f\"   üíæ Embeddings: {result['metadata']['embeddings']}\")\n",
        "    print(f\"   üìÑ Documents Used: {result['metadata']['documentsUsed']}\")\n",
        "    print(f\"   üéØ Total Found: {result['metadata']['totalFound']}\")\n",
        "    \n",
        "    print(\"‚úÖ MedGemma RAG system test completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è MedGemma RAG test failed: {str(e)}\")\n",
        "    print(\"   This might be normal if:\")\n",
        "    print(\"   ‚Ä¢ Supabase connection needs adjustment\")\n",
        "    print(\"   ‚Ä¢ No documents have been embedded yet\")\n",
        "    print(\"   ‚Ä¢ RPC function is not deployed\")\n",
        "    print(\"   ‚Ä¢ MedGemma model loading issues\")\n",
        "    print(\"   The Flask server will still start and you can test via the API\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: Flask API code would go here - keeping this cell short due to size constraints\n",
        "# The full Flask API with MedGemma support would include all the endpoints from the main notebook\n",
        "# but adapted for MedGemma's specific requirements\n",
        "\n",
        "print(\"üìù MedGemma Flask API Setup Notes:\")\n",
        "print(\"üîß Key adaptations needed for MedGemma:\")\n",
        "print(\"   ‚Ä¢ Memory management: Prioritize MedGemma GPU memory\")\n",
        "print(\"   ‚Ä¢ Error handling: Enhanced CUDA debugging\")\n",
        "print(\"   ‚Ä¢ Generation: Use generate_medgemma_response function\")\n",
        "print(\"   ‚Ä¢ Chat templates: Proper formatting for multimodal model\")\n",
        "print(\"   ‚Ä¢ Device management: CPU fallback for embeddings\")\n",
        "print(\"\")\n",
        "print(\"üåê To complete setup:\")\n",
        "print(\"   1. Add Flask endpoints (similar to main notebook)\")\n",
        "print(\"   2. Implement ngrok tunneling\")\n",
        "print(\"   3. Start server with enhanced MedGemma support\")\n",
        "print(\"   4. Test all endpoints with MedGemma-specific handling\")\n",
        "print(\"\")\n",
        "print(\"‚úÖ MedGemma RAG system core components ready!\")\n",
        "print(\"üè• Specialized for medical queries with multimodal architecture\")\n",
        "print(\"üîß All MedGemma-specific optimizations implemented\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Flask API Setup with MedGemma Support and Chat History\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Store chat sessions in memory (in production, use Redis or database)\n",
        "chat_sessions = {}\n",
        "\n",
        "@app.route('/embed', methods=['POST'])\n",
        "def generate_embedding():\n",
        "    \"\"\"Generate embeddings for text with enhanced error handling and GPU memory management\"\"\"\n",
        "    try:\n",
        "        global embedding_model, EMBEDDING_DEVICE\n",
        "        \n",
        "        data = request.get_json()\n",
        "        text = data.get(\"text\", \"\")\n",
        "        \n",
        "        if not text:\n",
        "            return jsonify({\"error\": \"Missing 'text' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"üîç Generating embedding for text: {text[:100]}...\")\n",
        "        \n",
        "        # Use pre-loaded embedding model or load with CPU fallback\n",
        "        if embedding_model is None:\n",
        "            logger.info(\"üì• Loading embedding model on-demand with CPU fallback...\")\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                \n",
        "                # Clear GPU cache first to free up memory for MedGemma\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    logger.info(\"üßπ GPU cache cleared\")\n",
        "                \n",
        "                # Always use CPU for Flask requests to avoid CUDA conflicts with MedGemma\n",
        "                embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "                EMBEDDING_DEVICE = 'cpu'\n",
        "                logger.info(\"‚úÖ Embedding model loaded on CPU (safer for Flask with MedGemma)\")\n",
        "                \n",
        "            except Exception as load_error:\n",
        "                logger.error(f\"‚ùå Failed to load embedding model: {str(load_error)}\")\n",
        "                return jsonify({\"error\": f\"Failed to load embedding model: {str(load_error)}\"}), 500\n",
        "        \n",
        "        # Generate embedding\n",
        "        logger.info(f\"üß† Generating embedding on {EMBEDDING_DEVICE}...\")\n",
        "        embedding = embedding_model.encode([text])[0].tolist()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Generated embedding with {len(embedding)} dimensions\")\n",
        "        \n",
        "        return jsonify({\n",
        "            \"embedding\": embedding,\n",
        "            \"dimensions\": len(embedding),\n",
        "            \"model\": f\"PubMedBERT ({EMBEDDING_DEVICE})\",\n",
        "            \"device\": EMBEDDING_DEVICE\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in embed endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return jsonify({\"error\": f\"Embedding generation failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_text():\n",
        "    \"\"\"Enhanced generate endpoint with MedGemma support and chat history\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get(\"query\", \"\")\n",
        "        context = data.get(\"context\", \"\")\n",
        "        history = data.get(\"history\", [])  # Chat history support\n",
        "        max_tokens = data.get(\"max_tokens\", 200)\n",
        "        temperature = data.get(\"temperature\", 0.7)\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"üî¨ Generating MedGemma response for query: {query[:100]}...\")\n",
        "        logger.info(f\"üìö Context length: {len(context)} characters\")\n",
        "        logger.info(f\"üí¨ Chat history: {len(history)} messages\")\n",
        "        \n",
        "        # Create enhanced prompt with chat history for MedGemma\n",
        "        if history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in history[-3:]])  # Last 3 exchanges\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {query}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Generate response using MedGemma-specific function\n",
        "        response = generate_medgemma_response(prompt, max_tokens)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"answer\": response,\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"model_path\": MODEL_INFO['path'],\n",
        "            \"architecture\": MODEL_INFO['architecture'],\n",
        "            \"context_used\": len(context) > 0,\n",
        "            \"history_used\": len(history) > 0\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in generate endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"MedGemma text generation failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/query', methods=['POST'])\n",
        "def query_docs():\n",
        "    \"\"\"Enhanced query endpoint with better error handling\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get(\"query\", \"\")\n",
        "        top_k = data.get(\"top_k\", CONFIG['top_k'])\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "\n",
        "        results = query_supabase_documents(query, top_k=top_k)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"documents\": results,\n",
        "            \"total_found\": len(results),\n",
        "            \"query\": query\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in query endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"Document query failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask_rag():\n",
        "    \"\"\"Enhanced RAG endpoint with MedGemma and chat history support\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        question = data.get(\"question\", \"\")\n",
        "        session_id = data.get(\"session_id\", \"default\")\n",
        "        history = data.get(\"history\", [])\n",
        "        \n",
        "        if not question:\n",
        "            return jsonify({\"error\": \"Missing 'question' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"ü§ñ Processing MedGemma RAG query: {question[:100]}...\")\n",
        "        logger.info(f\"üìù Session ID: {session_id}\")\n",
        "        \n",
        "        # Use session-specific history if no history provided\n",
        "        if not history and session_id in chat_sessions:\n",
        "            history = chat_sessions[session_id]\n",
        "        \n",
        "        # Get response using MedGemma RAG system\n",
        "        result = medgemma_rag.query(question)\n",
        "        \n",
        "        # Update chat history\n",
        "        new_message = {\"question\": question, \"answer\": result['response']}\n",
        "        if session_id not in chat_sessions:\n",
        "            chat_sessions[session_id] = []\n",
        "        chat_sessions[session_id].append(new_message)\n",
        "        \n",
        "        # Keep only last 10 messages to prevent memory issues\n",
        "        if len(chat_sessions[session_id]) > 10:\n",
        "            chat_sessions[session_id] = chat_sessions[session_id][-10:]\n",
        "        \n",
        "        return jsonify({\n",
        "            \"response\": result['response'],\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"title\": source['title'], \n",
        "                    \"content\": source['content_preview'],\n",
        "                    \"similarity\": float(source['similarity'])\n",
        "                }\n",
        "                for source in result['sources']\n",
        "            ],\n",
        "            \"chat_history\": chat_sessions[session_id],\n",
        "            \"session_id\": session_id,\n",
        "            \"mockMode\": False,\n",
        "            \"metadata\": result['metadata']\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in ask endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return jsonify({\n",
        "            \"response\": f\"I apologize, but I encountered an error processing your question with MedGemma: {str(e)}\",\n",
        "            \"sources\": [],\n",
        "            \"chat_history\": [],\n",
        "            \"mockMode\": True,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/chat/history/<session_id>', methods=['GET'])\n",
        "def get_chat_history(session_id):\n",
        "    \"\"\"Get chat history for a session\"\"\"\n",
        "    try:\n",
        "        history = chat_sessions.get(session_id, [])\n",
        "        return jsonify({\n",
        "            \"session_id\": session_id,\n",
        "            \"history\": history,\n",
        "            \"message_count\": len(history)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/chat/clear/<session_id>', methods=['POST'])\n",
        "def clear_chat_history(session_id):\n",
        "    \"\"\"Clear chat history for a session\"\"\"\n",
        "    try:\n",
        "        if session_id in chat_sessions:\n",
        "            del chat_sessions[session_id]\n",
        "        \n",
        "        return jsonify({\n",
        "            \"session_id\": session_id,\n",
        "            \"cleared\": True\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Enhanced health check endpoint for MedGemma\"\"\"\n",
        "    try:\n",
        "        # Test Supabase connection\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        \n",
        "        # Test embeddings\n",
        "        embed_result = supabase.table('document_embeddings').select('count').execute()\n",
        "        embed_count = len(embed_result.data) if embed_result.data else 0\n",
        "        \n",
        "        # Get GPU memory info if available\n",
        "        gpu_info = \"No GPU available\"\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                device = torch.cuda.current_device()\n",
        "                allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "                reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "                total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "                gpu_info = {\n",
        "                    \"allocated\": f\"{allocated:.2f} GB\",\n",
        "                    \"reserved\": f\"{reserved:.2f} GB\", \n",
        "                    \"total\": f\"{total:.2f} GB\",\n",
        "                    \"percentage_used\": f\"{(allocated/total)*100:.1f}%\"\n",
        "                }\n",
        "            except:\n",
        "                gpu_info = \"GPU info unavailable\"\n",
        "        \n",
        "        return jsonify({\n",
        "            \"status\": \"healthy\",\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"model_path\": MODEL_INFO['path'],\n",
        "            \"architecture\": MODEL_INFO['architecture'],\n",
        "            \"embedding_device\": EMBEDDING_DEVICE if 'EMBEDDING_DEVICE' in globals() else 'unknown',\n",
        "            \"database\": \"Supabase + pgvector\",\n",
        "            \"documents_in_db\": doc_count,\n",
        "            \"embeddings_in_db\": embed_count,\n",
        "            \"rag_system\": \"MedGemma-enhanced\",\n",
        "            \"chat_support\": True,\n",
        "            \"active_sessions\": len(chat_sessions),\n",
        "            \"gpu_memory\": gpu_info\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"status\": \"partial\",\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"architecture\": MODEL_INFO.get('architecture', 'unknown'),\n",
        "            \"embedding_device\": EMBEDDING_DEVICE if 'EMBEDDING_DEVICE' in globals() else 'unknown',\n",
        "            \"database\": \"Supabase (connection issues)\",\n",
        "            \"rag_system\": \"MedGemma-enhanced\",\n",
        "            \"chat_support\": True,\n",
        "            \"warning\": str(e)\n",
        "        })\n",
        "\n",
        "@app.route('/status', methods=['GET'])\n",
        "def status():\n",
        "    \"\"\"Detailed status endpoint for MedGemma system\"\"\"\n",
        "    try:\n",
        "        return jsonify({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"models\": {\n",
        "                \"medgemma_model\": \"loaded\" if 'medgemma_model' in globals() else \"not_loaded\",\n",
        "                \"processor\": \"loaded\" if 'processor' in globals() else \"not_loaded\",\n",
        "                \"pubmedbert\": \"available\"\n",
        "            },\n",
        "            \"database\": {\n",
        "                \"connected\": True,\n",
        "                \"url\": supabase_url[:30] + \"...\" if supabase_url else \"not_set\"\n",
        "            },\n",
        "            \"config\": CONFIG,\n",
        "            \"memory\": {\n",
        "                \"active_sessions\": len(chat_sessions),\n",
        "                \"session_ids\": list(chat_sessions.keys())\n",
        "            }\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "print(\"üåê Enhanced Flask API endpoints configured for MedGemma:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings with MedGemma-safe memory management\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} + chat history\")\n",
        "print(\"  ‚úÖ POST /ask - Enhanced RAG endpoint with MedGemma\")\n",
        "print(\"  ‚úÖ GET /health - Enhanced health check with MedGemma status\")\n",
        "print(\"  ‚úÖ POST /query - Enhanced document query\")\n",
        "print(\"  ‚úÖ GET /chat/history/<session_id> - Get chat history\")\n",
        "print(\"  ‚úÖ POST /chat/clear/<session_id> - Clear chat history\")\n",
        "print(\"  ‚úÖ GET /status - Detailed MedGemma system status\")\n",
        "print(f\"‚úÖ Enhanced Flask server ready with {MODEL_INFO['name']} and chat history support!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
