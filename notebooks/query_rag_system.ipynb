{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid RAG System - Multi-Model Edition\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) system for medical questions using:\n",
        "\n",
        "1. ü§ñ **Selectable Medical Models** for advanced medical text generation\n",
        "2. üóÑÔ∏è **Supabase + pgvector** for document retrieval  \n",
        "3. üåê **Flask API with ngrok** for external access\n",
        "4. üîç **Pre-embedded medical documents** from your database\n",
        "\n",
        "## Available Medical Models:\n",
        "- **OpenBioLLM-8B**: 8B parameter medical LLM optimized for biomedical tasks\n",
        "- **Med42-v2-8B**: 8B parameter medical model from M42 Health\n",
        "- **MedGemma-4B**: 4B parameter medical model based on Gemma architecture\n",
        "\n",
        "## Features:\n",
        "- **Model Selection**: Choose from multiple specialized medical LLMs\n",
        "- **Vector Search**: Supabase pgvector with existing embeddings\n",
        "- **Flask API**: Compatible with WellnessGrid frontend\n",
        "- **Google Colab**: GPU-accelerated inference\n",
        "- **ngrok**: Public URL for external access\n",
        "- **Fallback Support**: Automatic fallback to stable models if loading fails\n",
        "\n",
        "## Recent Updates (Multi-Model Support):\n",
        "- ‚úÖ **Model Selection**: Interactive choice between 3 medical models\n",
        "- ‚úÖ **Dynamic Loading**: Only loads the selected model to save memory\n",
        "- ‚úÖ **Fallback Support**: Automatic fallback if preferred model fails\n",
        "- ‚úÖ **Enhanced Error Handling**: Better validation and error messages\n",
        "- ‚úÖ **Updated API**: All endpoints reflect the selected model\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order and select your preferred medical model when prompted\n",
        "3. Enter your Supabase credentials and ngrok auth token when prompted  \n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n",
        "\n",
        "## Prerequisites:\n",
        "- Supabase database with `medical_documents` and `document_embeddings` tables\n",
        "- Documents embedded using `embed_documents.py` script\n",
        "- RPC function `search_embeddings` deployed in Supabase\n",
        "- **IMPORTANT**: Use `service_role` API key (NOT `anon` key)\n",
        "\n",
        "## üîë How to Get Your Supabase Credentials:\n",
        "1. **Go to your Supabase project dashboard**\n",
        "2. **Click \"Settings\" ‚Üí \"API\"**\n",
        "3. **Copy \"Project URL\"** (starts with `https://`)\n",
        "4. **Copy \"service_role\" key** (NOT the anon key!)\n",
        "   - ‚úÖ `service_role`: Long key starting with `eyJ...` (200+ chars)\n",
        "   - ‚ùå `anon`: Shorter public key (WRONG for this notebook)\n",
        "\n",
        "**‚ö° RAG system using your existing Supabase embeddings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Google Colab\n",
        "%pip install transformers torch sentence-transformers --quiet\n",
        "%pip install flask flask-cors pyngrok --quiet\n",
        "%pip install supabase python-dotenv --quiet\n",
        "%pip install sacremoses --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import torch\n",
        "from getpass import getpass\n",
        "\n",
        "# AI Models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client\n",
        "\n",
        "# Flask API\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê RAG session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üîß Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "\n",
        "# Available medical models\n",
        "MEDICAL_MODELS = {\n",
        "    \"1\": {\n",
        "        \"name\": \"OpenBioLLM-8B\",\n",
        "        \"path\": \"aaditya/OpenBioLLM-8B\",\n",
        "        \"description\": \"8B parameter medical LLM optimized for biomedical tasks\",\n",
        "        \"type\": \"causal\"\n",
        "    },\n",
        "    \"2\": {\n",
        "        \"name\": \"Med42-v2-8B\",\n",
        "        \"path\": \"m42-health/Llama3-Med42-8B\",\n",
        "        \"description\": \"8B parameter medical model from M42 Health based on Llama3\",\n",
        "        \"type\": \"causal\"\n",
        "    },\n",
        "    \"3\": {\n",
        "        \"name\": \"MedGemma-4B\",\n",
        "        \"path\": \"google/medgemma-4b-it\",\n",
        "        \"description\": \"4B parameter multimodal medical model from Google\",\n",
        "        \"type\": \"multimodal\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nü§ñ Available Medical Models:\")\n",
        "for key, model in MEDICAL_MODELS.items():\n",
        "    print(f\"  {key}. {model['name']} - {model['description']}\")\n",
        "\n",
        "print(\"\\nPlease select a model by entering the number (1, 2, or 3):\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection and loading\n",
        "print(\"ü§ñ Select your medical model:\")\n",
        "model_choice = input(\"Enter model number (1, 2, or 3): \").strip()\n",
        "\n",
        "if model_choice not in MEDICAL_MODELS:\n",
        "    print(f\"‚ùå Invalid choice '{model_choice}'. Defaulting to OpenBioLLM-8B (option 1)\")\n",
        "    model_choice = \"1\"\n",
        "\n",
        "selected_model = MEDICAL_MODELS[model_choice]\n",
        "model_name = selected_model[\"name\"]\n",
        "model_path = selected_model[\"path\"]\n",
        "model_type = selected_model[\"type\"]\n",
        "\n",
        "print(f\"üß† Loading {model_name} for medical text generation...\")\n",
        "print(f\"üì¶ Model path: {model_path}\")\n",
        "print(f\"üîß Model type: {model_type}\")\n",
        "\n",
        "try:\n",
        "    if model_type == \"multimodal\":\n",
        "        # Special handling for MedGemma (multimodal)\n",
        "        print(\"üî§ Loading processor (for multimodal model)...\")\n",
        "        tokenizer = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
        "        \n",
        "        print(\"üß† Loading multimodal model (this may take a few minutes)...\")\n",
        "        print(\"Note: MedGemma will be used in text-only mode for this RAG system\")\n",
        "        medical_model = AutoModelForImageTextToText.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    else:\n",
        "        # Standard causal LM handling\n",
        "        print(\"üî§ Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "        \n",
        "        print(\"üß† Loading model (this may take a few minutes)...\")\n",
        "        medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Set pad token if not available (only for causal models)\n",
        "    if model_type == \"causal\" and hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        else:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "    \n",
        "    # Store model info globally\n",
        "    MODEL_INFO = {\n",
        "        \"name\": model_name,\n",
        "        \"path\": model_path,\n",
        "        \"choice\": model_choice,\n",
        "        \"type\": model_type\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ {model_name} loaded and ready on {device}\")\n",
        "    print(f\"üéØ Selected model: {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n",
        "    print(\"üîÑ Falling back to a smaller model...\")\n",
        "    \n",
        "    # Fallback to a more reliable model\n",
        "    fallback_path = \"microsoft/DialoGPT-medium\"\n",
        "    print(f\"üîÑ Loading fallback model: {fallback_path}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(fallback_path)\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fallback_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    MODEL_INFO = {\n",
        "        \"name\": \"DialoGPT-medium (Fallback)\",\n",
        "        \"path\": fallback_path,\n",
        "        \"choice\": \"fallback\"\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Fallback model loaded successfully\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Setup Supabase connection using Colab secrets\n",
        "print(\"üóÑÔ∏è Setting up Supabase connection using Colab secrets...\")\n",
        "print(\"üìã Required Colab secrets:\")\n",
        "print(\"   1. SUPABASE_URL - Your project URL (e.g., https://abc123.supabase.co)\")\n",
        "print(\"   2. SUPABASE_SERVICE_ROLE_KEY - Your service role key (NOT anon key)\")\n",
        "print(\"   3. NGROK_AUTH_TOKEN - Your ngrok authentication token\")\n",
        "print(\"\")\n",
        "print(\"üîë To set these secrets:\")\n",
        "print(\"   1. Click the üîë key icon in the left sidebar\")\n",
        "print(\"   2. Add the three secrets listed above\")\n",
        "print(\"   3. Re-run this cell\")\n",
        "print(\"\")\n",
        "\n",
        "try:\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    \n",
        "    print(\"‚úÖ Successfully retrieved secrets from Colab\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving secrets: {str(e)}\")\n",
        "    print(\"üîß Make sure you've added the required secrets in Colab:\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "    print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    raise\n",
        "\n",
        "# Validate the inputs\n",
        "if not supabase_url or not supabase_key:\n",
        "    raise ValueError(\"‚ùå Both Supabase URL and Service Role Key are required!\")\n",
        "\n",
        "if not supabase_url.startswith('https://'):\n",
        "    raise ValueError(\"‚ùå Supabase URL should start with 'https://'\")\n",
        "\n",
        "if not supabase_key.startswith('eyJ'):\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys typically start with 'eyJ'\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "    \n",
        "if len(supabase_key) < 100:\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys are typically very long (200+ characters)\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "\n",
        "try:\n",
        "    supabase = create_client(supabase_url, supabase_key)\n",
        "    print(\"‚úÖ Supabase client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Supabase client: {str(e)}\")\n",
        "    print(\"üîß Common issues:\")\n",
        "    print(\"   ‚Ä¢ Wrong API key type (use service_role, not anon)\")\n",
        "    print(\"   ‚Ä¢ Typo in URL or key\")\n",
        "    print(\"   ‚Ä¢ Key might be expired or regenerated\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"top_k\": 5,\n",
        "    \"similarity_threshold\": 0.5,\n",
        "    \"max_context_length\": 2000,\n",
        "    \"max_response_length\": 150,\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è RAG Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Supabase document retrieval functions\n",
        "def query_supabase_documents(query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Query Supabase for similar documents using vector search\"\"\"\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        \n",
        "        # Load the same embedding model used for indexing\n",
        "        print(f\"üîç Loading embedding model for query: {query[:50]}...\")\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "        \n",
        "        top_k = top_k or CONFIG['top_k']\n",
        "        \n",
        "        # Generate embedding for the query\n",
        "        print(f\"üß† Generating embedding vector...\")\n",
        "        query_embedding = embedding_model.encode([query])[0].tolist()\n",
        "        \n",
        "        # Use the correct RPC function from schema.sql: search_embeddings\n",
        "        print(f\"üîç Searching embeddings with threshold {CONFIG['similarity_threshold']}...\")\n",
        "        result = supabase.rpc('search_embeddings', {\n",
        "            'query_embedding': query_embedding,\n",
        "            'match_threshold': CONFIG['similarity_threshold'],\n",
        "            'match_count': top_k\n",
        "        }).execute()\n",
        "        \n",
        "        if result.data:\n",
        "            documents = []\n",
        "            for i, doc in enumerate(result.data):\n",
        "                documents.append({\n",
        "                    'content': doc.get('chunk_content', ''),  # Correct field name from RPC\n",
        "                    'similarity_score': doc.get('similarity', 0.0),\n",
        "                    'metadata': {\n",
        "                        'title': doc.get('title', 'Medical Document'),\n",
        "                        'source': doc.get('source', 'unknown'),\n",
        "                        'topic': doc.get('topic', 'general'),\n",
        "                        'document_type': doc.get('document_type', 'unknown'),\n",
        "                        'document_id': doc.get('document_id', '')\n",
        "                    },\n",
        "                    'rank': i + 1,\n",
        "                    'doc_id': doc.get('document_id', '')\n",
        "                })\n",
        "            \n",
        "            print(f\"üìä Found {len(documents)} similar documents from Supabase\")\n",
        "            return documents\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No similar documents found in Supabase\")\n",
        "            return []\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error querying Supabase: {str(e)}\")\n",
        "        # Fallback: try direct table query if RPC function doesn't exist\n",
        "        try:\n",
        "            print(\"üîÑ Trying fallback query method...\")\n",
        "            result = supabase.table('medical_documents').select('*').limit(top_k).execute()\n",
        "            \n",
        "            if result.data:\n",
        "                documents = []\n",
        "                for i, doc in enumerate(result.data[:top_k]):\n",
        "                    documents.append({\n",
        "                        'content': doc.get('content', ''),\n",
        "                        'similarity_score': 0.8,  # Default similarity\n",
        "                        'metadata': {\n",
        "                            'title': doc.get('title', 'Medical Document'),\n",
        "                            'source': doc.get('source', 'unknown'),\n",
        "                            'topic': doc.get('topic', 'general'),\n",
        "                            'document_type': doc.get('document_type', 'unknown'),\n",
        "                            'document_id': doc.get('id', '')\n",
        "                        },\n",
        "                        'rank': i + 1,\n",
        "                        'doc_id': doc.get('id', '')\n",
        "                    })\n",
        "                \n",
        "                print(f\"üìä Fallback: Retrieved {len(documents)} documents from Supabase\")\n",
        "                return documents\n",
        "            \n",
        "        except Exception as fallback_error:\n",
        "            print(f\"‚ùå Fallback query also failed: {str(fallback_error)}\")\n",
        "            return []\n",
        "\n",
        "# Test Supabase connection and RPC functions\n",
        "print(\"üß™ Testing Supabase connection...\")\n",
        "try:\n",
        "    # Test basic connection\n",
        "    test_result = supabase.table('medical_documents').select('count').execute()\n",
        "    doc_count = len(test_result.data) if test_result.data else 0\n",
        "    print(f\"‚úÖ Supabase connected - Found {doc_count} documents in database\")\n",
        "    \n",
        "    # Test RPC function availability\n",
        "    print(\"üß™ Testing RPC functions...\")\n",
        "    try:\n",
        "        stats_result = supabase.rpc('get_document_stats').execute()\n",
        "        if stats_result.data:\n",
        "            print(\"‚úÖ RPC functions working\")\n",
        "            for stat in stats_result.data[:3]:  # Show first 3 document sources\n",
        "                print(f\"   üìÑ {stat['source']}: {stat['count']} documents\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è RPC function exists but returned no data\")\n",
        "    except Exception as rpc_error:\n",
        "        print(f\"‚ö†Ô∏è RPC function test failed: {str(rpc_error)}\")\n",
        "        print(\"   Vector search will use fallback method\")\n",
        "        \n",
        "except Exception as e:\n",
        "    error_str = str(e)\n",
        "    print(f\"‚ö†Ô∏è Supabase connection test failed: {error_str}\")\n",
        "    \n",
        "    # Provide specific guidance based on error type\n",
        "    if '401' in error_str or 'Invalid API key' in error_str:\n",
        "        print(\"üîß AUTHENTICATION ERROR - Invalid API Key:\")\n",
        "        print(\"   ‚ùå You're using the wrong API key!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Go to your Supabase project dashboard\")\n",
        "        print(\"   2. Settings ‚Üí API\")\n",
        "        print(\"   3. Copy the 'service_role' key (NOT anon key)\")\n",
        "        print(\"   4. The service_role key is much longer and starts with 'eyJ'\")\n",
        "        print(\"   5. Re-run Cell 3 with the correct key\")\n",
        "        print(\"\")\n",
        "        print(\"   üîç Key differences:\")\n",
        "        print(\"   ‚Ä¢ anon key: Used for client-side apps (WRONG for this notebook)\")\n",
        "        print(\"   ‚Ä¢ service_role key: Used for server-side/admin access (CORRECT)\")\n",
        "    elif '404' in error_str:\n",
        "        print(\"üîß TABLE NOT FOUND:\")\n",
        "        print(\"   ‚ùå The 'medical_documents' table doesn't exist!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Run the schema.sql in your Supabase SQL editor\")\n",
        "        print(\"   2. Or run the embed_documents.py script to create tables\")\n",
        "    elif 'timeout' in error_str.lower():\n",
        "        print(\"üîß CONNECTION TIMEOUT:\")\n",
        "        print(\"   ‚ùå Can't reach Supabase servers\")\n",
        "        print(\"   üìã Check your internet connection and Supabase URL\")\n",
        "    else:\n",
        "        print(\"üîß GENERAL CONNECTION ERROR:\")\n",
        "        print(\"   üìã Common fixes:\")\n",
        "        print(\"   ‚Ä¢ Double-check your Supabase URL\")\n",
        "        print(\"   ‚Ä¢ Verify you're using service_role key (not anon)\")\n",
        "        print(\"   ‚Ä¢ Check if your project is paused/suspended\")\n",
        "        print(\"   ‚Ä¢ Ensure database tables exist\")\n",
        "    \n",
        "    print(\"\\n   ‚ö†Ô∏è The system will continue but may have limited document retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_medical_response(prompt: str, max_new_tokens: int = 150) -> str:\n",
        "    \"\"\"Generate medical response using the selected medical model\"\"\"\n",
        "    try:\n",
        "        print(f\"ü§ñ Generating response using {MODEL_INFO['name']}...\")\n",
        "        \n",
        "        # Handle different model types\n",
        "        if MODEL_INFO['type'] == 'multimodal':\n",
        "            # For MedGemma - use chat template format\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful medical assistant. Provide accurate medical information based on the given context.\"}]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\", \n",
        "                    \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
        "                }\n",
        "            ]\n",
        "            \n",
        "            inputs = tokenizer.apply_chat_template(\n",
        "                messages, add_generation_prompt=True, tokenize=True,\n",
        "                return_dict=True, return_tensors=\"pt\"\n",
        "            ).to(medical_model.device, dtype=torch.bfloat16)\n",
        "            \n",
        "            input_len = inputs[\"input_ids\"].shape[-1]\n",
        "            \n",
        "        else:\n",
        "            # Standard causal LM handling\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "            if torch.cuda.is_available():\n",
        "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "            input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        # Generation parameters optimized for medical models\n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        # Set tokens for causal models only\n",
        "        if MODEL_INFO['type'] == 'causal':\n",
        "            if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
        "                generation_params[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "            if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:\n",
        "                generation_params[\"eos_token_id\"] = tokenizer.eos_token_id\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = medical_model.generate(\n",
        "                **inputs,\n",
        "                **generation_params\n",
        "            )\n",
        "        \n",
        "        # Decode response based on model type\n",
        "        if MODEL_INFO['type'] == 'multimodal':\n",
        "            # For MedGemma, decode only the new tokens\n",
        "            generation = outputs[0][input_len:]\n",
        "            response = tokenizer.decode(generation, skip_special_tokens=True)\n",
        "        else:\n",
        "            # Standard causal LM - decode only the generated part\n",
        "            response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "        \n",
        "        response = response.strip()\n",
        "        \n",
        "        # Clean up response for medical context\n",
        "        # Remove common artifacts\n",
        "        response = response.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
        "        \n",
        "        # Basic quality check\n",
        "        if not response or len(response) < 10:\n",
        "            response = \"I understand your question about health. Please consult with a healthcare professional for personalized medical advice.\"\n",
        "        \n",
        "        print(f\"‚úÖ Response generated successfully ({len(response)} characters)\")\n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation error: {str(e)}\")\n",
        "        return f\"I apologize, but I encountered an error processing your question. Please try rephrasing your question or consult with a healthcare professional.\"\n",
        "\n",
        "class WellnessRAGSystem:\n",
        "    \"\"\"RAG system for medical/wellness queries using Supabase and BioGPT\"\"\"\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "    \n",
        "    def retrieve_context(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Retrieve relevant document chunks from Supabase\"\"\"\n",
        "        retrieved_docs = query_supabase_documents(query, self.config['top_k'])\n",
        "        \n",
        "        context_parts = []\n",
        "        total_chars = 0\n",
        "        \n",
        "        for doc in retrieved_docs:\n",
        "            if total_chars + len(doc['content']) <= self.config['max_context_length']:\n",
        "                context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                total_chars += len(doc['content'])\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'context': context,\n",
        "            'retrieved_documents': retrieved_docs,\n",
        "            'total_documents_found': len(retrieved_docs),\n",
        "            'documents_used': len(retrieved_docs),\n",
        "            'context_length': len(context)\n",
        "        }\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG query: retrieve context and generate response\"\"\"\n",
        "        print(f\"üîç Processing query: {question}\")\n",
        "        \n",
        "        context_result = self.retrieve_context(question)\n",
        "        \n",
        "        print(f\"üìä Found {context_result['total_documents_found']} similar documents\")\n",
        "        print(f\"üìÑ Using {context_result['documents_used']} documents for context\")\n",
        "        \n",
        "        print(f\"ü§ñ Generating response using {MODEL_INFO['name']}...\")\n",
        "        # Use the advanced medical prompt structure\n",
        "        medical_prompt = f\"\"\"You are a helpful and accurate medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context_result['context']}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (based only on the context, no assumptions):\"\"\"\n",
        "        generated_response = generate_medical_response(medical_prompt, self.config['max_response_length'])\n",
        "        print(\"‚úÖ Response generated successfully\")\n",
        "        \n",
        "        result = {\n",
        "            'query': question,\n",
        "            'response': generated_response,\n",
        "            'sources': [\n",
        "                {\n",
        "                    'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                    'source': doc['metadata']['source'],\n",
        "                    'topic': doc['metadata']['topic'],\n",
        "                    'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                    'rank': doc['rank'],\n",
        "                    'content_preview': doc['content'][:150] + \"...\"\n",
        "                }\n",
        "                for doc in context_result['retrieved_documents']\n",
        "            ],\n",
        "            'metadata': {\n",
        "                'documentsUsed': context_result['documents_used'],\n",
        "                'totalFound': context_result['total_documents_found'],\n",
        "                'contextLength': context_result['context_length'],\n",
        "                'model': MODEL_INFO['name'],\n",
        "                'model_path': MODEL_INFO['path'],\n",
        "                'embeddings': 'Supabase pgvector',\n",
        "                'processingTime': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Initialize the RAG system\n",
        "rag_system = WellnessRAGSystem(config=CONFIG)\n",
        "print(\"‚úÖ WellnessGrid RAG system initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Flask API Setup with Chat History Support\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Store chat sessions in memory (in production, use Redis or database)\n",
        "chat_sessions = {}\n",
        "\n",
        "@app.route('/embed', methods=['POST'])\n",
        "def generate_embedding():\n",
        "    \"\"\"Generate embeddings for text with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        text = data.get(\"text\", \"\")\n",
        "        \n",
        "        if not text:\n",
        "            return jsonify({\"error\": \"Missing 'text' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"üîç Generating embedding for text: {text[:100]}...\")\n",
        "        \n",
        "        # Load embedding model for this request\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "        \n",
        "        # Generate embedding\n",
        "        embedding = embedding_model.encode([text])[0].tolist()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Generated embedding with {len(embedding)} dimensions\")\n",
        "        \n",
        "        return jsonify({\n",
        "            \"embedding\": embedding,\n",
        "            \"dimensions\": len(embedding),\n",
        "            \"model\": \"PubMedBERT\"\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in embed endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return jsonify({\"error\": f\"Embedding generation failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/generate', methods=['POST'])\n",
        "def generate_text():\n",
        "    \"\"\"Enhanced generate endpoint with chat history support\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get(\"query\", \"\")\n",
        "        context = data.get(\"context\", \"\")\n",
        "        history = data.get(\"history\", [])  # New: chat history support\n",
        "        max_tokens = data.get(\"max_tokens\", 200)\n",
        "        temperature = data.get(\"temperature\", 0.7)\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"üî¨ Generating response for query: {query[:100]}...\")\n",
        "        logger.info(f\"üìö Context length: {len(context)} characters\")\n",
        "        logger.info(f\"üí¨ Chat history: {len(history)} messages\")\n",
        "        \n",
        "        # Create enhanced prompt with chat history\n",
        "        if history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in history[-3:]])  # Last 3 exchanges\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {query}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Generate response using the selected medical model\n",
        "        response = generate_medical_response(prompt, max_tokens)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"answer\": response,\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"model_path\": MODEL_INFO['path'],\n",
        "            \"context_used\": len(context) > 0,\n",
        "            \"history_used\": len(history) > 0\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in generate endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"Text generation failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/query', methods=['POST'])\n",
        "def query_docs():\n",
        "    \"\"\"Enhanced query endpoint with better error handling\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        query = data.get(\"query\", \"\")\n",
        "        top_k = data.get(\"top_k\", CONFIG['top_k'])\n",
        "        \n",
        "        if not query:\n",
        "            return jsonify({\"error\": \"Missing 'query' field.\"}), 400\n",
        "\n",
        "        results = query_supabase_documents(query, top_k=top_k)\n",
        "        \n",
        "        return jsonify({\n",
        "            \"documents\": results,\n",
        "            \"total_found\": len(results),\n",
        "            \"query\": query\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in query endpoint: {str(e)}\")\n",
        "        return jsonify({\"error\": f\"Document query failed: {str(e)}\"}), 500\n",
        "\n",
        "@app.route('/ask', methods=['POST'])\n",
        "def ask_rag():\n",
        "    \"\"\"Enhanced RAG endpoint with chat history support\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        question = data.get(\"question\", \"\")\n",
        "        session_id = data.get(\"session_id\", \"default\")\n",
        "        history = data.get(\"history\", [])\n",
        "        \n",
        "        if not question:\n",
        "            return jsonify({\"error\": \"Missing 'question' field.\"}), 400\n",
        "        \n",
        "        logger.info(f\"ü§ñ Processing RAG query: {question[:100]}...\")\n",
        "        logger.info(f\"üìù Session ID: {session_id}\")\n",
        "        \n",
        "        # Use session-specific history if no history provided\n",
        "        if not history and session_id in chat_sessions:\n",
        "            history = chat_sessions[session_id]\n",
        "        \n",
        "        # Get response using existing query method\n",
        "        result = rag_system.query(question)\n",
        "        \n",
        "        # Update chat history\n",
        "        new_message = {\"question\": question, \"answer\": result['response']}\n",
        "        if session_id not in chat_sessions:\n",
        "            chat_sessions[session_id] = []\n",
        "        chat_sessions[session_id].append(new_message)\n",
        "        \n",
        "        # Keep only last 10 messages to prevent memory issues\n",
        "        if len(chat_sessions[session_id]) > 10:\n",
        "            chat_sessions[session_id] = chat_sessions[session_id][-10:]\n",
        "        \n",
        "        return jsonify({\n",
        "            \"response\": result['response'],\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"title\": source['title'], \n",
        "                    \"content\": source['content_preview'],\n",
        "                    \"similarity\": float(source['similarity'])\n",
        "                }\n",
        "                for source in result['sources']\n",
        "            ],\n",
        "            \"chat_history\": chat_sessions[session_id],\n",
        "            \"session_id\": session_id,\n",
        "            \"mockMode\": False,\n",
        "            \"metadata\": result['metadata']\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in ask endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        return jsonify({\n",
        "            \"response\": f\"I apologize, but I encountered an error processing your question: {str(e)}\",\n",
        "            \"sources\": [],\n",
        "            \"chat_history\": [],\n",
        "            \"mockMode\": True,\n",
        "            \"error\": str(e)\n",
        "        }), 500\n",
        "\n",
        "@app.route('/chat/history/<session_id>', methods=['GET'])\n",
        "def get_chat_history(session_id):\n",
        "    \"\"\"Get chat history for a session\"\"\"\n",
        "    try:\n",
        "        history = chat_sessions.get(session_id, [])\n",
        "        return jsonify({\n",
        "            \"session_id\": session_id,\n",
        "            \"history\": history,\n",
        "            \"message_count\": len(history)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/chat/clear/<session_id>', methods=['POST'])\n",
        "def clear_chat_history(session_id):\n",
        "    \"\"\"Clear chat history for a session\"\"\"\n",
        "    try:\n",
        "        if session_id in chat_sessions:\n",
        "            del chat_sessions[session_id]\n",
        "        \n",
        "        # RAG system doesn't maintain its own history, only session-based history\n",
        "        \n",
        "        return jsonify({\n",
        "            \"session_id\": session_id,\n",
        "            \"cleared\": True\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Enhanced health check endpoint\"\"\"\n",
        "    try:\n",
        "        # Test Supabase connection\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        \n",
        "        # Test embeddings\n",
        "        embed_result = supabase.table('document_embeddings').select('count').execute()\n",
        "        embed_count = len(embed_result.data) if embed_result.data else 0\n",
        "        \n",
        "        return jsonify({\n",
        "            \"status\": \"healthy\",\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"model_path\": MODEL_INFO['path'],\n",
        "            \"database\": \"Supabase + pgvector\",\n",
        "            \"documents_in_db\": doc_count,\n",
        "            \"embeddings_in_db\": embed_count,\n",
        "            \"rag_system\": \"enhanced\",\n",
        "            \"chat_support\": True,\n",
        "            \"active_sessions\": len(chat_sessions)\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\n",
        "            \"status\": \"partial\",\n",
        "            \"model\": MODEL_INFO['name'],\n",
        "            \"model_path\": MODEL_INFO['path'],\n",
        "            \"database\": \"Supabase (connection issues)\",\n",
        "            \"documents_in_db\": \"unknown\",\n",
        "            \"rag_system\": \"enhanced\",\n",
        "            \"chat_support\": True,\n",
        "            \"warning\": str(e)\n",
        "        })\n",
        "\n",
        "@app.route('/status', methods=['GET'])\n",
        "def status():\n",
        "    \"\"\"Detailed status endpoint\"\"\"\n",
        "    try:\n",
        "        return jsonify({\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"models\": {\n",
        "                \"selected_model\": MODEL_INFO['name'] if 'MODEL_INFO' in globals() else \"not_selected\",\n",
        "                \"medical_model\": \"loaded\" if 'medical_model' in globals() else \"not_loaded\",\n",
        "                \"pubmedbert\": \"available\"\n",
        "            },\n",
        "            \"database\": {\n",
        "                \"connected\": True,\n",
        "                \"url\": supabase_url[:30] + \"...\" if supabase_url else \"not_set\"\n",
        "            },\n",
        "            \"config\": CONFIG,\n",
        "            \"memory\": {\n",
        "                \"active_sessions\": len(chat_sessions),\n",
        "                \"session_ids\": list(chat_sessions.keys())\n",
        "            }\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "print(\"üåê Enhanced Flask API endpoints configured:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings with enhanced error handling\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} + chat history\")\n",
        "print(\"  ‚úÖ POST /ask - Enhanced RAG endpoint with chat history\")\n",
        "print(\"  ‚úÖ GET /health - Enhanced health check\")\n",
        "print(\"  ‚úÖ POST /query - Enhanced document query\")\n",
        "print(\"  ‚úÖ GET /chat/history/<session_id> - Get chat history\")\n",
        "print(\"  ‚úÖ POST /chat/clear/<session_id> - Clear chat history\")\n",
        "print(\"  ‚úÖ GET /status - Detailed status information\")\n",
        "print(f\"‚úÖ Enhanced Flask server ready with {MODEL_INFO['name']} and chat history support!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG system\n",
        "print(\"üß™ Testing RAG system with sample question...\")\n",
        "\n",
        "test_question = \"What are the symptoms of diabetes?\"\n",
        "try:\n",
        "    print(f\"üîç Testing query: {test_question}\")\n",
        "    result = rag_system.query(test_question)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚ùì QUESTION: {result['query']}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nü§ñ AI RESPONSE:\")\n",
        "    print(f\"{result['response']}\")\n",
        "    \n",
        "    print(f\"\\nüìö SOURCES ({result['metadata']['documentsUsed']} documents):\")\n",
        "    if result['sources']:\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"   {i}. {source['title']} - {source['source']}\")\n",
        "            print(f\"      üìä Similarity: {source['similarity']}\")\n",
        "            print(f\"      üìÑ Preview: {source['content_preview']}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è No sources found - this could indicate:\")\n",
        "        print(\"   ‚Ä¢ No documents in database yet\")\n",
        "        print(\"   ‚Ä¢ Similarity threshold too high\")\n",
        "        print(\"   ‚Ä¢ RPC function needs adjustment\")\n",
        "    \n",
        "    print(f\"\\nüìä Metadata:\")\n",
        "    print(f\"   üîß Model: {result['metadata']['model']}\")\n",
        "    print(f\"   üíæ Embeddings: {result['metadata']['embeddings']}\")\n",
        "    print(f\"   üìÑ Documents Used: {result['metadata']['documentsUsed']}\")\n",
        "    print(f\"   üéØ Total Found: {result['metadata']['totalFound']}\")\n",
        "    \n",
        "    print(\"‚úÖ RAG system test completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è RAG test failed: {str(e)}\")\n",
        "    print(\"   This might be normal if:\")\n",
        "    print(\"   ‚Ä¢ Supabase connection needs adjustment\")\n",
        "    print(\"   ‚Ä¢ No documents have been embedded yet\")\n",
        "    print(\"   ‚Ä¢ RPC function is not deployed\")\n",
        "    print(\"   The Flask server will still start and you can test via the API\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ngrok setup and Flask server startup\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Quick pre-check to give immediate feedback\n",
        "print(\"üîç Pre-flight check...\")\n",
        "required_vars = ['medical_model', 'tokenizer', 'supabase', 'rag_system', 'CONFIG', 'MODEL_INFO']\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"‚ùå Missing required variables: {', '.join(missing_vars)}\")\n",
        "    print(f\"üîß Please run all previous cells (1-7) in order first!\")\n",
        "    print(f\"   Then come back to this cell.\")\n",
        "    raise RuntimeError(f\"Required setup incomplete. Missing: {', '.join(missing_vars)}\")\n",
        "\n",
        "print(\"‚úÖ Pre-flight check passed!\")\n",
        "print(f\"üéØ Selected model: {MODEL_INFO['name']}\")\n",
        "\n",
        "print(\"üîë Using ngrok auth token from Colab secrets...\")\n",
        "\n",
        "# Use the token we already retrieved in Cell 3\n",
        "if 'ngrok_token' not in globals() or not ngrok_token:\n",
        "    print(\"‚ùå Ngrok token not found in secrets!\")\n",
        "    print(\"üîß Make sure you've added NGROK_AUTH_TOKEN to Colab secrets\")\n",
        "    raise ValueError(\"Missing NGROK_AUTH_TOKEN in Colab secrets\")\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"‚úÖ Ngrok token set successfully!\")\n",
        "\n",
        "# Validate setup\n",
        "print(\"üîç Validating setup...\")\n",
        "\n",
        "try:\n",
        "    # Check if variables exist before asserting their values\n",
        "    missing_components = []\n",
        "    \n",
        "    # Check Medical model\n",
        "    try:\n",
        "        if 'medical_model' not in globals() or medical_model is None:\n",
        "            missing_components.append(\"Medical model (run Cell 3)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {MODEL_INFO['name']}: Loaded\")\n",
        "    except NameError:\n",
        "        missing_components.append(\"Medical model (run Cell 3)\")\n",
        "    \n",
        "    # Check tokenizer\n",
        "    try:\n",
        "        if 'tokenizer' not in globals() or tokenizer is None:\n",
        "            missing_components.append(\"Tokenizer (run Cell 3)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Tokenizer: Loaded\")\n",
        "    except NameError:\n",
        "        missing_components.append(\"Tokenizer (run Cell 3)\")\n",
        "    \n",
        "    # Check Supabase client\n",
        "    try:\n",
        "        if 'supabase' not in globals() or supabase is None:\n",
        "            missing_components.append(\"Supabase client (run Cell 3)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Supabase client: Initialized\")\n",
        "    except NameError:\n",
        "        missing_components.append(\"Supabase client (run Cell 3)\")\n",
        "    \n",
        "    # Check RAG system\n",
        "    try:\n",
        "        if 'rag_system' not in globals() or rag_system is None:\n",
        "            missing_components.append(\"RAG system (run Cell 5)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ RAG system: Initialized\")\n",
        "    except NameError:\n",
        "        missing_components.append(\"RAG system (run Cell 5)\")\n",
        "    \n",
        "    # Check device\n",
        "    try:\n",
        "        if 'device' not in globals():\n",
        "            missing_components.append(\"Device variable (run Cell 3)\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Device: {device}\")\n",
        "    except NameError:\n",
        "        missing_components.append(\"Device variable (run Cell 3)\")\n",
        "    \n",
        "    # If any components are missing, show helpful error message\n",
        "    if missing_components:\n",
        "        print(f\"\\n‚ùå Missing components:\")\n",
        "        for component in missing_components:\n",
        "            print(f\"   ‚Ä¢ {component}\")\n",
        "        print(f\"\\nüîß To fix this:\")\n",
        "        print(f\"   1. Run all previous cells in order (Cells 1-7)\")\n",
        "        print(f\"   2. Wait for each cell to complete before running the next\")\n",
        "        print(f\"   3. Look for any error messages in the cell outputs\")\n",
        "        print(f\"   4. Then run this cell again\")\n",
        "        raise ValueError(f\"Missing required components: {', '.join(missing_components)}\")\n",
        "    \n",
        "    # Test Supabase connection one more time\n",
        "    try:\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        print(f\"‚úÖ Supabase: Connected ({doc_count} documents)\")\n",
        "    except Exception as db_error:\n",
        "        print(f\"‚ö†Ô∏è Supabase: Connection issues ({str(db_error)})\")\n",
        "        print(\"   RAG system will still work but may have limited retrieval\")\n",
        "    \n",
        "    # Start ngrok tunnel\n",
        "    print(\"üåê Starting ngrok tunnel...\")\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print(f\"üåç Public URL: {public_url}\")\n",
        "    print(\"üìã Copy this URL to your WellnessGrid app configuration!\")\n",
        "    \n",
        "    # Start Flask app\n",
        "    print(\"üöÄ Starting Flask app...\")\n",
        "    print(\"üì° Available endpoints:\")\n",
        "    print(\"  ‚úÖ POST /embed - Generate embeddings (required by WellnessGrid)\")\n",
        "    print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} (required by WellnessGrid)\")\n",
        "    print(\"  ‚úÖ POST /ask - Main RAG endpoint for WellnessGrid\")\n",
        "    print(\"  ‚úÖ GET /health - Health check\")\n",
        "    print(\"  ‚úÖ POST /query - Query documents from Supabase\")\n",
        "    print(\"\\nüéØ IMPORTANT: Copy the ngrok URL above to your WellnessGrid .env.local:\")\n",
        "    print(\"   FLASK_API_URL=https://your-ngrok-id.ngrok.io\")\n",
        "    print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
        "    print(f\"\\nüöÄ Your WellnessGrid RAG system with {MODEL_INFO['name']} is now live!\")\n",
        "    \n",
        "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Setup validation failed: {str(e)}\")\n",
        "    print(\"\\nüîß Troubleshooting steps:\")\n",
        "    print(\"   1. ‚ö° Run ALL previous cells (1-7) in order\")\n",
        "    print(\"   2. üîç Check for errors in cell outputs\")\n",
        "    print(\"   3. üîÑ Restart kernel if needed: Runtime ‚Üí Restart Runtime\")\n",
        "    print(\"   4. üìã Re-run cells 1-7, then try this cell again\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Test of Flask Endpoints (Optional)\n",
        "# Run this AFTER starting the Flask server in Cell 8\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def test_flask_endpoints():\n",
        "    \"\"\"Test the Flask endpoints locally\"\"\"\n",
        "    base_url = \"http://localhost:5000\"\n",
        "    \n",
        "    print(\"üß™ Testing Flask endpoints locally...\")\n",
        "    print(\"‚ö†Ô∏è Make sure Cell 8 (Flask server) is running first!\\n\")\n",
        "    \n",
        "    # Test 1: Health check\n",
        "    try:\n",
        "        print(\"1. Testing /health endpoint...\")\n",
        "        response = requests.get(f\"{base_url}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Health check passed\")\n",
        "            print(f\"   Response: {response.json()}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Health check failed: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Health check error: {str(e)}\")\n",
        "        print(\"   Make sure Flask server is running (Cell 8)\")\n",
        "        return\n",
        "    \n",
        "    # Test 2: Embedding endpoint\n",
        "    try:\n",
        "        print(\"\\n2. Testing /embed endpoint...\")\n",
        "        test_data = {\"text\": \"What is diabetes?\"}\n",
        "        response = requests.post(f\"{base_url}/embed\", \n",
        "                               json=test_data, \n",
        "                               headers={\"Content-Type\": \"application/json\"},\n",
        "                               timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            embedding = result.get('embedding', [])\n",
        "            print(f\"‚úÖ Embedding test passed\")\n",
        "            print(f\"   Embedding dimensions: {len(embedding)}\")\n",
        "            print(f\"   First 3 values: {embedding[:3]}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Embedding test failed: {response.status_code}\")\n",
        "            print(f\"   Error: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Embedding test error: {str(e)}\")\n",
        "    \n",
        "    # Test 3: Generation endpoint\n",
        "    try:\n",
        "        print(\"\\n3. Testing /generate endpoint...\")\n",
        "        test_data = {\n",
        "            \"query\": \"What is diabetes?\",\n",
        "            \"context\": \"Diabetes is a chronic condition affecting blood sugar levels.\",\n",
        "            \"max_tokens\": 50,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        response = requests.post(f\"{base_url}/generate\", \n",
        "                               json=test_data, \n",
        "                               headers={\"Content-Type\": \"application/json\"},\n",
        "                               timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            answer = result.get('answer', '')\n",
        "            print(f\"‚úÖ Generation test passed\")\n",
        "            print(f\"   Answer length: {len(answer)} characters\")\n",
        "            print(f\"   Answer preview: {answer[:100]}...\")\n",
        "        else:\n",
        "            print(f\"‚ùå Generation test failed: {response.status_code}\")\n",
        "            print(f\"   Error: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation test error: {str(e)}\")\n",
        "    \n",
        "    print(\"\\nüéØ Test completed!\")\n",
        "    print(\"If all tests pass, your Flask server is ready for WellnessGrid!\")\n",
        "\n",
        "# Uncomment the line below to run the test\n",
        "# test_flask_endpoints()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
