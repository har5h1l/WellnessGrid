{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid RAG System - Multi-Model Edition\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) system for medical questions using:\n",
        "\n",
        "1. ü§ñ **Selectable Medical Models** for advanced medical text generation\n",
        "2. üóÑÔ∏è **Supabase + pgvector** for document retrieval  \n",
        "3. üåê **Flask API with ngrok** for external access\n",
        "4. üîç **Pre-embedded medical documents** from your database\n",
        "\n",
        "## Available Medical Models:\n",
        "- **OpenBioLLM-8B**: 8B parameter medical LLM optimized for biomedical tasks\n",
        "- **Med42-v2-8B**: 8B parameter medical model from M42 Health\n",
        "- **MedGemma-4B**: 4B parameter medical model based on Gemma architecture\n",
        "\n",
        "## Features:\n",
        "- **Model Selection**: Choose from multiple specialized medical LLMs\n",
        "- **Vector Search**: Supabase pgvector with existing embeddings\n",
        "- **Flask API**: Compatible with WellnessGrid frontend\n",
        "- **Google Colab**: GPU-accelerated inference\n",
        "- **ngrok**: Public URL for external access\n",
        "- **Fallback Support**: Automatic fallback to stable models if loading fails\n",
        "\n",
        "## Recent Updates (Multi-Model Support):\n",
        "- ‚úÖ **Model Selection**: Interactive choice between 3 medical models\n",
        "- ‚úÖ **Dynamic Loading**: Only loads the selected model to save memory\n",
        "- ‚úÖ **Fallback Support**: Automatic fallback if preferred model fails\n",
        "- ‚úÖ **Enhanced Error Handling**: Better validation and error messages\n",
        "- ‚úÖ **Updated API**: All endpoints reflect the selected model\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order and select your preferred medical model when prompted\n",
        "3. Enter your Supabase credentials and ngrok auth token when prompted  \n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n",
        "\n",
        "## Prerequisites:\n",
        "- Supabase database with `medical_documents` and `document_embeddings` tables\n",
        "- Documents embedded using `embed_documents.py` script\n",
        "- RPC function `search_embeddings` deployed in Supabase\n",
        "- **IMPORTANT**: Use `service_role` API key (NOT `anon` key)\n",
        "\n",
        "## üîë How to Get Your Supabase Credentials:\n",
        "1. **Go to your Supabase project dashboard**\n",
        "2. **Click \"Settings\" ‚Üí \"API\"**\n",
        "3. **Copy \"Project URL\"** (starts with `https://`)\n",
        "4. **Copy \"service_role\" key** (NOT the anon key!)\n",
        "   - ‚úÖ `service_role`: Long key starting with `eyJ...` (200+ chars)\n",
        "   - ‚ùå `anon`: Shorter public key (WRONG for this notebook)\n",
        "\n",
        "**‚ö° RAG system using your existing Supabase embeddings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Google Colab\n",
        "%pip install transformers torch sentence-transformers --quiet\n",
        "%pip install supabase python-dotenv --quiet\n",
        "%pip install sacremoses --quiet\n",
        "%pip install fastapi uvicorn pydantic python-multipart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import torch\n",
        "from getpass import getpass\n",
        "\n",
        "# AI Models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client\n",
        "\n",
        "# FastAPI\n",
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "import uvicorn\n",
        "\n",
        "# Pydantic models for validation\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê RAG session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üîß Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "\n",
        "# Available medical models\n",
        "MEDICAL_MODELS = {\n",
        "    \"1\": {\n",
        "        \"name\": \"OpenBioLLM-8B\",\n",
        "        \"path\": \"aaditya/OpenBioLLM-Llama3-8B\",\n",
        "        \"description\": \"8B parameter medical LLM optimized for biomedical tasks\",\n",
        "        \"type\": \"causal\"\n",
        "    },\n",
        "    \"2\": {\n",
        "        \"name\": \"Med42-v2-8B\",\n",
        "        \"path\": \"m42-health/Llama3-Med42-8B\",\n",
        "        \"description\": \"8B parameter medical model from M42 Health based on Llama3\",\n",
        "        \"type\": \"causal\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nü§ñ Available Medical Models:\")\n",
        "for key, model in MEDICAL_MODELS.items():\n",
        "    print(f\"  {key}. {model['name']} - {model['description']}\")\n",
        "\n",
        "print(\"\\nPlease select a model by entering the number (1 or 2):\")\n",
        "print(\"Note: MedGemma-4B is available in a separate notebook (query_rag_medgemma.ipynb)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic models for request/response validation\n",
        "class AskRequest(BaseModel):\n",
        "    question: str = Field(..., min_length=1, max_length=1000, description=\"The medical question to ask\")\n",
        "    session_id: Optional[str] = Field(default=\"default\", description=\"Chat session identifier\")\n",
        "    history: Optional[List[dict]] = Field(default=[], description=\"Previous chat history\")\n",
        "\n",
        "class AskResponse(BaseModel):\n",
        "    response: str = Field(..., description=\"AI generated response\")\n",
        "    sources: List[dict] = Field(..., description=\"Source documents used\")\n",
        "    chat_history: List[dict] = Field(..., description=\"Updated chat history\")\n",
        "    session_id: str = Field(..., description=\"Session identifier\")\n",
        "    mockMode: bool = Field(default=False, description=\"Whether response is from mock mode\")\n",
        "    metadata: dict = Field(..., description=\"Response metadata\")\n",
        "\n",
        "class EmbedRequest(BaseModel):\n",
        "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to embed\")\n",
        "\n",
        "class EmbedResponse(BaseModel):\n",
        "    embedding: List[float] = Field(..., description=\"Generated embedding vector\")\n",
        "    dimensions: int = Field(..., description=\"Embedding dimensions\")\n",
        "    model: str = Field(..., description=\"Model used for embedding\")\n",
        "    device: str = Field(..., description=\"Device used for embedding\")\n",
        "\n",
        "class GenerateRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Query text\")\n",
        "    context: str = Field(default=\"\", description=\"Context information\")\n",
        "    history: List[dict] = Field(default=[], description=\"Chat history\")\n",
        "    max_tokens: int = Field(default=200, ge=1, le=1000, description=\"Maximum tokens to generate\")\n",
        "    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Generation temperature\")\n",
        "\n",
        "class GenerateResponse(BaseModel):\n",
        "    answer: str = Field(..., description=\"Generated answer\")\n",
        "    model: str = Field(..., description=\"Model used for generation\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    context_used: bool = Field(..., description=\"Whether context was used\")\n",
        "    history_used: bool = Field(..., description=\"Whether history was used\")\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Search query\")\n",
        "    top_k: int = Field(default=5, ge=1, le=20, description=\"Number of documents to retrieve\")\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    documents: List[dict] = Field(..., description=\"Retrieved documents\")\n",
        "    total_found: int = Field(..., description=\"Total documents found\")\n",
        "    query: str = Field(..., description=\"Original query\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str = Field(..., description=\"System status\")\n",
        "    model: str = Field(..., description=\"Active model name\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    embedding_device: str = Field(..., description=\"Embedding device\")\n",
        "    database: str = Field(..., description=\"Database type\")\n",
        "    documents_in_db: int = Field(..., description=\"Number of documents in database\")\n",
        "    embeddings_in_db: int = Field(..., description=\"Number of embeddings in database\")\n",
        "    rag_system: str = Field(..., description=\"RAG system type\")\n",
        "    chat_support: bool = Field(..., description=\"Chat support status\")\n",
        "    active_sessions: int = Field(..., description=\"Number of active sessions\")\n",
        "    gpu_memory: dict = Field(..., description=\"GPU memory information\")\n",
        "\n",
        "print(\"‚úÖ Pydantic models defined for request/response validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory Management and Background Processing\n",
        "import gc\n",
        "import psutil\n",
        "from typing import Optional\n",
        "\n",
        "class MemoryManager:\n",
        "    \"\"\"Manages GPU and CPU memory efficiently\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gpu_memory_threshold = 0.8  # 80% GPU memory usage\n",
        "        self.cpu_memory_threshold = 0.9  # 90% CPU memory usage\n",
        "    \n",
        "    async def check_gpu_memory(self) -> bool:\n",
        "        \"\"\"Check if GPU memory is available\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                allocated = torch.cuda.memory_allocated()\n",
        "                total = torch.cuda.get_device_properties(0).total_memory\n",
        "                usage_ratio = allocated / total\n",
        "                return usage_ratio < self.gpu_memory_threshold\n",
        "            except:\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    async def check_cpu_memory(self) -> bool:\n",
        "        \"\"\"Check if CPU memory is available\"\"\"\n",
        "        try:\n",
        "            memory = psutil.virtual_memory()\n",
        "            return memory.percent < (self.cpu_memory_threshold * 100)\n",
        "        except:\n",
        "            return True\n",
        "    \n",
        "    async def clear_gpu_cache(self):\n",
        "        \"\"\"Clear GPU memory cache\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            print(\"üßπ GPU cache cleared\")\n",
        "    \n",
        "    async def clear_cpu_cache(self):\n",
        "        \"\"\"Clear CPU memory cache\"\"\"\n",
        "        gc.collect()\n",
        "        print(\"üßπ CPU cache cleared\")\n",
        "    \n",
        "    async def get_memory_status(self) -> dict:\n",
        "        \"\"\"Get current memory status\"\"\"\n",
        "        status = {\n",
        "            \"gpu_available\": torch.cuda.is_available(),\n",
        "            \"cpu_memory_percent\": psutil.virtual_memory().percent\n",
        "        }\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "                total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                status.update({\n",
        "                    \"gpu_allocated_gb\": f\"{allocated:.2f}\",\n",
        "                    \"gpu_total_gb\": f\"{total:.2f}\",\n",
        "                    \"gpu_usage_percent\": f\"{(allocated/total)*100:.1f}%\"\n",
        "                })\n",
        "            except:\n",
        "                status[\"gpu_info\"] = \"unavailable\"\n",
        "        \n",
        "        return status\n",
        "\n",
        "class BackgroundTaskManager:\n",
        "    \"\"\"Manages background tasks and queues\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.embedding_queue = asyncio.Queue(maxsize=100)\n",
        "        self.generation_queue = asyncio.Queue(maxsize=50)\n",
        "        self.memory_manager = MemoryManager()\n",
        "        self.running = False\n",
        "    \n",
        "    async def start_background_workers(self):\n",
        "        \"\"\"Start background workers for embedding and generation\"\"\"\n",
        "        self.running = True\n",
        "        \n",
        "        # Start embedding worker\n",
        "        asyncio.create_task(self._embedding_worker())\n",
        "        \n",
        "        # Start generation worker\n",
        "        asyncio.create_task(self._generation_worker())\n",
        "        \n",
        "        print(\"‚úÖ Background workers started\")\n",
        "    \n",
        "    async def stop_background_workers(self):\n",
        "        \"\"\"Stop background workers\"\"\"\n",
        "        self.running = False\n",
        "        print(\"ÔøΩÔøΩ Background workers stopped\")\n",
        "    \n",
        "    async def _embedding_worker(self):\n",
        "        \"\"\"Background worker for embedding tasks\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Check memory before processing\n",
        "                if not await self.memory_manager.check_cpu_memory():\n",
        "                    await asyncio.sleep(1)\n",
        "                    continue\n",
        "                \n",
        "                # Process embedding tasks\n",
        "                if not self.embedding_queue.empty():\n",
        "                    task = await self.embedding_queue.get()\n",
        "                    # Process task here\n",
        "                    self.embedding_queue.task_done()\n",
        "                \n",
        "                await asyncio.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Embedding worker error: {str(e)}\")\n",
        "                await asyncio.sleep(1)\n",
        "    \n",
        "    async def _generation_worker(self):\n",
        "        \"\"\"Background worker for generation tasks\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Check GPU memory before processing\n",
        "                if not await self.memory_manager.check_gpu_memory():\n",
        "                    await asyncio.sleep(1)\n",
        "                    continue\n",
        "                \n",
        "                # Process generation tasks\n",
        "                if not self.generation_queue.empty():\n",
        "                    task = await self.generation_queue.get()\n",
        "                    # Process task here\n",
        "                    self.generation_queue.task_done()\n",
        "                \n",
        "                await asyncio.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Generation worker error: {str(e)}\")\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "# Initialize memory and background managers\n",
        "print(\"üóÉÔ∏è Initializing memory and background managers...\")\n",
        "memory_manager = MemoryManager()\n",
        "background_manager = BackgroundTaskManager()\n",
        "\n",
        "# Start background workers\n",
        "await background_manager.start_background_workers()\n",
        "\n",
        "print(\"‚úÖ Memory and background managers initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection and loading\n",
        "print(\"ü§ñ Select your medical model:\")\n",
        "model_choice = input(\"Enter model number (1 or 2): \").strip()\n",
        "\n",
        "if model_choice not in MEDICAL_MODELS:\n",
        "    print(f\"‚ùå Invalid choice '{model_choice}'. Defaulting to OpenBioLLM-8B (option 1)\")\n",
        "    model_choice = \"1\"\n",
        "\n",
        "selected_model = MEDICAL_MODELS[model_choice]\n",
        "model_name = selected_model[\"name\"]\n",
        "model_path = selected_model[\"path\"]\n",
        "model_type = selected_model[\"type\"]\n",
        "\n",
        "print(f\"üß† Loading {model_name} for medical text generation...\")\n",
        "print(f\"üì¶ Model path: {model_path}\")\n",
        "print(f\"üîß Model type: {model_type}\")\n",
        "\n",
        "# Check GPU memory before loading\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîç GPU Memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "\n",
        "try:\n",
        "    # Standard causal LM handling (models 1 and 2 are both causal models)\n",
        "    print(\"üî§ Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "    \n",
        "    print(\"üß† Loading model (this may take a few minutes)...\")\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Set pad token if not available\n",
        "    if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        else:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "    \n",
        "    # Store model info globally\n",
        "    MODEL_INFO = {\n",
        "        \"name\": model_name,\n",
        "        \"path\": model_path,\n",
        "        \"choice\": model_choice,\n",
        "        \"type\": model_type\n",
        "    }\n",
        "    \n",
        "    # Check GPU memory after loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üîç GPU Memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "    \n",
        "    print(f\"‚úÖ {model_name} loaded and ready on {device}\")\n",
        "    print(f\"üéØ Selected model: {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n",
        "    print(\"üîÑ Falling back to a smaller model...\")\n",
        "    \n",
        "    # Fallback to a more reliable model\n",
        "    fallback_path = \"microsoft/DialoGPT-medium\"\n",
        "    print(f\"üîÑ Loading fallback model: {fallback_path}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(fallback_path)\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fallback_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    MODEL_INFO = {\n",
        "        \"name\": \"DialoGPT-medium (Fallback)\",\n",
        "        \"path\": fallback_path,\n",
        "        \"choice\": \"fallback\"\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Fallback model loaded successfully\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pre-load the embedding model to avoid CUDA errors during Flask requests\n",
        "print(\"üîç Pre-loading embedding model to avoid CUDA memory conflicts...\")\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    # Check if there's enough GPU memory for the embedding model\n",
        "    if torch.cuda.is_available():\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "        allocated_memory = torch.cuda.memory_allocated()\n",
        "        free_memory = total_memory - allocated_memory\n",
        "        \n",
        "        print(f\"üîç Available GPU memory: {free_memory/1024**3:.2f} GB\")\n",
        "        \n",
        "        # If less than 2GB free, use CPU for embeddings\n",
        "        if free_memory < 2 * 1024**3:  # 2GB threshold\n",
        "            print(\"‚ö†Ô∏è Limited GPU memory - using CPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            EMBEDDING_DEVICE = 'cpu'\n",
        "        else:\n",
        "            print(\"‚úÖ Sufficient GPU memory - using GPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "            EMBEDDING_DEVICE = 'cuda'\n",
        "    else:\n",
        "        print(\"üì± No GPU available - using CPU for embedding model\")\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "        EMBEDDING_DEVICE = 'cpu'\n",
        "    \n",
        "    print(f\"‚úÖ Embedding model loaded on {EMBEDDING_DEVICE}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error pre-loading embedding model: {str(e)}\")\n",
        "    print(\"üîÑ Will load embedding model on-demand with CPU fallback\")\n",
        "    embedding_model = None\n",
        "    EMBEDDING_DEVICE = 'cpu'\n",
        "\n",
        "# Setup Supabase connection using Colab secrets\n",
        "print(\"üóÑÔ∏è Setting up Supabase connection using Colab secrets...\")\n",
        "print(\"üìã Required Colab secrets:\")\n",
        "print(\"   1. SUPABASE_URL - Your project URL (e.g., https://abc123.supabase.co)\")\n",
        "print(\"   2. SUPABASE_SERVICE_ROLE_KEY - Your service role key (NOT anon key)\")\n",
        "print(\"   3. NGROK_AUTH_TOKEN - Your ngrok authentication token\")\n",
        "print(\"\")\n",
        "print(\"üîë To set these secrets:\")\n",
        "print(\"   1. Click the üîë key icon in the left sidebar\")\n",
        "print(\"   2. Add the three secrets listed above\")\n",
        "print(\"   3. Re-run this cell\")\n",
        "print(\"\")\n",
        "\n",
        "try:\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    \n",
        "    print(\"‚úÖ Successfully retrieved secrets from Colab\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving secrets: {str(e)}\")\n",
        "    print(\"üîß Make sure you've added the required secrets in Colab:\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "    print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    raise\n",
        "\n",
        "# Validate the inputs\n",
        "if not supabase_url or not supabase_key:\n",
        "    raise ValueError(\"‚ùå Both Supabase URL and Service Role Key are required!\")\n",
        "\n",
        "if not supabase_url.startswith('https://'):\n",
        "    raise ValueError(\"‚ùå Supabase URL should start with 'https://'\")\n",
        "\n",
        "if not supabase_key.startswith('eyJ'):\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys typically start with 'eyJ'\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "    \n",
        "if len(supabase_key) < 100:\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys are typically very long (200+ characters)\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "\n",
        "try:\n",
        "    supabase = create_client(supabase_url, supabase_key)\n",
        "    print(\"‚úÖ Supabase client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Supabase client: {str(e)}\")\n",
        "    print(\"üîß Common issues:\")\n",
        "    print(\"   ‚Ä¢ Wrong API key type (use service_role, not anon)\")\n",
        "    print(\"   ‚Ä¢ Typo in URL or key\")\n",
        "    print(\"   ‚Ä¢ Key might be expired or regenerated\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"top_k\": 5,\n",
        "    \"similarity_threshold\": 0.5,\n",
        "    \"max_context_length\": 2000,\n",
        "    \"max_response_length\": 150,\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è RAG Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modular Service Architecture\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseService(ABC):\n",
        "    \"\"\"Base class for all services\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
        "    \n",
        "    async def run_in_executor(self, func, *args):\n",
        "        \"\"\"Run CPU-intensive tasks in thread pool\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return await loop.run_in_executor(self.executor, func, *args)\n",
        "\n",
        "class EmbedderService(BaseService):\n",
        "    \"\"\"Handles text embedding generation on CPU\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = None\n",
        "        self.device = 'cpu'\n",
        "    \n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize embedding model on CPU\"\"\"\n",
        "        if self.model is None:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            self.model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            print(\"‚úÖ EmbedderService: Model loaded on CPU\")\n",
        "    \n",
        "    async def embed_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embeddings for text\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run embedding on CPU thread pool\n",
        "        embedding = await self.run_in_executor(self.model.encode, [text])\n",
        "        return embedding[0].tolist()\n",
        "    \n",
        "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run batch embedding on CPU thread pool\n",
        "        embeddings = await self.run_in_executor(self.model.encode, texts)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "class RetrieverService(BaseService):\n",
        "    \"\"\"Handles document retrieval from Supabase\"\"\"\n",
        "    \n",
        "    def __init__(self, supabase_client, embedder_service: EmbedderService):\n",
        "        super().__init__()\n",
        "        self.supabase = supabase_client\n",
        "        self.embedder = embedder_service\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "    \n",
        "    async def retrieve_documents(self, query: str, top_k: int = None) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents using vector search\"\"\"\n",
        "        top_k = top_k or self.top_k\n",
        "        \n",
        "        # Generate query embedding\n",
        "        query_embedding = await self.embedder.embed_text(query)\n",
        "        \n",
        "        # Search Supabase\n",
        "        try:\n",
        "            result = self.supabase.rpc('search_embeddings', {\n",
        "                'query_embedding': query_embedding,\n",
        "                'match_threshold': self.similarity_threshold,\n",
        "                'match_count': top_k\n",
        "            }).execute()\n",
        "            \n",
        "            if result.data:\n",
        "                documents = []\n",
        "                for i, doc in enumerate(result.data):\n",
        "                    documents.append({\n",
        "                        'content': doc.get('chunk_content', ''),\n",
        "                        'similarity_score': doc.get('similarity', 0.0),\n",
        "                        'metadata': {\n",
        "                            'title': doc.get('title', 'Medical Document'),\n",
        "                            'source': doc.get('source', 'unknown'),\n",
        "                            'topic': doc.get('topic', 'general'),\n",
        "                            'document_type': doc.get('document_type', 'unknown'),\n",
        "                            'document_id': doc.get('document_id', '')\n",
        "                        },\n",
        "                        'rank': i + 1,\n",
        "                        'doc_id': doc.get('document_id', '')\n",
        "                    })\n",
        "                return documents\n",
        "            else:\n",
        "                return []\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RetrieverService error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class GeneratorService(BaseService):\n",
        "    \"\"\"Handles text generation using medical LLMs on GPU\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, tokenizer, medical_model):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.medical_model = medical_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    async def generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
        "        \"\"\"Generate medical response using the selected model\"\"\"\n",
        "        try:\n",
        "            # Run generation on GPU thread pool\n",
        "            response = await self.run_in_executor(self._generate_sync, prompt, max_tokens)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå GeneratorService error: {str(e)}\")\n",
        "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
        "    \n",
        "    def _generate_sync(self, prompt: str, max_tokens: int) -> str:\n",
        "        \"\"\"Synchronous generation method for thread pool\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "        \n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:\n",
        "            generation_params[\"pad_token_id\"] = self.tokenizer.pad_token_id\n",
        "        if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "            generation_params[\"eos_token_id\"] = self.tokenizer.eos_token_id\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.medical_model.generate(**inputs, **generation_params)\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "        return response.strip()\n",
        "\n",
        "class RAGController:\n",
        "    \"\"\"Orchestrates the complete RAG pipeline\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: EmbedderService, retriever: RetrieverService, generator: GeneratorService):\n",
        "        self.embedder = embedder\n",
        "        self.retriever = retriever\n",
        "        self.generator = generator\n",
        "        self.max_context_length = 2000\n",
        "    \n",
        "    async def process_query(self, question: str, history: List[dict] = None) -> dict:\n",
        "        \"\"\"Complete RAG pipeline: retrieve context and generate response\"\"\"\n",
        "        try:\n",
        "            print(f\"üîç Processing query: {question}\")\n",
        "            \n",
        "            # Step 1: Retrieve relevant documents\n",
        "            retrieved_docs = await self.retriever.retrieve_documents(question)\n",
        "            print(f\"üìä Found {len(retrieved_docs)} similar documents\")\n",
        "            \n",
        "            # Step 2: Build context from documents\n",
        "            context_parts = []\n",
        "            total_chars = 0\n",
        "            \n",
        "            for doc in retrieved_docs:\n",
        "                if total_chars + len(doc['content']) <= self.max_context_length:\n",
        "                    context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                    total_chars += len(doc['content'])\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            context = \"\\n\\n\".join(context_parts)\n",
        "            print(f\"ÔøΩÔøΩ Using {len(context_parts)} documents for context ({len(context)} chars)\")\n",
        "            \n",
        "            # Step 3: Create prompt with history\n",
        "            if history:\n",
        "                history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in history[-3:]])\n",
        "                prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {question}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "            else:\n",
        "                prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "            \n",
        "            # Step 4: Generate response\n",
        "            print(f\"ü§ñ Generating response using {self.generator.model_name}...\")\n",
        "            response = await self.generator.generate_response(prompt, 150)\n",
        "            \n",
        "            # Step 5: Format result\n",
        "            result = {\n",
        "                'query': question,\n",
        "                'response': response,\n",
        "                'sources': [\n",
        "                    {\n",
        "                        'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                        'source': doc['metadata']['source'],\n",
        "                        'topic': doc['metadata']['topic'],\n",
        "                        'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                        'rank': doc['rank'],\n",
        "                        'content_preview': doc['content'][:150] + \"...\"\n",
        "                    }\n",
        "                    for doc in retrieved_docs\n",
        "                ],\n",
        "                'metadata': {\n",
        "                    'documentsUsed': len(context_parts),\n",
        "                    'totalFound': len(retrieved_docs),\n",
        "                    'contextLength': len(context),\n",
        "                    'model': self.generator.model_name,\n",
        "                    'processingTime': datetime.now().isoformat()\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAGController error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Initialize services in correct order\n",
        "print(\"üîß Initializing modular services...\")\n",
        "embedder_service = EmbedderService()\n",
        "retriever_service = RetrieverService(supabase, embedder_service)\n",
        "generator_service = GeneratorService(MODEL_INFO['name'], tokenizer, medical_model)\n",
        "rag_controller = RAGController(embedder_service, retriever_service, generator_service)\n",
        "\n",
        "print(\"‚úÖ Modular services initialized!\")\n",
        "print(f\"   üìç EmbedderService: CPU-based embedding generation\")\n",
        "print(f\"   üìç RetrieverService: Supabase vector search\")\n",
        "print(f\"   üìç GeneratorService: GPU-based text generation\")\n",
        "print(f\"   üìç RAGController: Pipeline orchestration\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Supabase connection and RPC functions\n",
        "print(\"üß™ Testing Supabase connection...\")\n",
        "try:\n",
        "    # Test basic connection\n",
        "    test_result = supabase.table('medical_documents').select('count').execute()\n",
        "    doc_count = len(test_result.data) if test_result.data else 0\n",
        "    print(f\"‚úÖ Supabase connected - Found {doc_count} documents in database\")\n",
        "    \n",
        "    # Test RPC function availability\n",
        "    print(\"üß™ Testing RPC functions...\")\n",
        "    try:\n",
        "        stats_result = supabase.rpc('get_document_stats').execute()\n",
        "        if stats_result.data:\n",
        "            print(\"‚úÖ RPC functions working\")\n",
        "            for stat in stats_result.data[:3]:  # Show first 3 document sources\n",
        "                print(f\"   üìÑ {stat['source']}: {stat['count']} documents\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è RPC function exists but returned no data\")\n",
        "    except Exception as rpc_error:\n",
        "        print(f\"‚ö†Ô∏è RPC function test failed: {str(rpc_error)}\")\n",
        "        print(\"   Vector search will use fallback method\")\n",
        "        \n",
        "except Exception as e:\n",
        "    error_str = str(e)\n",
        "    print(f\"‚ö†Ô∏è Supabase connection test failed: {error_str}\")\n",
        "    \n",
        "    # Provide specific guidance based on error type\n",
        "    if '401' in error_str or 'Invalid API key' in error_str:\n",
        "        print(\"üîß AUTHENTICATION ERROR - Invalid API Key:\")\n",
        "        print(\"   ‚ùå You're using the wrong API key!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Go to your Supabase project dashboard\")\n",
        "        print(\"   2. Settings ‚Üí API\")\n",
        "        print(\"   3. Copy the 'service_role' key (NOT anon key)\")\n",
        "        print(\"   4. The service_role key is much longer and starts with 'eyJ'\")\n",
        "        print(\"   5. Re-run Cell 3 with the correct key\")\n",
        "        print(\"\")\n",
        "        print(\"   üîç Key differences:\")\n",
        "        print(\"   ‚Ä¢ anon key: Used for client-side apps (WRONG for this notebook)\")\n",
        "        print(\"   ‚Ä¢ service_role key: Used for server-side/admin access (CORRECT)\")\n",
        "    elif '404' in error_str:\n",
        "        print(\"üîß TABLE NOT FOUND:\")\n",
        "        print(\"   ‚ùå The 'medical_documents' table doesn't exist!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Run the schema.sql in your Supabase SQL editor\")\n",
        "        print(\"   2. Or run the embed_documents.py script to create tables\")\n",
        "    elif 'timeout' in error_str.lower():\n",
        "        print(\"üîß CONNECTION TIMEOUT:\")\n",
        "        print(\"   ‚ùå Can't reach Supabase servers\")\n",
        "        print(\"   üìã Check your internet connection and Supabase URL\")\n",
        "    else:\n",
        "        print(\"üîß GENERAL CONNECTION ERROR:\")\n",
        "        print(\"   üìã Common fixes:\")\n",
        "        print(\"   ‚Ä¢ Double-check your Supabase URL\")\n",
        "        print(\"   ‚Ä¢ Verify you're using service_role key (not anon)\")\n",
        "        print(\"   ‚Ä¢ Check if your project is paused/suspended\")\n",
        "        print(\"   ‚Ä¢ Ensure database tables exist\")\n",
        "    \n",
        "    print(\"\\n   ‚ö†Ô∏è The system will continue but may have limited document retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_medical_response(prompt: str, max_new_tokens: int = 150) -> str:\n",
        "    \"\"\"Generate medical response using the selected medical model\"\"\"\n",
        "    try:\n",
        "        print(f\"ü§ñ Generating response using {MODEL_INFO['name']}...\")\n",
        "        \n",
        "        # Check GPU memory before generation\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated()/1024**3\n",
        "            reserved = torch.cuda.memory_reserved()/1024**3\n",
        "            print(f\"üîç GPU Memory before generation: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
        "        \n",
        "        # Standard causal LM handling (models 1 and 2 are both causal models)\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        # Generation parameters optimized for medical models\n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_new_tokens,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        # Set tokens for generation\n",
        "        if hasattr(tokenizer, 'pad_token_id') and tokenizer.pad_token_id is not None:\n",
        "            generation_params[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "        if hasattr(tokenizer, 'eos_token_id') and tokenizer.eos_token_id is not None:\n",
        "            generation_params[\"eos_token_id\"] = tokenizer.eos_token_id\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                outputs = medical_model.generate(\n",
        "                    **inputs,\n",
        "                    **generation_params\n",
        "                )\n",
        "            except RuntimeError as cuda_error:\n",
        "                if \"out of memory\" in str(cuda_error).lower() or \"cuda\" in str(cuda_error).lower():\n",
        "                    print(f\"‚ö†Ô∏è CUDA memory error during generation: {str(cuda_error)}\")\n",
        "                    print(\"üßπ Clearing GPU cache and retrying...\")\n",
        "                    \n",
        "                    if torch.cuda.is_available():\n",
        "                        torch.cuda.empty_cache()\n",
        "                    \n",
        "                    # Retry with smaller parameters\n",
        "                    generation_params[\"max_new_tokens\"] = min(generation_params[\"max_new_tokens\"], 50)\n",
        "                    print(f\"üîÑ Retrying with reduced tokens: {generation_params['max_new_tokens']}\")\n",
        "                    \n",
        "                    outputs = medical_model.generate(\n",
        "                        **inputs,\n",
        "                        **generation_params\n",
        "                    )\n",
        "                else:\n",
        "                    raise cuda_error\n",
        "        \n",
        "        # Decode response (standard causal LM - decode only the generated part)\n",
        "        response = tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "        \n",
        "        response = response.strip()\n",
        "        \n",
        "        # Clean up response for medical context\n",
        "        # Remove common artifacts\n",
        "        response = response.replace(\"</s>\", \"\").replace(\"<s>\", \"\").strip()\n",
        "        \n",
        "        # Basic quality check\n",
        "        if not response or len(response) < 10:\n",
        "            response = \"I understand your question about health. Please consult with a healthcare professional for personalized medical advice.\"\n",
        "        \n",
        "        print(f\"‚úÖ Response generated successfully ({len(response)} characters)\")\n",
        "        return response\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation error: {str(e)}\")\n",
        "        return f\"I apologize, but I encountered an error processing your question. Please try rephrasing your question or consult with a healthcare professional.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced FastAPI Setup with Chat History Support\n",
        "app = FastAPI(\n",
        "    title=\"WellnessGrid RAG API\",\n",
        "    description=\"Medical AI Assistant with RAG capabilities using FastAPI\",\n",
        "    version=\"2.0.0\",\n",
        "    docs_url=\"/docs\",\n",
        "    redoc_url=\"/redoc\"\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Store chat sessions in memory (in production, use Redis or database)\n",
        "chat_sessions = {}\n",
        "\n",
        "@app.post(\"/embed\", response_model=EmbedResponse)\n",
        "async def generate_embedding(request: EmbedRequest):\n",
        "    \"\"\"Generate embeddings for text with enhanced error handling and GPU memory management\"\"\"\n",
        "    try:\n",
        "        global embedding_model, EMBEDDING_DEVICE\n",
        "        \n",
        "        logger.info(f\"üîç Generating embedding for text: {request.text[:100]}...\")\n",
        "        \n",
        "        # Use pre-loaded embedding model or load with CPU fallback\n",
        "        if embedding_model is None:\n",
        "            logger.info(\"üì• Loading embedding model on-demand with CPU fallback...\")\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                \n",
        "                # Clear GPU cache first to free up memory\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    logger.info(\"üßπ GPU cache cleared\")\n",
        "                \n",
        "                # Always use CPU for FastAPI requests to avoid CUDA conflicts\n",
        "                embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "                EMBEDDING_DEVICE = 'cpu'\n",
        "                logger.info(\"‚úÖ Embedding model loaded on CPU (safer for FastAPI)\")\n",
        "                \n",
        "            except Exception as load_error:\n",
        "                logger.error(f\"‚ùå Failed to load embedding model: {str(load_error)}\")\n",
        "                raise HTTPException(status_code=500, detail=f\"Failed to load embedding model: {str(load_error)}\")\n",
        "        \n",
        "        # Generate embedding\n",
        "        logger.info(f\"üß† Generating embedding on {EMBEDDING_DEVICE}...\")\n",
        "        embedding = embedding_model.encode([request.text])[0].tolist()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Generated embedding with {len(embedding)} dimensions\")\n",
        "        \n",
        "        return EmbedResponse(\n",
        "            embedding=embedding,\n",
        "            dimensions=len(embedding),\n",
        "            model=f\"PubMedBERT ({EMBEDDING_DEVICE})\",\n",
        "            device=EMBEDDING_DEVICE\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in embed endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise HTTPException(status_code=500, detail=f\"Embedding generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerateResponse)\n",
        "async def generate_text(request: GenerateRequest):\n",
        "    \"\"\"Enhanced generate endpoint using modular generator service\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üî¨ Generating response for query: {request.query[:100]}...\")\n",
        "        logger.info(f\"üìö Context length: {len(request.context)} characters\")\n",
        "        logger.info(f\"ÔøΩÔøΩ Chat history: {len(request.history)} messages\")\n",
        "        \n",
        "        # Create enhanced prompt with chat history\n",
        "        if request.history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in request.history[-3:]])\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Current question: {request.query}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Question: {request.query}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Generate response using NEW generator service\n",
        "        response = await generator_service.generate_response(prompt, request.max_tokens)\n",
        "        \n",
        "        return GenerateResponse(\n",
        "            answer=response,\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            context_used=len(request.context) > 0,\n",
        "            history_used=len(request.history) > 0\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in generate endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Text generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/query\", response_model=QueryResponse)\n",
        "async def query_docs(request: QueryRequest):\n",
        "    \"\"\"Enhanced query endpoint using modular retriever service\"\"\"\n",
        "    try:\n",
        "        # Use NEW retriever service\n",
        "        results = await retriever_service.retrieve_documents(request.query, request.top_k)\n",
        "        \n",
        "        return QueryResponse(\n",
        "            documents=results,\n",
        "            total_found=len(results),\n",
        "            query=request.query\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in query endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Document query failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/ask\", response_model=AskResponse)\n",
        "async def ask_rag(request: AskRequest):\n",
        "    \"\"\"Enhanced RAG endpoint using modular services\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"ü§ñ Processing RAG query: {request.question[:100]}...\")\n",
        "        logger.info(f\"üìù Session ID: {request.session_id}\")\n",
        "        \n",
        "        # Use session-specific history if no history provided\n",
        "        if not request.history and request.session_id in chat_sessions:\n",
        "            request.history = chat_sessions[request.session_id]\n",
        "        \n",
        "        # Get response using NEW modular RAG controller\n",
        "        result = await rag_controller.process_query(request.question, request.history)\n",
        "        \n",
        "        # Update chat history\n",
        "        new_message = {\"question\": request.question, \"answer\": result['response']}\n",
        "        if request.session_id not in chat_sessions:\n",
        "            chat_sessions[request.session_id] = []\n",
        "        chat_sessions[request.session_id].append(new_message)\n",
        "        \n",
        "        # Keep only last 10 messages to prevent memory issues\n",
        "        if len(chat_sessions[request.session_id]) > 10:\n",
        "            chat_sessions[request.session_id] = chat_sessions[request.session_id][-10:]\n",
        "        \n",
        "        return AskResponse(\n",
        "            response=result['response'],\n",
        "            sources=result['sources'],\n",
        "            chat_history=chat_sessions[request.session_id],\n",
        "            session_id=request.session_id,\n",
        "            mockMode=False,\n",
        "            metadata=result['metadata']\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in ask endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise HTTPException(\n",
        "            status_code=500, \n",
        "            detail=f\"I apologize, but I encountered an error processing your question: {str(e)}\"\n",
        "        )\n",
        "\n",
        "@app.get(\"/chat/history/{session_id}\")\n",
        "async def get_chat_history(session_id: str):\n",
        "    \"\"\"Get chat history for a session\"\"\"\n",
        "    try:\n",
        "        history = chat_sessions.get(session_id, [])\n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"history\": history,\n",
        "            \"message_count\": len(history)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat/clear/{session_id}\")\n",
        "async def clear_chat_history(session_id: str):\n",
        "    \"\"\"Clear chat history for a session\"\"\"\n",
        "    try:\n",
        "        if session_id in chat_sessions:\n",
        "            del chat_sessions[session_id]\n",
        "        \n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"cleared\": True\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health_check():\n",
        "    \"\"\"Enhanced health check with memory monitoring\"\"\"\n",
        "    try:\n",
        "        # Get memory status\n",
        "        memory_status = await memory_manager.get_memory_status()\n",
        "        \n",
        "        # Test Supabase connection\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        \n",
        "        # Test embeddings\n",
        "        embed_result = supabase.table('document_embeddings').select('count').execute()\n",
        "        embed_count = len(embed_result.data) if embed_result.data else 0\n",
        "        \n",
        "        return HealthResponse(\n",
        "            status=\"healthy\",\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            embedding_device=EMBEDDING_DEVICE if 'EMBEDDING_DEVICE' in globals() else 'unknown',\n",
        "            database=\"Supabase + pgvector\",\n",
        "            documents_in_db=doc_count,\n",
        "            embeddings_in_db=embed_count,\n",
        "            rag_system=\"enhanced_modular\",\n",
        "            chat_support=True,\n",
        "            active_sessions=len(chat_sessions),\n",
        "            gpu_memory=memory_status\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=503, detail=str(e))\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def status():\n",
        "    \"\"\"Detailed status endpoint\"\"\"\n",
        "    try:\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"models\": {\n",
        "                \"selected_model\": MODEL_INFO['name'] if 'MODEL_INFO' in globals() else \"not_selected\",\n",
        "                \"medical_model\": \"loaded\" if 'medical_model' in globals() else \"not_loaded\",\n",
        "                \"pubmedbert\": \"available\"\n",
        "            },\n",
        "            \"database\": {\n",
        "                \"connected\": True,\n",
        "                \"url\": supabase_url[:30] + \"...\" if supabase_url else \"not_set\"\n",
        "            },\n",
        "            \"config\": CONFIG,\n",
        "            \"memory\": {\n",
        "                \"active_sessions\": len(chat_sessions),\n",
        "                \"session_ids\": list(chat_sessions.keys())\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "print(\"üåê Enhanced FastAPI endpoints configured:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings with enhanced error handling\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} + chat history\")\n",
        "print(\"  ‚úÖ POST /ask - Enhanced RAG endpoint with chat history\")\n",
        "print(\"  ‚úÖ GET /health - Enhanced health check\")\n",
        "print(\"  ‚úÖ POST /query - Enhanced document query\")\n",
        "print(\"  ‚úÖ GET /chat/history/{session_id} - Get chat history\")\n",
        "print(\"  ‚úÖ POST /chat/clear/{session_id} - Clear chat history\")\n",
        "print(\"  ‚úÖ GET /status - Detailed status information\")\n",
        "print(\"  ‚úÖ GET /docs - Auto-generated API documentation\")\n",
        "print(\"  ‚úÖ GET /redoc - Alternative API documentation\")\n",
        "print(f\"‚úÖ Enhanced FastAPI server ready with {MODEL_INFO['name']} and chat history support!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG system\n",
        "print(\"ÔøΩÔøΩ Testing RAG system with sample question...\")\n",
        "\n",
        "test_question = \"What are the symptoms of diabetes?\"\n",
        "try:\n",
        "    print(f\"üîç Testing query: {test_question}\")\n",
        "    # Use NEW modular controller\n",
        "    result = await rag_controller.process_query(test_question)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚ùì QUESTION: {result['query']}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\nÔøΩÔøΩ AI RESPONSE:\")\n",
        "    print(f\"{result['response']}\")\n",
        "    \n",
        "    print(f\"\\nÔøΩÔøΩ SOURCES ({result['metadata']['documentsUsed']} documents):\")\n",
        "    if result['sources']:\n",
        "        for i, source in enumerate(result['sources'], 1):\n",
        "            print(f\"   {i}. {source['title']} - {source['source']}\")\n",
        "            print(f\"      üìä Similarity: {source['similarity']}\")\n",
        "            print(f\"      ÔøΩÔøΩ Preview: {source['content_preview']}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è No sources found - this could indicate:\")\n",
        "        print(\"   ‚Ä¢ No documents in database yet\")\n",
        "        print(\"   ‚Ä¢ Similarity threshold too high\")\n",
        "        print(\"   ‚Ä¢ RPC function needs adjustment\")\n",
        "    \n",
        "    print(f\"\\nüìä Metadata:\")\n",
        "    print(f\"   üîß Model: {result['metadata']['model']}\")\n",
        "    print(f\"   üìÑ Documents Used: {result['metadata']['documentsUsed']}\")\n",
        "    print(f\"   üéØ Total Found: {result['metadata']['totalFound']}\")\n",
        "    \n",
        "    print(\"‚úÖ RAG system test completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è RAG test failed: {str(e)}\")\n",
        "    print(\"   This might be normal if:\")\n",
        "    print(\"   ‚Ä¢ Supabase connection needs adjustment\")\n",
        "    print(\"   ‚Ä¢ No documents have been embedded yet\")\n",
        "    print(\"   ‚Ä¢ RPC function is not deployed\")\n",
        "    print(\"   The FastAPI server will still start and you can test via the API\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Memory Management Utilities\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Get detailed GPU memory information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"No GPU available\"\n",
        "    \n",
        "    device = torch.cuda.current_device()\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "    total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "    free = total - allocated\n",
        "    \n",
        "    return {\n",
        "        \"allocated\": f\"{allocated:.2f} GB\",\n",
        "        \"reserved\": f\"{reserved:.2f} GB\", \n",
        "        \"total\": f\"{total:.2f} GB\",\n",
        "        \"free\": f\"{free:.2f} GB\",\n",
        "        \"percentage_used\": f\"{(allocated/total)*100:.1f}%\"\n",
        "    }\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"üßπ GPU cache cleared\")\n",
        "    else:\n",
        "        print(\"üì± No GPU to clear\")\n",
        "\n",
        "# Show current GPU memory status\n",
        "print(\"üîç Current GPU Memory Status:\")\n",
        "memory_info = get_gpu_memory_info()\n",
        "if isinstance(memory_info, dict):\n",
        "    for key, value in memory_info.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "else:\n",
        "    print(f\"   {memory_info}\")\n",
        "\n",
        "# Add GPU memory management to Flask health endpoint\n",
        "print(\"‚úÖ GPU memory utilities ready!\")\n",
        "print(\"   Use get_gpu_memory_info() to check memory\")\n",
        "print(\"   Use clear_gpu_memory() to free cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ngrok setup and FastAPI server startup\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Quick pre-check to give immediate feedback\n",
        "print(\"üîç Pre-flight check...\")\n",
        "required_vars = ['medical_model', 'tokenizer', 'supabase', 'rag_system', 'CONFIG', 'MODEL_INFO']\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"‚ùå Missing required variables: {', '.join(missing_vars)}\")\n",
        "    print(f\"üîß Please run all previous cells (1-7) in order first!\")\n",
        "    print(f\"   Then come back to this cell.\")\n",
        "    raise RuntimeError(f\"Required setup incomplete. Missing: {', '.join(missing_vars)}\")\n",
        "\n",
        "print(\"‚úÖ Pre-flight check passed!\")\n",
        "print(f\"üéØ Selected model: {MODEL_INFO['name']}\")\n",
        "\n",
        "print(\"üîë Using ngrok auth token from Colab secrets...\")\n",
        "\n",
        "# Use the token we already retrieved in Cell 3\n",
        "if 'ngrok_token' not in globals() or not ngrok_token:\n",
        "    print(\"‚ùå Ngrok token not found in secrets!\")\n",
        "    print(\"üîß Make sure you've added NGROK_AUTH_TOKEN to Colab secrets\")\n",
        "    raise ValueError(\"Missing NGROK_AUTH_TOKEN in Colab secrets\")\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"‚úÖ Ngrok token set successfully!\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "print(\"üåê Starting ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8000)  # Changed from 5000 to 8000 for FastAPI\n",
        "print(f\"üåç Public URL: {public_url}\")\n",
        "print(\"üìã Copy this URL to your WellnessGrid app configuration!\")\n",
        "\n",
        "# Start FastAPI app with uvicorn\n",
        "print(\"üöÄ Starting FastAPI app with uvicorn...\")\n",
        "print(\"üì° Available endpoints:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings (required by WellnessGrid)\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} (required by WellnessGrid)\")\n",
        "print(\"  ‚úÖ POST /ask - Main RAG endpoint for WellnessGrid\")\n",
        "print(\"  ‚úÖ GET /health - Health check\")\n",
        "print(\"  ‚úÖ POST /query - Query documents from Supabase\")\n",
        "print(\"  ‚úÖ GET /docs - Interactive API documentation\")\n",
        "print(\"  ‚úÖ GET /redoc - Alternative API documentation\")\n",
        "print(\"\\nüéØ IMPORTANT: Copy the ngrok URL above to your WellnessGrid .env.local:\")\n",
        "print(\"   FLASK_API_URL=https://your-ngrok-id.ngrok.io\")\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
        "print(f\"\\nüöÄ Your WellnessGrid RAG system with {MODEL_INFO['name']} is now live!\")\n",
        "\n",
        "# Run the FastAPI app with uvicorn\n",
        "import uvicorn\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick Test of Flask Endpoints (Optional)\n",
        "# Run this AFTER starting the Flask server in Cell 8\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def test_flask_endpoints():\n",
        "    \"\"\"Test the Flask endpoints locally\"\"\"\n",
        "    base_url = \"http://localhost:5000\"\n",
        "    \n",
        "    print(\"üß™ Testing Flask endpoints locally...\")\n",
        "    print(\"‚ö†Ô∏è Make sure Cell 8 (Flask server) is running first!\\n\")\n",
        "    \n",
        "    # Test 1: Health check\n",
        "    try:\n",
        "        print(\"1. Testing /health endpoint...\")\n",
        "        response = requests.get(f\"{base_url}/health\", timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Health check passed\")\n",
        "            print(f\"   Response: {response.json()}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Health check failed: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Health check error: {str(e)}\")\n",
        "        print(\"   Make sure Flask server is running (Cell 8)\")\n",
        "        return\n",
        "    \n",
        "    # Test 2: Embedding endpoint\n",
        "    try:\n",
        "        print(\"\\n2. Testing /embed endpoint...\")\n",
        "        test_data = {\"text\": \"What is diabetes?\"}\n",
        "        response = requests.post(f\"{base_url}/embed\", \n",
        "                               json=test_data, \n",
        "                               headers={\"Content-Type\": \"application/json\"},\n",
        "                               timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            embedding = result.get('embedding', [])\n",
        "            print(f\"‚úÖ Embedding test passed\")\n",
        "            print(f\"   Embedding dimensions: {len(embedding)}\")\n",
        "            print(f\"   First 3 values: {embedding[:3]}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Embedding test failed: {response.status_code}\")\n",
        "            print(f\"   Error: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Embedding test error: {str(e)}\")\n",
        "    \n",
        "    # Test 3: Generation endpoint\n",
        "    try:\n",
        "        print(\"\\n3. Testing /generate endpoint...\")\n",
        "        test_data = {\n",
        "            \"query\": \"What is diabetes?\",\n",
        "            \"context\": \"Diabetes is a chronic condition affecting blood sugar levels.\",\n",
        "            \"max_tokens\": 50,\n",
        "            \"temperature\": 0.7\n",
        "        }\n",
        "        response = requests.post(f\"{base_url}/generate\", \n",
        "                               json=test_data, \n",
        "                               headers={\"Content-Type\": \"application/json\"},\n",
        "                               timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            answer = result.get('answer', '')\n",
        "            print(f\"‚úÖ Generation test passed\")\n",
        "            print(f\"   Answer length: {len(answer)} characters\")\n",
        "            print(f\"   Answer preview: {answer[:100]}...\")\n",
        "        else:\n",
        "            print(f\"‚ùå Generation test failed: {response.status_code}\")\n",
        "            print(f\"   Error: {response.text}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Generation test error: {str(e)}\")\n",
        "    \n",
        "    print(\"\\nüéØ Test completed!\")\n",
        "    print(\"If all tests pass, your Flask server is ready for WellnessGrid!\")\n",
        "\n",
        "# Uncomment the line below to run the test\n",
        "# test_flask_endpoints()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
