{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# RAG Query System for WellnessGrid - Supabase Integration\n",
        "\n",
        "This notebook demonstrates how to query the medical knowledge base stored in Supabase and integrates with your existing Flask backend. It performs RAG (Retrieval-Augmented Generation) by:\n",
        "\n",
        "1. üîç Converting user questions to embeddings using PubMedBERT\n",
        "2. üóÑÔ∏è Finding similar document chunks from Supabase pgvector database  \n",
        "3. ü§ñ Using your existing Flask backend for AI response generation\n",
        "4. üìö Providing source citations and confidence scores\n",
        "\n",
        "## Features:\n",
        "- **Supabase pgvector**: Fast similarity search using PostgreSQL with pgvector\n",
        "- **Flask Backend Integration**: Uses your existing `/embed` and `/generate` endpoints\n",
        "- **Context Retrieval**: Top-k most relevant document chunks with HNSW indexing\n",
        "- **Source Attribution**: Track which documents informed the response\n",
        "- **Compatible with Next.js**: Works with your current `/api/ask` route\n",
        "\n",
        "## Integration with Your Current Setup:\n",
        "- ‚úÖ **No changes to Flask backend needed**\n",
        "- ‚úÖ **No changes to Next.js frontend needed** \n",
        "- ‚úÖ **Drop-in replacement for Chroma**\n",
        "- ‚úÖ **Uses your existing BioGPT/BioBERT models**\n",
        "\n",
        "**‚ö° Run this notebook for querying your Supabase medical knowledge base**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Supabase integration\n",
        "%pip install sentence-transformers pandas numpy requests --quiet\n",
        "%pip install supabase psycopg2-binary python-dotenv --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Vector embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Supabase integration\n",
        "from supabase import create_client, Client\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê Query session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"üîÑ Using Supabase pgvector instead of Chroma!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Configuration & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Supabase and Flask Backend Integration\n",
        "CONFIG = {\n",
        "    \"embedding_model\": \"NeuML/pubmedbert-base-embeddings\",\n",
        "    \"table_name\": \"medical_documents\",\n",
        "    \"embeddings_table\": \"document_embeddings\", \n",
        "    \"top_k\": 5,  # Number of similar documents to retrieve\n",
        "    \"similarity_threshold\": 0.5,  # Minimum similarity score to consider\n",
        "    \"max_context_length\": 2000,  # Maximum characters for context\n",
        "    \"use_flask_backend\": True,  # Use your existing Flask backend\n",
        "}\n",
        "\n",
        "# Supabase configuration\n",
        "SUPABASE_CONFIG = {\n",
        "    \"url\": os.getenv(\"NEXT_PUBLIC_SUPABASE_URL\"),\n",
        "    \"key\": os.getenv(\"NEXT_PUBLIC_SUPABASE_ANON_KEY\"),\n",
        "}\n",
        "\n",
        "# Flask backend configuration (your existing setup)\n",
        "FLASK_CONFIG = {\n",
        "    \"api_url\": os.getenv(\"FLASK_API_URL\", \"http://localhost:5000\"),\n",
        "    \"embed_endpoint\": \"/embed\",\n",
        "    \"generate_endpoint\": \"/generate\"\n",
        "}\n",
        "\n",
        "# Validate configuration\n",
        "if not SUPABASE_CONFIG[\"url\"] or not SUPABASE_CONFIG[\"key\"]:\n",
        "    print(\"‚ö†Ô∏è WARNING: Supabase credentials not found!\")\n",
        "    print(\"Please set NEXT_PUBLIC_SUPABASE_URL and NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
        "else:\n",
        "    print(\"‚úÖ Supabase credentials loaded\")\n",
        "\n",
        "if not FLASK_CONFIG[\"api_url\"]:\n",
        "    print(\"‚ö†Ô∏è WARNING: Flask API URL not found!\")\n",
        "    print(\"Please set FLASK_API_URL in your environment\")\n",
        "    CONFIG[\"use_flask_backend\"] = False\n",
        "else:\n",
        "    print(f\"‚úÖ Flask backend configured: {FLASK_CONFIG['api_url']}\")\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è Query Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")\n",
        "print(f\"   ü§ñ Use Flask backend: {CONFIG['use_flask_backend']}\")\n",
        "print(f\"   üóÑÔ∏è Supabase tables: {CONFIG['table_name']}, {CONFIG['embeddings_table']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Load Models & Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load PubMedBERT for query embeddings\n",
        "print(\"üß† Loading PubMedBERT model...\")\n",
        "embedding_model = SentenceTransformer(CONFIG['embedding_model'])\n",
        "print(f\"‚úÖ Embedding model loaded: {CONFIG['embedding_model']}\")\n",
        "\n",
        "# Connect to Supabase database\n",
        "print(\"üóÑÔ∏è Connecting to Supabase database...\")\n",
        "if not SUPABASE_CONFIG[\"url\"] or not SUPABASE_CONFIG[\"key\"]:\n",
        "    raise ValueError(\"Supabase credentials not found. Please set NEXT_PUBLIC_SUPABASE_URL and NEXT_PUBLIC_SUPABASE_ANON_KEY\")\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_CONFIG[\"url\"], SUPABASE_CONFIG[\"key\"])\n",
        "\n",
        "# Test connection and get database stats\n",
        "try:\n",
        "    docs_result = supabase.table(CONFIG['table_name']).select('*', count='exact').execute()\n",
        "    embeddings_result = supabase.table(CONFIG['embeddings_table']).select('*', count='exact').execute()\n",
        "    \n",
        "    print(f\"‚úÖ Connected to Supabase successfully!\")\n",
        "    print(f\"üìä Database contains:\")\n",
        "    print(f\"   üìÑ {docs_result.count} documents\")\n",
        "    print(f\"   üîç {embeddings_result.count} embedded chunks\")\n",
        "    \n",
        "    if docs_result.count == 0:\n",
        "        print(\"‚ö†Ô∏è No documents found! Make sure to run embed_documents.ipynb first.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to connect to Supabase: {e}\")\n",
        "    raise\n",
        "\n",
        "# Test Flask backend connection (optional)\n",
        "flask_available = False\n",
        "if CONFIG['use_flask_backend']:\n",
        "    try:\n",
        "        print(f\"üß™ Testing Flask backend connection...\")\n",
        "        health_url = f\"{FLASK_CONFIG['api_url']}/health\"\n",
        "        response = requests.get(health_url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"‚úÖ Flask backend is available at {FLASK_CONFIG['api_url']}\")\n",
        "            flask_available = True\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Flask backend responded with status {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Flask backend not available: {e}\")\n",
        "        print(\"   Will proceed with context retrieval only\")\n",
        "\n",
        "CONFIG['flask_available'] = flask_available\n",
        "print(\"‚úÖ Setup complete!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## RAG Query System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WellnessRAGSystem:\n",
        "    \"\"\"RAG system for medical/wellness queries using Supabase and Flask backend\"\"\"\n",
        "    \n",
        "    def __init__(self, embedding_model, supabase_client, config, flask_config):\n",
        "        self.embedding_model = embedding_model\n",
        "        self.supabase = supabase_client\n",
        "        self.config = config\n",
        "        self.flask_config = flask_config\n",
        "        \n",
        "    def retrieve_context(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Retrieve relevant document chunks from Supabase using pgvector similarity search\"\"\"\n",
        "        \n",
        "        # Generate query embedding using local PubMedBERT model\n",
        "        query_embedding = self.embedding_model.encode([query])[0].tolist()\n",
        "        \n",
        "        # Perform vector similarity search using Supabase RPC function\n",
        "        # This uses the pgvector extension for fast similarity search\n",
        "        try:\n",
        "            # Use RPC function for vector similarity search\n",
        "            rpc_result = self.supabase.rpc(\n",
        "                'search_embeddings',\n",
        "                {\n",
        "                    'query_embedding': query_embedding,\n",
        "                    'match_threshold': self.config['similarity_threshold'],\n",
        "                    'match_count': self.config['top_k']\n",
        "                }\n",
        "            ).execute()\n",
        "            \n",
        "            if rpc_result.data:\n",
        "                search_results = rpc_result.data\n",
        "            else:\n",
        "                # Fallback: manual similarity search if RPC not available\n",
        "                print(\"‚ö†Ô∏è RPC function not available, using fallback search...\")\n",
        "                search_results = self._fallback_similarity_search(query_embedding)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Vector search error: {e}\")\n",
        "            # Fallback search\n",
        "            search_results = self._fallback_similarity_search(query_embedding)\n",
        "        \n",
        "        # Process results\n",
        "        retrieved_docs = []\n",
        "        for i, result in enumerate(search_results):\n",
        "            similarity_score = result.get('similarity', 0)\n",
        "            \n",
        "            if similarity_score >= self.config['similarity_threshold']:\n",
        "                retrieved_docs.append({\n",
        "                    'content': result.get('chunk_content', ''),\n",
        "                    'metadata': {\n",
        "                        'source': result.get('source', 'Unknown'),\n",
        "                        'topic': result.get('topic', 'Unknown'),\n",
        "                        'title': result.get('title', 'Unknown'),\n",
        "                        'document_type': result.get('document_type', 'Unknown')\n",
        "                    },\n",
        "                    'similarity_score': similarity_score,\n",
        "                    'rank': i + 1\n",
        "                })\n",
        "        \n",
        "        # Prepare context string\n",
        "        context_parts = []\n",
        "        total_chars = 0\n",
        "        \n",
        "        for doc in retrieved_docs:\n",
        "            if total_chars + len(doc['content']) <= self.config['max_context_length']:\n",
        "                context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                total_chars += len(doc['content'])\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'context': context,\n",
        "            'retrieved_documents': retrieved_docs,\n",
        "            'total_documents_found': len(search_results),\n",
        "            'documents_used': len(retrieved_docs),\n",
        "            'context_length': len(context)\n",
        "        }\n",
        "    \n",
        "    def _fallback_similarity_search(self, query_embedding: List[float]) -> List[Dict]:\n",
        "        \"\"\"Fallback similarity search when RPC function is not available\"\"\"\n",
        "        try:\n",
        "            # Get all embeddings (this is not efficient for large datasets)\n",
        "            # In production, you'd want to implement proper vector search\n",
        "            embeddings_result = self.supabase.table('document_embeddings').select(\n",
        "                'chunk_content, embedding, document_id'\n",
        "            ).limit(1000).execute()  # Limit to prevent memory issues\n",
        "            \n",
        "            if not embeddings_result.data:\n",
        "                return []\n",
        "            \n",
        "            # Calculate similarities manually (not recommended for production)\n",
        "            similarities = []\n",
        "            for row in embeddings_result.data:\n",
        "                try:\n",
        "                    embedding = np.array(row['embedding'])\n",
        "                    query_emb = np.array(query_embedding)\n",
        "                    \n",
        "                    # Cosine similarity\n",
        "                    similarity = np.dot(query_emb, embedding) / (\n",
        "                        np.linalg.norm(query_emb) * np.linalg.norm(embedding)\n",
        "                    )\n",
        "                    \n",
        "                    similarities.append({\n",
        "                        'chunk_content': row['chunk_content'],\n",
        "                        'similarity': float(similarity),\n",
        "                        'document_id': row['document_id']\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    continue\n",
        "            \n",
        "            # Sort by similarity and return top results\n",
        "            similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "            return similarities[:self.config['top_k']]\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Fallback search failed: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def generate_response_flask(self, query: str, context: str) -> Optional[str]:\n",
        "        \"\"\"Generate response using your existing Flask backend\"\"\"\n",
        "        if not self.config.get('flask_available', False):\n",
        "            return None\n",
        "            \n",
        "        try:\n",
        "            # Call your existing Flask /generate endpoint\n",
        "            generate_url = f\"{self.flask_config['api_url']}{self.flask_config['generate_endpoint']}\"\n",
        "            \n",
        "            payload = {\n",
        "                'query': query,\n",
        "                'context': context,\n",
        "                'max_tokens': 200,\n",
        "                'temperature': 0.7\n",
        "            }\n",
        "            \n",
        "            response = requests.post(\n",
        "                generate_url,\n",
        "                json=payload,\n",
        "                timeout=30,\n",
        "                headers={'Content-Type': 'application/json'}\n",
        "            )\n",
        "            \n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                return result.get('answer', '')\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Flask backend error: {response.status_code}\")\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error calling Flask backend: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Complete RAG query: retrieve context and optionally generate response\"\"\"\n",
        "        print(f\"üîç Processing query: {question}\")\n",
        "        \n",
        "        # Step 1: Retrieve relevant context\n",
        "        context_result = self.retrieve_context(question)\n",
        "        \n",
        "        print(f\"üìä Found {context_result['total_documents_found']} similar documents\")\n",
        "        print(f\"üìÑ Using {context_result['documents_used']} documents for context\")\n",
        "        \n",
        "        # Step 2: Generate response using Flask backend (optional)\n",
        "        generated_response = None\n",
        "        if self.config.get('flask_available', False):\n",
        "            print(\"ü§ñ Generating response using Flask backend...\")\n",
        "            generated_response = self.generate_response_flask(\n",
        "                question, \n",
        "                context_result['context']\n",
        "            )\n",
        "            \n",
        "            if generated_response:\n",
        "                print(\"‚úÖ Response generated successfully\")\n",
        "            else:\n",
        "                print(\"‚ö†Ô∏è Response generation failed\")\n",
        "        \n",
        "        # Prepare final result\n",
        "        result = {\n",
        "            'query': question,\n",
        "            'answer': generated_response or \"Context retrieved successfully. Enable Flask backend for AI responses.\",\n",
        "            'sources': [\n",
        "                {\n",
        "                    'title': doc['metadata']['title'],\n",
        "                    'source': doc['metadata']['source'],\n",
        "                    'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                    'rank': doc['rank']\n",
        "                }\n",
        "                for doc in context_result['retrieved_documents']\n",
        "            ],\n",
        "            'metadata': {\n",
        "                'documentsUsed': context_result['documents_used'],\n",
        "                'totalFound': context_result['total_documents_found'],\n",
        "                'contextLength': context_result['context_length'],\n",
        "                'flaskBackendUsed': bool(generated_response),\n",
        "                'processingTime': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "                 return result\n",
        "        \n",
        "        context = \"\\\\n\\\\n\".join(context_parts)\n",
        "        \n",
        "        return {\n",
        "            'query': query,\n",
        "            'context': context,\n",
        "            'retrieved_documents': retrieved_docs,\n",
        "            'num_retrieved': len(retrieved_docs),\n",
        "            'context_length': len(context)\n",
        "        }\n",
        "    \n",
        "    def generate_response(self, query: str, context: str) -> str:\n",
        "        \"\"\"Generate response using the language model\"\"\"\n",
        "        if not CONFIG['generate_response'] or 'model' not in globals():\n",
        "            return \"Response generation not available. Context retrieval completed.\"\n",
        "        \n",
        "        try:\n",
        "            # Create medical prompt\n",
        "            prompt = f\"\"\"Based on the following medical information, provide a helpful and accurate response to the user's question.\n",
        "\n",
        "Medical Context:\n",
        "{context}\n",
        "\n",
        "User Question: {query}\n",
        "\n",
        "Response:\"\"\"\n",
        "            \n",
        "            # Generate response\n",
        "            inputs = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "            inputs = inputs.to(model.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs,\n",
        "                    max_length=inputs.shape[1] + 150,\n",
        "                    num_return_sequences=1,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "            \n",
        "            # Decode response\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Extract only the generated part\n",
        "            if \"Response:\" in response:\n",
        "                response = response.split(\"Response:\")[-1].strip()\n",
        "            \n",
        "            # Clean up response\n",
        "            if not response or len(response) < 10:\n",
        "                response = \"Based on the available medical information, I recommend consulting with a healthcare professional for personalized advice.\"\n",
        "            \n",
        "            return response\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"I apologize, but I encountered an error generating a response. Please consult with a healthcare professional. Error: {str(e)}\"\n",
        "    \n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main query method - retrieves context and generates response\"\"\"\n",
        "        print(f\"üîç Processing query: '{question}'\")\n",
        "        \n",
        "        # Step 1: Retrieve relevant context\n",
        "        retrieval_result = self.retrieve_context(question)\n",
        "        print(f\"üìö Retrieved {retrieval_result['num_retrieved']} relevant documents\")\n",
        "        print(f\"üìè Context length: {retrieval_result['context_length']} characters\")\n",
        "        \n",
        "        # Step 2: Generate response (if enabled)\n",
        "        if CONFIG['generate_response']:\n",
        "            print(\"ü§ñ Generating response...\")\n",
        "            response = self.generate_response(question, retrieval_result['context'])\n",
        "        else:\n",
        "            response = \"Response generation disabled. Please review the retrieved context below.\"\n",
        "        \n",
        "        # Prepare final result\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'response': response,\n",
        "            'context': retrieval_result['context'],\n",
        "            'sources': [\n",
        "                {\n",
        "                    'source': doc['metadata']['source'],\n",
        "                    'topic': doc['metadata']['topic'],\n",
        "                    'similarity': round(doc['similarity_score'], 3),\n",
        "                    'content_preview': doc['content'][:150] + \"...\"\n",
        "                }\n",
        "                for doc in retrieval_result['retrieved_documents']\n",
        "            ],\n",
        "            'metadata': {\n",
        "                'num_sources': len(retrieval_result['retrieved_documents']),\n",
        "                'context_length': retrieval_result['context_length'],\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "\n",
        "# Initialize the RAG system\n",
        "rag_system = WellnessRAGSystem(embedding_model, collection, CONFIG)\n",
        "print(\"‚úÖ RAG system initialized and ready for queries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the RAG system\n",
        "rag_system = WellnessRAGSystem(\n",
        "    embedding_model=embedding_model,\n",
        "    supabase_client=supabase,\n",
        "    config=CONFIG,\n",
        "    flask_config=FLASK_CONFIG\n",
        ")\n",
        "\n",
        "print(\"üéØ WellnessGrid RAG system initialized!\")\n",
        "print(\"Ready to answer medical questions using Supabase + Flask backend integration\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Test Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG system with sample medical questions\n",
        "test_questions = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How can I manage high blood pressure?\",\n",
        "    \"What are the risk factors for heart disease?\",\n",
        "    \"How do I know if I have depression?\",\n",
        "    \"What foods should I eat for better nutrition?\",\n",
        "    \"What are the side effects of metformin?\"\n",
        "]\n",
        "\n",
        "def display_rag_result(result: Dict[str, Any]):\n",
        "    \"\"\"Display RAG query results in a nice format\"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚ùì QUESTION: {result['question']}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\\\nü§ñ AI RESPONSE:\")\n",
        "    print(f\"{result['response']}\")\n",
        "    \n",
        "    print(f\"\\\\nüìö SOURCES ({result['metadata']['num_sources']} documents):\")\n",
        "    for i, source in enumerate(result['sources'], 1):\n",
        "        print(f\"   {i}. {source['source']} - {source['topic']}\")\n",
        "        print(f\"      üìä Similarity: {source['similarity']:.1%}\")\n",
        "        print(f\"      üìÑ Preview: {source['content_preview']}\")\n",
        "        print()\n",
        "    \n",
        "    print(f\"üìã METADATA:\")\n",
        "    print(f\"   üîç Context length: {result['metadata']['context_length']} chars\")\n",
        "    print(f\"   ‚è∞ Generated at: {result['metadata']['timestamp']}\")\n",
        "    print(\"\\\\n\")\n",
        "\n",
        "# Test with a few questions\n",
        "print(\"üß™ Testing RAG system with sample questions...\\\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions[:3], 1):  # Test first 3 questions\n",
        "    print(f\"\\\\nüìã Test {i}/{len(test_questions[:3])}\")\n",
        "    result = rag_system.query(question)\n",
        "    display_rag_result(result)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Interactive Query Interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive query interface\n",
        "def interactive_query():\n",
        "    \"\"\"Interactive interface for asking medical questions\"\"\"\n",
        "    print(\"ü©∫ WellnessGrid RAG System - Interactive Query Interface\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Ask any medical or wellness question. Type 'quit' to exit.\")\n",
        "    print(\"Example questions:\")\n",
        "    for q in test_questions[:3]:\n",
        "        print(f\"  ‚Ä¢ {q}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    while True:\n",
        "        try:\n",
        "            # Get user input\n",
        "            question = input(\"\\\\n‚ùì Your question: \").strip()\n",
        "            \n",
        "            if question.lower() in ['quit', 'exit', 'q']:\n",
        "                print(\"üëã Thank you for using WellnessGrid RAG system!\")\n",
        "                break\n",
        "            \n",
        "            if not question:\n",
        "                print(\"‚ö†Ô∏è Please enter a question.\")\n",
        "                continue\n",
        "                \n",
        "            # Process query\n",
        "            print(\"\\\\nüîÑ Processing your question...\")\n",
        "            result = rag_system.query(question)\n",
        "            \n",
        "            # Display results\n",
        "            display_rag_result(result)\n",
        "            \n",
        "            # Ask if user wants to continue\n",
        "            continue_query = input(\"‚ùì Ask another question? (y/n): \").strip().lower()\n",
        "            if continue_query in ['n', 'no']:\n",
        "                print(\"üëã Thank you for using WellnessGrid RAG system!\")\n",
        "                break\n",
        "                \n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\\\n\\\\nüëã Session ended by user. Goodbye!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error: {e}\")\n",
        "            print(\"Please try again with a different question.\")\n",
        "\n",
        "# Run interactive interface\n",
        "print(\"üöÄ Starting interactive query interface...\")\n",
        "print(\"üí° Tip: Run this cell and then ask your medical questions!\")\n",
        "# Uncomment the next line to start interactive mode\n",
        "# interactive_query()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Integration with WellnessGrid App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# API-compatible function for WellnessGrid integration\n",
        "def wellness_rag_api(question: str, user_context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    API-compatible function for WellnessGrid app integration\n",
        "    Matches the expected format from the current Flask backend\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Process the question with RAG\n",
        "        result = rag_system.query(question)\n",
        "        \n",
        "        # Format response to match Flask API\n",
        "        api_response = {\n",
        "            \"response\": result['response'],\n",
        "            \"sources\": [\n",
        "                {\n",
        "                    \"title\": f\"{source['source']} - {source['topic']}\",\n",
        "                    \"content\": source['content_preview'],\n",
        "                    \"similarity\": source['similarity']\n",
        "                }\n",
        "                for source in result['sources']\n",
        "            ],\n",
        "            \"mockMode\": False,\n",
        "            \"metadata\": {\n",
        "                \"num_sources\": result['metadata']['num_sources'],\n",
        "                \"context_length\": result['metadata']['context_length'],\n",
        "                \"model\": CONFIG['embedding_model'],\n",
        "                \"timestamp\": result['metadata']['timestamp']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return api_response\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Return error response in Flask API format\n",
        "        return {\n",
        "            \"response\": f\"I apologize, but I encountered an error processing your question: {str(e)}. Please consult with a healthcare professional.\",\n",
        "            \"sources\": [],\n",
        "            \"mockMode\": True,\n",
        "            \"error\": str(e)\n",
        "        }\n",
        "\n",
        "# Test the API function\n",
        "print(\"üîó Testing WellnessGrid API integration...\")\n",
        "test_question = \"What are the symptoms of diabetes?\"\n",
        "api_result = wellness_rag_api(test_question)\n",
        "\n",
        "print(f\"‚úÖ API Response Format:\")\n",
        "print(f\"   üìù Response: {api_result['response'][:100]}...\")\n",
        "print(f\"   üìö Sources: {len(api_result['sources'])} documents\")\n",
        "print(f\"   üîß Mock mode: {api_result['mockMode']}\")\n",
        "print(f\"   üìä Metadata: {list(api_result['metadata'].keys())}\")\n",
        "\n",
        "# Save API function for external use\n",
        "print(\"\\\\nüíæ Saving API function for integration...\")\n",
        "\n",
        "api_code = '''\n",
        "def wellness_rag_query(question: str, user_context: dict = None):\n",
        "    \"\"\"\n",
        "    Standalone function for WellnessGrid RAG integration\n",
        "    Copy this function to your Flask backend or API endpoint\n",
        "    \"\"\"\n",
        "    # Load models and database (do this once at startup)\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import chromadb\n",
        "    \n",
        "    # Configuration\n",
        "    CONFIG = {\n",
        "        \"embedding_model\": \"NeuML/pubmedbert-base-embeddings\",\n",
        "        \"collection_name\": \"wellness_medical_docs\",\n",
        "        \"persist_directory\": \"./chroma_db\",\n",
        "        \"top_k\": 5\n",
        "    }\n",
        "    \n",
        "    # Load models\n",
        "    embedding_model = SentenceTransformer(CONFIG['embedding_model'])\n",
        "    chroma_client = chromadb.PersistentClient(path=CONFIG['persist_directory'])\n",
        "    collection = chroma_client.get_collection(name=CONFIG['collection_name'])\n",
        "    \n",
        "    # Query the RAG system\n",
        "    rag_system = WellnessRAGSystem(embedding_model, collection, CONFIG)\n",
        "    result = rag_system.query(question)\n",
        "    \n",
        "    return {\n",
        "        \"response\": result['response'],\n",
        "        \"sources\": result['sources'],\n",
        "        \"mockMode\": False\n",
        "    }\n",
        "'''\n",
        "\n",
        "with open('wellness_rag_api.py', 'w') as f:\n",
        "    f.write(api_code)\n",
        "\n",
        "print(\"‚úÖ API integration code saved to 'wellness_rag_api.py'\")\n",
        "print(\"\\\\nüìã Integration Steps:\")\n",
        "print(\"1. Copy the RAG system classes to your Flask backend\")\n",
        "print(\"2. Initialize the models once at startup\")\n",
        "print(\"3. Replace the current '/ask' endpoint logic with wellness_rag_api()\")\n",
        "print(\"4. Update the Flask API URL in your frontend if needed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary & Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéâ RAG Query System Setup Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ What's Working:\")\n",
        "print(\"   ‚Ä¢ PubMedBERT embeddings for medical domain\")\n",
        "print(\"   ‚Ä¢ Chroma vector database with persistent storage\")\n",
        "print(\"   ‚Ä¢ Semantic search across medical documents\")\n",
        "print(\"   ‚Ä¢ Source attribution and similarity scores\")\n",
        "print(\"   ‚Ä¢ API-ready format for WellnessGrid integration\")\n",
        "if CONFIG['generate_response']:\n",
        "    print(\"   ‚Ä¢ BioGPT response generation\")\n",
        "else:\n",
        "    print(\"   ‚Ä¢ Context retrieval (response generation disabled)\")\n",
        "\n",
        "print(\"\\\\nüîÑ Next Steps:\")\n",
        "print(\"1. üìù Test more queries using the interactive interface above\")\n",
        "print(\"2. üîó Integrate with your WellnessGrid Flask backend:\")\n",
        "print(\"   - Copy the WellnessRAGSystem class\")\n",
        "print(\"   - Update your /ask endpoint to use wellness_rag_api()\")\n",
        "print(\"   - Update FLASK_API_URL in your Next.js app\")\n",
        "print(\"3. üìä Monitor query performance and similarity scores\")\n",
        "print(\"4. üîÑ Re-run embed_documents.ipynb to add more medical sources\")\n",
        "print(\"5. üéØ Fine-tune similarity thresholds and context length\")\n",
        "\n",
        "print(\"\\\\nüí° Advanced Features to Add:\")\n",
        "print(\"   ‚Ä¢ User-specific document upload\")\n",
        "print(\"   ‚Ä¢ Query history and analytics\")\n",
        "print(\"   ‚Ä¢ Multi-language support\")\n",
        "print(\"   ‚Ä¢ Custom medical domain fine-tuning\")\n",
        "print(\"   ‚Ä¢ Integration with health records\")\n",
        "\n",
        "print(\"\\\\n‚ö†Ô∏è  Important Reminders:\")\n",
        "print(\"   ‚Ä¢ Always include medical disclaimers in responses\")\n",
        "print(\"   ‚Ä¢ Direct users to healthcare professionals for serious concerns\")\n",
        "print(\"   ‚Ä¢ Keep the vector database updated with latest medical information\")\n",
        "print(\"   ‚Ä¢ Monitor for hallucinations and incorrect medical advice\")\n",
        "\n",
        "print(\"\\\\nüìä Current System Stats:\")\n",
        "if embedding_summary:\n",
        "    print(f\"   üìÑ Documents in knowledge base: {embedding_summary['statistics']['total_documents']}\")\n",
        "    print(f\"   üß© Total chunks: {embedding_summary['statistics']['total_chunks']}\")\n",
        "    print(f\"   üìê Embedding dimensions: {embedding_summary['statistics']['embedding_dimension']}\")\n",
        "print(f\"   üîç Retrieving top {CONFIG['top_k']} results per query\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} characters\")\n",
        "\n",
        "print(f\"\\\\nüïê Session completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
