{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid RAG System - Multi-Model Edition with RAG Fusion\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) system for medical questions using:\n",
        "\n",
        "1. ü§ñ **Selectable Medical Models** for advanced medical text generation\n",
        "2. üóÑÔ∏è **Supabase + pgvector** for document retrieval  \n",
        "3. üåê **Flask API with ngrok** for external access\n",
        "4. üîç **Pre-embedded medical documents** from your database\n",
        "5. üîÑ **RAG Fusion** for enhanced document retrieval using multiple query variations\n",
        "\n",
        "## Available Medical Models:\n",
        "- **OpenBioLLM-8B**: 8B parameter medical LLM optimized for biomedical tasks\n",
        "- **Med42-v2-8B**: 8B parameter medical model from M42 Health\n",
        "\n",
        "## Features:\n",
        "- **Model Selection**: Choose from multiple specialized medical LLMs\n",
        "- **RAG Fusion**: Enhanced retrieval using multiple query variations and RRF\n",
        "- **Vector Search**: Supabase pgvector with existing embeddings\n",
        "- **FastAPI**: Modern API with automatic documentation\n",
        "- **Google Colab**: GPU-accelerated inference\n",
        "- **ngrok**: Public URL for external access\n",
        "- **Fallback Support**: Automatic fallback to stable models if loading fails\n",
        "\n",
        "## RAG Fusion Enhancements:\n",
        "- ‚úÖ **Multi-Query Generation**: Creates multiple query variations for better retrieval\n",
        "- ‚úÖ **Reciprocal Rank Fusion (RRF)**: Combines results from multiple queries\n",
        "- ‚úÖ **Enhanced Coverage**: Finds more relevant documents than single queries\n",
        "- ‚úÖ **Fallback Support**: Falls back to single query if fusion fails\n",
        "- ‚úÖ **Configurable**: Easy to enable/disable and tune parameters\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order and select your preferred medical model when prompted\n",
        "3. Enter your Supabase credentials and ngrok auth token when prompted  \n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n",
        "\n",
        "## Prerequisites:\n",
        "- Supabase database with `medical_documents` and `document_embeddings` tables\n",
        "- Documents embedded using `embed_documents.py` script\n",
        "- RPC function `search_embeddings` deployed in Supabase\n",
        "- **IMPORTANT**: Use `service_role` API key (NOT `anon` key)\n",
        "\n",
        "## üîë How to Get Your Supabase Credentials:\n",
        "1. **Go to your Supabase project dashboard**\n",
        "2. **Click \"Settings\" ‚Üí \"API\"**\n",
        "3. **Copy \"Project URL\"** (starts with `https://`)\n",
        "4. **Copy \"service_role\" key** (NOT the anon key!)\n",
        "   - ‚úÖ `service_role`: Long key starting with `eyJ...` (200+ chars)\n",
        "   - ‚ùå `anon`: Shorter public key (WRONG for this notebook)\n",
        "\n",
        "**‚ö° Enhanced RAG system with fusion capabilities using your existing Supabase embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: The script tqdm is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script normalizer is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts huggingface-cli and tiny-agents are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts transformers and transformers-cli are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The script websockets is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script dotenv is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script websockets is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script dotenv is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tests is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script tests is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The script sacremoses is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script sacremoses is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The script uvicorn is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script fastapi is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The script fastapi is installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[33m  WARNING: The scripts ngrok and pyngrok are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m  WARNING: The scripts ngrok and pyngrok are installed in '/Library/Frameworks/Python.framework/Versions/3.12/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages for Google Colab\n",
        "%pip install transformers sentence-transformers\n",
        "%pip install supabase python-dotenv\n",
        "%pip install sacremoses\n",
        "%pip install fastapi uvicorn pydantic python-multipart\n",
        "%pip install pyngrok\n",
        "%pip install psutil  # For memory management\n",
        "%pip install nest-asyncio  # Add this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì± Local environment detected - using environment variables\n",
            "üì¶ All packages imported successfully!\n",
            "üïê RAG session started at: 2025-07-18 16:57:13\n",
            "üîß Using device: cpu\n",
            "üåç Environment: Local Development\n",
            "\n",
            "ü§ñ Available Medical Models:\n",
            "  1. OpenBioLLM-8B - 8B parameter medical LLM optimized for biomedical tasks\n",
            "  2. Med42-v2-8B - 8B parameter medical model from M42 Health based on Llama3\n",
            "\n",
            "üí° For local development:\n",
            "   Set environment variables: SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, NGROK_AUTH_TOKEN\n",
            "   Or run this notebook in Google Colab for interactive setup\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import torch\n",
        "from getpass import getpass\n",
        "\n",
        "# AI Models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client\n",
        "\n",
        "# FastAPI\n",
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "import uvicorn\n",
        "\n",
        "# Pydantic models for validation\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Try to import Colab secrets (for Google Colab)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    COLAB_AVAILABLE = True\n",
        "    print(\"‚úÖ Google Colab environment detected\")\n",
        "except ImportError:\n",
        "    # Create a mock userdata object for local development\n",
        "    class MockUserdata:\n",
        "        def get(self, key):\n",
        "            return os.environ.get(key)\n",
        "    \n",
        "    userdata = MockUserdata()\n",
        "    COLAB_AVAILABLE = False\n",
        "    print(\"üì± Local environment detected - using environment variables\")\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê RAG session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üîß Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "print(f\"üåç Environment: {'Google Colab' if COLAB_AVAILABLE else 'Local Development'}\")\n",
        "\n",
        "# Available medical models\n",
        "MEDICAL_MODELS = {\n",
        "    \"1\": {\n",
        "        \"name\": \"OpenBioLLM-8B\",\n",
        "        \"path\": \"aaditya/OpenBioLLM-Llama3-8B\",\n",
        "        \"description\": \"8B parameter medical LLM optimized for biomedical tasks\",\n",
        "        \"type\": \"causal\"\n",
        "    },\n",
        "    \"2\": {\n",
        "        \"name\": \"Med42-v2-8B\",\n",
        "        \"path\": \"m42-health/Llama3-Med42-8B\",\n",
        "        \"description\": \"8B parameter medical model from M42 Health based on Llama3\",\n",
        "        \"type\": \"causal\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nü§ñ Available Medical Models:\")\n",
        "for key, model in MEDICAL_MODELS.items():\n",
        "    print(f\"  {key}. {model['name']} - {model['description']}\")\n",
        "\n",
        "if COLAB_AVAILABLE:\n",
        "    print(\"\\nPlease select a model by entering the number (1 or 2):\")\n",
        "    print(\"Note: MedGemma-4B is available in a separate notebook (query_rag_medgemma.ipynb)\")\n",
        "else:\n",
        "    print(\"\\nüí° For local development:\")\n",
        "    print(\"   Set environment variables: SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, NGROK_AUTH_TOKEN\")\n",
        "    print(\"   Or run this notebook in Google Colab for interactive setup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All service classes defined!\n",
            "   üìç QueryReformulatorService: Multi-query generation\n",
            "   üìç EmbedderService: Text embeddings on CPU\n",
            "   üìç RetrieverService: Document retrieval from Supabase\n",
            "   üìç GeneratorService: Medical text generation\n"
          ]
        }
      ],
      "source": [
        "# Modular Service Architecture\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseService(ABC):\n",
        "    \"\"\"Base class for all services\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
        "    \n",
        "    async def run_in_executor(self, func, *args):\n",
        "        \"\"\"Run CPU-intensive tasks in thread pool\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return await loop.run_in_executor(self.executor, func, *args)\n",
        "\n",
        "class QueryReformulatorService(BaseService):\n",
        "    \"\"\"Generates multiple query variations for RAG Fusion\"\"\"\n",
        "    \n",
        "    def __init__(self, openrouter_api_key: str = None):\n",
        "        super().__init__()\n",
        "        self.openrouter_api_key = openrouter_api_key\n",
        "        self.base_url = \"https://openrouter.ai/api/v1\"\n",
        "        self.model = \"anthropic/claude-3-haiku\"  # Fast and reliable\n",
        "        \n",
        "    async def reformulate_query(self, original_query: str, num_variations: int = 3) -> List[str]:\n",
        "        \"\"\"Generate multiple query variations using LLM\"\"\"\n",
        "        try:\n",
        "            # Simple prompt for query reformulation\n",
        "            prompt = f\"\"\"You are a medical query enhancement specialist. Create {num_variations} different ways to ask this medical question for better information retrieval.\n",
        "\n",
        "Original query: \"{original_query}\"\n",
        "\n",
        "Generate {num_variations} variations that:\n",
        "1. Use different medical terminology\n",
        "2. Include synonyms and related terms\n",
        "3. Focus on different aspects (symptoms, causes, treatments, etc.)\n",
        "4. Are concise and clear\n",
        "\n",
        "Return only the variations, one per line:\"\"\"\n",
        "\n",
        "            if self.openrouter_api_key:\n",
        "                # Use OpenRouter API\n",
        "                variations = await self._call_openrouter_api(prompt, num_variations)\n",
        "            else:\n",
        "                # Fallback to simple rule-based reformulation\n",
        "                variations = await self._rule_based_reformulation(original_query, num_variations)\n",
        "            \n",
        "            # Always include original query\n",
        "            all_queries = [original_query] + variations\n",
        "            return all_queries[:num_variations + 1]  # Limit total queries\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Query reformulation failed: {str(e)}\")\n",
        "            # Fallback to original query only\n",
        "            return [original_query]\n",
        "    \n",
        "    async def _call_openrouter_api(self, prompt: str, num_variations: int) -> List[str]:\n",
        "        \"\"\"Call OpenRouter API for query reformulation\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.openrouter_api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "            \n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"max_tokens\": 200,\n",
        "                \"temperature\": 0.7\n",
        "            }\n",
        "            \n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.post(\n",
        "                    f\"{self.base_url}/chat/completions\",\n",
        "                    headers=headers,\n",
        "                    json=payload,\n",
        "                    timeout=10\n",
        "                ) as response:\n",
        "                    if response.status == 200:\n",
        "                        result = await response.json()\n",
        "                        content = result['choices'][0]['message']['content']\n",
        "                        \n",
        "                        # Parse variations from response\n",
        "                        variations = []\n",
        "                        lines = content.strip().split('\\n')\n",
        "                        for line in lines:\n",
        "                            line = line.strip()\n",
        "                            if line and not line.startswith(('#', '-', '*', '1.', '2.', '3.')):\n",
        "                                variations.append(line)\n",
        "                        \n",
        "                        return variations[:num_variations]\n",
        "                    else:\n",
        "                        raise Exception(f\"API call failed: {response.status}\")\n",
        "                        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OpenRouter API call failed: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    async def _rule_based_reformulation(self, query: str, num_variations: int) -> List[str]:\n",
        "        \"\"\"Simple rule-based query reformulation as fallback\"\"\"\n",
        "        variations = []\n",
        "        \n",
        "        # Medical synonyms mapping\n",
        "        medical_synonyms = {\n",
        "            'diabetes': ['diabetes mellitus', 'blood sugar', 'glucose', 'type 2 diabetes'],\n",
        "            'hypertension': ['high blood pressure', 'elevated blood pressure', 'HTN'],\n",
        "            'heart attack': ['myocardial infarction', 'MI', 'cardiac arrest', 'heart failure'],\n",
        "            'stroke': ['cerebrovascular accident', 'CVA', 'brain attack'],\n",
        "            'cancer': ['malignancy', 'tumor', 'neoplasm', 'carcinoma'],\n",
        "            'pain': ['discomfort', 'ache', 'soreness', 'tenderness'],\n",
        "            'fever': ['elevated temperature', 'pyrexia', 'hyperthermia'],\n",
        "            'fatigue': ['tiredness', 'exhaustion', 'weakness', 'lethargy'],\n",
        "            'nausea': ['sick to stomach', 'queasiness', 'upset stomach'],\n",
        "            'headache': ['head pain', 'migraine', 'cephalalgia']\n",
        "        }\n",
        "        \n",
        "        # Generate variations using synonyms\n",
        "        for term, synonyms in medical_synonyms.items():\n",
        "            if term.lower() in query.lower() and len(variations) < num_variations:\n",
        "                for synonym in synonyms[:2]:  # Use max 2 synonyms per term\n",
        "                    variation = query.lower().replace(term.lower(), synonym)\n",
        "                    if variation != query.lower():\n",
        "                        variations.append(variation.capitalize())\n",
        "                        if len(variations) >= num_variations:\n",
        "                            break\n",
        "        \n",
        "        # Add question format variations\n",
        "        if len(variations) < num_variations:\n",
        "            if '?' not in query:\n",
        "                variations.append(f\"{query}?\")\n",
        "            if 'what' not in query.lower():\n",
        "                variations.append(f\"What are {query.lower()}?\")\n",
        "            if 'how' not in query.lower():\n",
        "                variations.append(f\"How to {query.lower()}?\")\n",
        "        \n",
        "        return variations[:num_variations]\n",
        "\n",
        "class EmbedderService(BaseService):\n",
        "    \"\"\"Handles text embedding generation on CPU\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = None\n",
        "        self.device = 'cpu'\n",
        "    \n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize embedding model on CPU\"\"\"\n",
        "        if self.model is None:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            self.model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            print(\"‚úÖ EmbedderService: Model loaded on CPU\")\n",
        "    \n",
        "    async def embed_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embeddings for text\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run embedding on CPU thread pool\n",
        "        embedding = await self.run_in_executor(self.model.encode, [text])\n",
        "        return embedding[0].tolist()\n",
        "    \n",
        "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run batch embedding on CPU thread pool\n",
        "        embeddings = await self.run_in_executor(self.model.encode, texts)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "class RetrieverService(BaseService):\n",
        "    \"\"\"Handles document retrieval from Supabase\"\"\"\n",
        "    \n",
        "    def __init__(self, supabase_client, embedder_service: EmbedderService):\n",
        "        super().__init__()\n",
        "        self.supabase = supabase_client\n",
        "        self.embedder = embedder_service\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "    \n",
        "    async def retrieve_documents(self, query: str, top_k: int = None) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents using vector search\"\"\"\n",
        "        top_k = top_k or self.top_k\n",
        "        \n",
        "        # Generate query embedding\n",
        "        query_embedding = await self.embedder.embed_text(query)\n",
        "        \n",
        "        # Search Supabase\n",
        "        try:\n",
        "            result = self.supabase.rpc('search_embeddings', {\n",
        "                'query_embedding': query_embedding,\n",
        "                'match_threshold': self.similarity_threshold,\n",
        "                'match_count': top_k\n",
        "            }).execute()\n",
        "            \n",
        "            if result.data:\n",
        "                documents = []\n",
        "                for i, doc in enumerate(result.data):\n",
        "                    documents.append({\n",
        "                        'content': doc.get('chunk_content', ''),\n",
        "                        'similarity_score': doc.get('similarity', 0.0),\n",
        "                        'metadata': {\n",
        "                            'title': doc.get('title', 'Medical Document'),\n",
        "                            'source': doc.get('source', 'unknown'),\n",
        "                            'topic': doc.get('topic', 'general'),\n",
        "                            'document_type': doc.get('document_type', 'unknown'),\n",
        "                            'document_id': doc.get('document_id', '')\n",
        "                        },\n",
        "                        'rank': i + 1,\n",
        "                        'doc_id': doc.get('document_id', '')\n",
        "                    })\n",
        "                return documents\n",
        "            else:\n",
        "                return []\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RetrieverService error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class GeneratorService(BaseService):\n",
        "    \"\"\"Handles text generation using medical LLMs on GPU\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, tokenizer, medical_model):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.medical_model = medical_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    async def generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
        "        \"\"\"Generate medical response using the selected model\"\"\"\n",
        "        try:\n",
        "            # Run generation on GPU thread pool\n",
        "            response = await self.run_in_executor(self._generate_sync, prompt, max_tokens)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå GeneratorService error: {str(e)}\")\n",
        "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
        "    \n",
        "    def _generate_sync(self, prompt: str, max_tokens: int) -> str:\n",
        "        \"\"\"Synchronous generation method for thread pool\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "        \n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:\n",
        "            generation_params[\"pad_token_id\"] = self.tokenizer.pad_token_id\n",
        "        if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "            generation_params[\"eos_token_id\"] = self.tokenizer.eos_token_id\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.medical_model.generate(**inputs, **generation_params)\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "        return response.strip()\n",
        "\n",
        "print(\"‚úÖ All service classes defined!\")\n",
        "print(\"   üìç QueryReformulatorService: Multi-query generation\")\n",
        "print(\"   üìç EmbedderService: Text embeddings on CPU\")\n",
        "print(\"   üìç RetrieverService: Document retrieval from Supabase\")\n",
        "print(\"   üìç GeneratorService: Medical text generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Supabase connection\n",
        "print(\"üóÑÔ∏è Setting up Supabase connection...\")\n",
        "\n",
        "if COLAB_AVAILABLE:\n",
        "    print(\"üìã Required Colab secrets:\")\n",
        "    print(\"   1. SUPABASE_URL - Your project URL (e.g., https://abc123.supabase.co)\")\n",
        "    print(\"   2. SUPABASE_SERVICE_ROLE_KEY - Your service role key (NOT anon key)\")\n",
        "    print(\"   3. NGROK_AUTH_TOKEN - Your ngrok authentication token\")\n",
        "    print(\"\")\n",
        "    print(\"üîë To set these secrets:\")\n",
        "    print(\"   1. Click the üîë key icon in the left sidebar\")\n",
        "    print(\"   2. Add the three secrets listed above\")\n",
        "    print(\"   3. Re-run this cell\")\n",
        "    print(\"\")\n",
        "else:\n",
        "    print(\"üíª For local development, set environment variables:\")\n",
        "    print(\"   export SUPABASE_URL='https://your-project.supabase.co'\")\n",
        "    print(\"   export SUPABASE_SERVICE_ROLE_KEY='your-service-role-key'\")\n",
        "    print(\"   export NGROK_AUTH_TOKEN='your-ngrok-token'\")\n",
        "    print(\"\")\n",
        "\n",
        "try:\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    \n",
        "    if COLAB_AVAILABLE:\n",
        "        print(\"‚úÖ Successfully retrieved secrets from Colab\")\n",
        "    else:\n",
        "        print(\"‚úÖ Successfully retrieved environment variables\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving credentials: {str(e)}\")\n",
        "    if COLAB_AVAILABLE:\n",
        "        print(\"üîß Make sure you've added the required secrets in Colab:\")\n",
        "        print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "        print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "        print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    else:\n",
        "        print(\"üîß Make sure you've set the required environment variables:\")\n",
        "        print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "        print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "        print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    raise\n",
        "\n",
        "# Validate the inputs\n",
        "if not supabase_url or not supabase_key:\n",
        "    raise ValueError(\"‚ùå Both Supabase URL and Service Role Key are required!\")\n",
        "\n",
        "if not supabase_url.startswith('https://'):\n",
        "    raise ValueError(\"‚ùå Supabase URL should start with 'https://'\")\n",
        "\n",
        "if not supabase_key.startswith('eyJ'):\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys typically start with 'eyJ'\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "    \n",
        "if len(supabase_key) < 100:\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys are typically very long (200+ characters)\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "\n",
        "try:\n",
        "    supabase = create_client(supabase_url, supabase_key)\n",
        "    print(\"‚úÖ Supabase client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Supabase client: {str(e)}\")\n",
        "    print(\"üîß Common issues:\")\n",
        "    print(\"   ‚Ä¢ Wrong API key type (use service_role, not anon)\")\n",
        "    print(\"   ‚Ä¢ Typo in URL or key\")\n",
        "    print(\"   ‚Ä¢ Key might be expired or regenerated\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"top_k\": 5,\n",
        "    \"similarity_threshold\": 0.5,\n",
        "    \"max_context_length\": 2000,\n",
        "    \"max_response_length\": 150,\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è RAG Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic models for request/response validation\n",
        "class AskRequest(BaseModel):\n",
        "    question: str = Field(..., min_length=1, max_length=1000, description=\"The medical question to ask\")\n",
        "    session_id: Optional[str] = Field(default=\"default\", description=\"Chat session identifier\")\n",
        "    history: Optional[List[dict]] = Field(default=[], description=\"Previous chat history\")\n",
        "\n",
        "class AskResponse(BaseModel):\n",
        "    response: str = Field(..., description=\"AI generated response\")\n",
        "    sources: List[dict] = Field(..., description=\"Source documents used\")\n",
        "    chat_history: List[dict] = Field(..., description=\"Updated chat history\")\n",
        "    session_id: str = Field(..., description=\"Session identifier\")\n",
        "    mockMode: bool = Field(default=False, description=\"Whether response is from mock mode\")\n",
        "    metadata: dict = Field(..., description=\"Response metadata\")\n",
        "\n",
        "class EmbedRequest(BaseModel):\n",
        "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to embed\")\n",
        "\n",
        "class EmbedResponse(BaseModel):\n",
        "    embedding: List[float] = Field(..., description=\"Generated embedding vector\")\n",
        "    dimensions: int = Field(..., description=\"Embedding dimensions\")\n",
        "    model: str = Field(..., description=\"Model used for embedding\")\n",
        "    device: str = Field(..., description=\"Device used for embedding\")\n",
        "\n",
        "class GenerateRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Query text\")\n",
        "    context: str = Field(default=\"\", description=\"Context information\")\n",
        "    history: List[dict] = Field(default=[], description=\"Chat history\")\n",
        "    max_tokens: int = Field(default=200, ge=1, le=1000, description=\"Maximum tokens to generate\")\n",
        "    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Generation temperature\")\n",
        "\n",
        "class GenerateResponse(BaseModel):\n",
        "    answer: str = Field(..., description=\"Generated answer\")\n",
        "    model: str = Field(..., description=\"Model used for generation\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    context_used: bool = Field(..., description=\"Whether context was used\")\n",
        "    history_used: bool = Field(..., description=\"Whether history was used\")\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Search query\")\n",
        "    top_k: int = Field(default=5, ge=1, le=20, description=\"Number of documents to retrieve\")\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    documents: List[dict] = Field(..., description=\"Retrieved documents\")\n",
        "    total_found: int = Field(..., description=\"Total documents found\")\n",
        "    query: str = Field(..., description=\"Original query\")\n",
        "\n",
        "class RetrieveRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Search query for fusion retrieval\")\n",
        "    top_k: Optional[int] = Field(default=5, ge=1, le=20, description=\"Number of documents to retrieve\")\n",
        "\n",
        "class RetrieveResponse(BaseModel):\n",
        "    query: str = Field(..., description=\"Original query\")\n",
        "    documents: List[dict] = Field(..., description=\"Retrieved documents with fusion metadata\")\n",
        "    total_found: int = Field(..., description=\"Total documents found\")\n",
        "    fusion_metadata: dict = Field(..., description=\"RAG Fusion specific metadata\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str = Field(..., description=\"System status\")\n",
        "    model: str = Field(..., description=\"Active model name\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    embedding_device: str = Field(..., description=\"Embedding device\")\n",
        "    database: str = Field(..., description=\"Database type\")\n",
        "    documents_in_db: int = Field(..., description=\"Number of documents in database\")\n",
        "    embeddings_in_db: int = Field(..., description=\"Number of embeddings in database\")\n",
        "    rag_system: str = Field(..., description=\"RAG system type\")\n",
        "    chat_support: bool = Field(..., description=\"Chat support status\")\n",
        "    active_sessions: int = Field(..., description=\"Number of active sessions\")\n",
        "    gpu_memory: dict = Field(..., description=\"GPU memory information\")\n",
        "\n",
        "print(\"‚úÖ Pydantic models defined for request/response validation\")\n",
        "print(\"   üìç Added RetrieveRequest/Response for RAG Fusion endpoints\")\n",
        "print(\"   üìç All endpoints have proper validation and documentation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory Management Utilities\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Get detailed GPU memory information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"No GPU available\"\n",
        "    \n",
        "    device = torch.cuda.current_device()\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "    total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "    free = total - allocated\n",
        "    \n",
        "    return {\n",
        "        \"allocated\": f\"{allocated:.2f} GB\",\n",
        "        \"reserved\": f\"{reserved:.2f} GB\", \n",
        "        \"total\": f\"{total:.2f} GB\",\n",
        "        \"free\": f\"{free:.2f} GB\",\n",
        "        \"percentage_used\": f\"{(allocated/total)*100:.1f}%\"\n",
        "    }\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"üßπ GPU cache cleared\")\n",
        "    else:\n",
        "        print(\"üì± No GPU to clear\")\n",
        "\n",
        "# Show current GPU memory status\n",
        "print(\"üîç Current GPU Memory Status:\")\n",
        "memory_info = get_gpu_memory_info()\n",
        "if isinstance(memory_info, dict):\n",
        "    for key, value in memory_info.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "else:\n",
        "    print(f\"   {memory_info}\")\n",
        "\n",
        "print(\"\\n‚úÖ Memory management utilities ready!\")\n",
        "print(\"   Use get_gpu_memory_info() to check memory\")\n",
        "print(\"   Use clear_gpu_memory() to free cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection and loading\n",
        "print(\"ü§ñ Select your medical model:\")\n",
        "\n",
        "# For interactive environments (Colab), ask for input\n",
        "# For automated environments, default to model 1\n",
        "if COLAB_AVAILABLE:\n",
        "    model_choice = input(\"Enter model number (1 or 2): \").strip()\n",
        "else:\n",
        "    # Default to model 1 for local development, but allow override via environment\n",
        "    model_choice = os.environ.get('MODEL_CHOICE', '1')\n",
        "    print(f\"Using model choice: {model_choice} (set MODEL_CHOICE env var to override)\")\n",
        "\n",
        "if model_choice not in MEDICAL_MODELS:\n",
        "    print(f\"‚ùå Invalid choice '{model_choice}'. Defaulting to OpenBioLLM-8B (option 1)\")\n",
        "    model_choice = \"1\"\n",
        "\n",
        "selected_model = MEDICAL_MODELS[model_choice]\n",
        "model_name = selected_model[\"name\"]\n",
        "model_path = selected_model[\"path\"]\n",
        "model_type = selected_model[\"type\"]\n",
        "\n",
        "print(f\"üß† Loading {model_name} for medical text generation...\")\n",
        "print(f\"üì¶ Model path: {model_path}\")\n",
        "print(f\"üîß Model type: {model_type}\")\n",
        "\n",
        "# Check GPU memory before loading\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîç GPU Memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "\n",
        "try:\n",
        "    # Standard causal LM handling (models 1 and 2 are both causal models)\n",
        "    print(\"üî§ Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "    \n",
        "    print(\"üß† Loading model (this may take a few minutes)...\")\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Set pad token if not available\n",
        "    if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        else:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "    \n",
        "    # Store model info globally\n",
        "    MODEL_INFO = {\n",
        "        \"name\": model_name,\n",
        "        \"path\": model_path,\n",
        "        \"choice\": model_choice,\n",
        "        \"type\": model_type\n",
        "    }\n",
        "    \n",
        "    # Check GPU memory after loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üîç GPU Memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "    \n",
        "    print(f\"‚úÖ {model_name} loaded and ready on {device}\")\n",
        "    print(f\"üéØ Selected model: {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n",
        "    print(\"üîÑ Falling back to a smaller model...\")\n",
        "    \n",
        "    # Fallback to a more reliable model\n",
        "    fallback_path = \"microsoft/DialoGPT-medium\"\n",
        "    print(f\"üîÑ Loading fallback model: {fallback_path}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(fallback_path)\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fallback_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    MODEL_INFO = {\n",
        "        \"name\": \"DialoGPT-medium (Fallback)\",\n",
        "        \"path\": fallback_path,\n",
        "        \"choice\": \"fallback\"\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Fallback model loaded successfully\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pre-load the embedding model to avoid CUDA errors during Flask requests\n",
        "print(\"üîç Pre-loading embedding model to avoid CUDA memory conflicts...\")\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    # Check if there's enough GPU memory for the embedding model\n",
        "    if torch.cuda.is_available():\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "        allocated_memory = torch.cuda.memory_allocated()\n",
        "        free_memory = total_memory - allocated_memory\n",
        "        \n",
        "        print(f\"üîç Available GPU memory: {free_memory/1024**3:.2f} GB\")\n",
        "        \n",
        "        # If less than 2GB free, use CPU for embeddings\n",
        "        if free_memory < 2 * 1024**3:  # 2GB threshold\n",
        "            print(\"‚ö†Ô∏è Limited GPU memory - using CPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            EMBEDDING_DEVICE = 'cpu'\n",
        "        else:\n",
        "            print(\"‚úÖ Sufficient GPU memory - using GPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "            EMBEDDING_DEVICE = 'cuda'\n",
        "    else:\n",
        "        print(\"üì± No GPU available - using CPU for embedding model\")\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "        EMBEDDING_DEVICE = 'cpu'\n",
        "    \n",
        "    print(f\"‚úÖ Embedding model loaded on {EMBEDDING_DEVICE}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error pre-loading embedding model: {str(e)}\")\n",
        "    print(\"üîÑ Will load embedding model on-demand with CPU fallback\")\n",
        "    embedding_model = None\n",
        "    EMBEDDING_DEVICE = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize RAG Fusion Services\n",
        "print(\"üîß Initializing RAG Fusion services...\")\n",
        "\n",
        "# Initialize base services\n",
        "embedder_service = EmbedderService()\n",
        "base_retriever_service = RetrieverService(supabase, embedder_service)\n",
        "generator_service = GeneratorService(MODEL_INFO['name'], tokenizer, medical_model)\n",
        "\n",
        "# Initialize RAG Fusion components\n",
        "query_reformulator = QueryReformulatorService()  # No API key for now - uses rule-based fallback\n",
        "fusion_retriever = FusionRetrieverService(base_retriever_service, query_reformulator)\n",
        "\n",
        "# Create enhanced RAG controller with fusion support\n",
        "rag_controller = RAGController(\n",
        "    embedder=embedder_service,\n",
        "    retriever=base_retriever_service,\n",
        "    generator=generator_service,\n",
        "    fusion_retriever=fusion_retriever\n",
        ")\n",
        "\n",
        "print(\"‚úÖ RAG Fusion services initialized!\")\n",
        "print(f\"   üìç QueryReformulatorService: Multi-query generation\")\n",
        "print(f\"   üìç FusionRetrieverService: RRF-based document fusion\")\n",
        "print(f\"   üìç RAGController: Complete pipeline with fusion support\")\n",
        "print(f\"   üìç Enhanced services ready for {MODEL_INFO['name']}\")\n",
        "\n",
        "# RAG Fusion Configuration\n",
        "FUSION_CONFIG = {\n",
        "    \"enabled\": True,\n",
        "    \"max_query_variations\": 4,\n",
        "    \"rrf_k_parameter\": 60,\n",
        "    \"retrieval_multiplier\": 2,  # Get 2x docs for fusion\n",
        "    \"use_openrouter\": False,  # Set to True if you have API key\n",
        "    \"openrouter_api_key\": None,  # Add your key here\n",
        "    \"fallback_to_single\": True  # Fallback if fusion fails\n",
        "}\n",
        "\n",
        "print(\"\\n‚öôÔ∏è RAG Fusion Configuration:\")\n",
        "for key, value in FUSION_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Update services with configuration\n",
        "if FUSION_CONFIG[\"use_openrouter\"] and FUSION_CONFIG[\"openrouter_api_key\"]:\n",
        "    query_reformulator.openrouter_api_key = FUSION_CONFIG[\"openrouter_api_key\"]\n",
        "    print(\"üîë OpenRouter API configured for query reformulation\")\n",
        "else:\n",
        "    print(\"üìù Using rule-based query reformulation (fallback mode)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Fusion: Fusion Retriever Service\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "class FusionRetrieverService(BaseService):\n",
        "    \"\"\"Combines multiple query retrievals using Reciprocal Rank Fusion (RRF)\"\"\"\n",
        "    \n",
        "    def __init__(self, base_retriever: RetrieverService, reformulator: QueryReformulatorService):\n",
        "        super().__init__()\n",
        "        self.base_retriever = base_retriever\n",
        "        self.reformulator = reformulator\n",
        "        self.k = 60  # RRF parameter (typically 60)\n",
        "        self.max_queries = 4  # Maximum number of query variations\n",
        "        \n",
        "    async def retrieve_documents_fusion(self, query: str, top_k: int = 5) -> List[dict]:\n",
        "        \"\"\"Retrieve documents using RAG Fusion with multiple query variations\"\"\"\n",
        "        try:\n",
        "            print(f\"üîÑ RAG Fusion: Generating query variations...\")\n",
        "            \n",
        "            # Step 1: Generate query variations\n",
        "            query_variations = await self.reformulator.reformulate_query(query, self.max_queries - 1)\n",
        "            print(f\"üìù Generated {len(query_variations)} query variations\")\n",
        "            \n",
        "            # Step 2: Retrieve documents for each variation\n",
        "            all_documents = []\n",
        "            for i, variation in enumerate(query_variations):\n",
        "                print(f\"üîç Retrieving for variation {i+1}: '{variation[:50]}...'\")\n",
        "                docs = await self.base_retriever.retrieve_documents(variation, top_k * 2)  # Get more docs for fusion\n",
        "                \n",
        "                # Add query source info\n",
        "                for doc in docs:\n",
        "                    doc['query_source'] = i\n",
        "                    doc['query_variation'] = variation\n",
        "                \n",
        "                all_documents.extend(docs)\n",
        "            \n",
        "            # Step 3: Apply Reciprocal Rank Fusion\n",
        "            print(f\"üîó Fusing {len(all_documents)} documents from {len(query_variations)} queries...\")\n",
        "            fused_docs = await self._apply_rrf(all_documents, top_k)\n",
        "            \n",
        "            # Step 4: Deduplicate and format results\n",
        "            final_docs = await self._deduplicate_and_format(fused_docs, top_k)\n",
        "            \n",
        "            print(f\"‚úÖ RAG Fusion complete: {len(final_docs)} unique documents retrieved\")\n",
        "            return final_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAG Fusion failed: {str(e)}\")\n",
        "            # Fallback to single query retrieval\n",
        "            print(\"üîÑ Falling back to single query retrieval...\")\n",
        "            return await self.base_retriever.retrieve_documents(query, top_k)\n",
        "    \n",
        "    async def _apply_rrf(self, documents: List[dict], top_k: int) -> List[dict]:\n",
        "        \"\"\"Apply Reciprocal Rank Fusion to combine document scores\"\"\"\n",
        "        # Group documents by document ID\n",
        "        doc_groups = defaultdict(list)\n",
        "        for doc in documents:\n",
        "            doc_id = doc.get('doc_id', doc.get('metadata', {}).get('document_id', ''))\n",
        "            if doc_id:\n",
        "                doc_groups[doc_id].append(doc)\n",
        "        \n",
        "        # Calculate RRF scores\n",
        "        rrf_scores = {}\n",
        "        for doc_id, doc_list in doc_groups.items():\n",
        "            rrf_score = 0.0\n",
        "            \n",
        "            for doc in doc_list:\n",
        "                rank = doc.get('rank', 1)\n",
        "                rrf_score += 1.0 / (self.k + rank)\n",
        "            \n",
        "            # Store best document with RRF score\n",
        "            best_doc = max(doc_list, key=lambda x: x.get('similarity_score', 0))\n",
        "            best_doc['rrf_score'] = rrf_score\n",
        "            best_doc['appearance_count'] = len(doc_list)\n",
        "            best_doc['query_sources'] = [d.get('query_source', 0) for d in doc_list]\n",
        "            \n",
        "            rrf_scores[doc_id] = best_doc\n",
        "        \n",
        "        # Sort by RRF score\n",
        "        sorted_docs = sorted(rrf_scores.values(), key=lambda x: x['rrf_score'], reverse=True)\n",
        "        return sorted_docs[:top_k * 2]  # Return more for deduplication\n",
        "    \n",
        "    async def _deduplicate_and_format(self, documents: List[dict], top_k: int) -> List[dict]:\n",
        "        \"\"\"Remove duplicates and format final results\"\"\"\n",
        "        seen_content = set()\n",
        "        final_docs = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            # Create content hash for deduplication\n",
        "            content_preview = doc.get('content', '')[:100]\n",
        "            content_hash = hash(content_preview)\n",
        "            \n",
        "            if content_hash not in seen_content:\n",
        "                seen_content.add(content_hash)\n",
        "                \n",
        "                # Format final document\n",
        "                final_doc = {\n",
        "                    'content': doc.get('content', ''),\n",
        "                    'similarity_score': doc.get('rrf_score', doc.get('similarity_score', 0.0)),\n",
        "                    'metadata': doc.get('metadata', {}),\n",
        "                    'rank': len(final_docs) + 1,\n",
        "                    'doc_id': doc.get('doc_id', ''),\n",
        "                    'fusion_metadata': {\n",
        "                        'rrf_score': doc.get('rrf_score', 0.0),\n",
        "                        'appearance_count': doc.get('appearance_count', 1),\n",
        "                        'query_sources': doc.get('query_sources', []),\n",
        "                        'fusion_method': 'RRF'\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                final_docs.append(final_doc)\n",
        "                \n",
        "                if len(final_docs) >= top_k:\n",
        "                    break\n",
        "        \n",
        "        return final_docs\n",
        "\n",
        "class RAGController:\n",
        "    \"\"\"Orchestrates the complete RAG pipeline with fusion support\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: EmbedderService, retriever: RetrieverService, generator: GeneratorService, fusion_retriever: FusionRetrieverService = None):\n",
        "        self.embedder = embedder\n",
        "        self.retriever = retriever\n",
        "        self.generator = generator\n",
        "        self.fusion_retriever = fusion_retriever\n",
        "        self.max_context_length = 2000\n",
        "    \n",
        "    async def process_query(self, question: str, history: List[dict] = None, use_fusion: bool = True) -> dict:\n",
        "        \"\"\"Complete RAG pipeline: retrieve context and generate response\"\"\"\n",
        "        try:\n",
        "            print(f\"üîç Processing query: {question}\")\n",
        "            \n",
        "            # Step 1: Retrieve relevant documents\n",
        "            if use_fusion and self.fusion_retriever:\n",
        "                retrieved_docs = await self.fusion_retriever.retrieve_documents_fusion(question)\n",
        "            else:\n",
        "                retrieved_docs = await self.retriever.retrieve_documents(question)\n",
        "            \n",
        "            print(f\"üìä Found {len(retrieved_docs)} similar documents\")\n",
        "            \n",
        "            # Step 2: Build context from documents\n",
        "            context_parts = []\n",
        "            total_chars = 0\n",
        "            \n",
        "            for doc in retrieved_docs:\n",
        "                if total_chars + len(doc['content']) <= self.max_context_length:\n",
        "                    context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                    total_chars += len(doc['content'])\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            context = \"\\n\\n\".join(context_parts)\n",
        "            print(f\"üìÑ Using {len(context_parts)} documents for context ({len(context)} chars)\")\n",
        "            \n",
        "            # Step 3: Create prompt with history\n",
        "            if history:\n",
        "                history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in history[-3:]])\n",
        "                prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {question}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "            else:\n",
        "                prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "            \n",
        "            # Step 4: Generate response\n",
        "            print(f\"ü§ñ Generating response using {self.generator.model_name}...\")\n",
        "            response = await self.generator.generate_response(prompt, 150)\n",
        "            \n",
        "            # Step 5: Format result\n",
        "            result = {\n",
        "                'query': question,\n",
        "                'response': response,\n",
        "                'sources': [\n",
        "                    {\n",
        "                        'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                        'source': doc['metadata']['source'],\n",
        "                        'topic': doc['metadata']['topic'],\n",
        "                        'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                        'rank': doc['rank'],\n",
        "                        'content_preview': doc['content'][:150] + \"...\"\n",
        "                    }\n",
        "                    for doc in retrieved_docs\n",
        "                ],\n",
        "                'metadata': {\n",
        "                    'documentsUsed': len(context_parts),\n",
        "                    'totalFound': len(retrieved_docs),\n",
        "                    'contextLength': len(context),\n",
        "                    'model': self.generator.model_name,\n",
        "                    'processingTime': datetime.now().isoformat(),\n",
        "                    'fusionUsed': use_fusion and self.fusion_retriever is not None\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAGController error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "print(\"‚úÖ FusionRetrieverService and RAGController ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Supabase connection and RPC functions\n",
        "print(\"üß™ Testing Supabase connection...\")\n",
        "try:\n",
        "    # Test basic connection\n",
        "    test_result = supabase.table('medical_documents').select('count').execute()\n",
        "    doc_count = len(test_result.data) if test_result.data else 0\n",
        "    print(f\"‚úÖ Supabase connected - Found {doc_count} documents in database\")\n",
        "    \n",
        "    # Test RPC function availability\n",
        "    print(\"üß™ Testing RPC functions...\")\n",
        "    try:\n",
        "        stats_result = supabase.rpc('get_document_stats').execute()\n",
        "        if stats_result.data:\n",
        "            print(\"‚úÖ RPC functions working\")\n",
        "            for stat in stats_result.data[:3]:  # Show first 3 document sources\n",
        "                print(f\"   üìÑ {stat['source']}: {stat['count']} documents\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è RPC function exists but returned no data\")\n",
        "    except Exception as rpc_error:\n",
        "        print(f\"‚ö†Ô∏è RPC function test failed: {str(rpc_error)}\")\n",
        "        print(\"   Vector search will use fallback method\")\n",
        "        \n",
        "except Exception as e:\n",
        "    error_str = str(e)\n",
        "    print(f\"‚ö†Ô∏è Supabase connection test failed: {error_str}\")\n",
        "    \n",
        "    # Provide specific guidance based on error type\n",
        "    if '401' in error_str or 'Invalid API key' in error_str:\n",
        "        print(\"üîß AUTHENTICATION ERROR - Invalid API Key:\")\n",
        "        print(\"   ‚ùå You're using the wrong API key!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Go to your Supabase project dashboard\")\n",
        "        print(\"   2. Settings ‚Üí API\")\n",
        "        print(\"   3. Copy the 'service_role' key (NOT anon key)\")\n",
        "        print(\"   4. The service_role key is much longer and starts with 'eyJ'\")\n",
        "        print(\"   5. Re-run Cell 3 with the correct key\")\n",
        "        print(\"\")\n",
        "        print(\"   üîç Key differences:\")\n",
        "        print(\"   ‚Ä¢ anon key: Used for client-side apps (WRONG for this notebook)\")\n",
        "        print(\"   ‚Ä¢ service_role key: Used for server-side/admin access (CORRECT)\")\n",
        "    elif '404' in error_str:\n",
        "        print(\"üîß TABLE NOT FOUND:\")\n",
        "        print(\"   ‚ùå The 'medical_documents' table doesn't exist!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Run the schema.sql in your Supabase SQL editor\")\n",
        "        print(\"   2. Or run the embed_documents.py script to create tables\")\n",
        "    elif 'timeout' in error_str.lower():\n",
        "        print(\"üîß CONNECTION TIMEOUT:\")\n",
        "        print(\"   ‚ùå Can't reach Supabase servers\")\n",
        "        print(\"   üìã Check your internet connection and Supabase URL\")\n",
        "    else:\n",
        "        print(\"üîß GENERAL CONNECTION ERROR:\")\n",
        "        print(\"   üìã Common fixes:\")\n",
        "        print(\"   ‚Ä¢ Double-check your Supabase URL\")\n",
        "        print(\"   ‚Ä¢ Verify you're using service_role key (not anon)\")\n",
        "        print(\"   ‚Ä¢ Check if your project is paused/suspended\")\n",
        "        print(\"   ‚Ä¢ Ensure database tables exist\")\n",
        "    \n",
        "    print(\"\\n   ‚ö†Ô∏è The system will continue but may have limited document retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced FastAPI Setup with Chat History Support\n",
        "app = FastAPI(\n",
        "    title=\"WellnessGrid RAG API\",\n",
        "    description=\"Medical AI Assistant with RAG Fusion capabilities using FastAPI\",\n",
        "    version=\"2.0.0\",\n",
        "    docs_url=\"/docs\",\n",
        "    redoc_url=\"/redoc\"\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Store chat sessions in memory (in production, use Redis or database)\n",
        "chat_sessions = {}\n",
        "\n",
        "@app.post(\"/retrieve-fusion\", response_model=RetrieveResponse)\n",
        "async def retrieve_with_fusion(request: RetrieveRequest):\n",
        "    \"\"\"Retrieve documents using RAG Fusion\"\"\"\n",
        "    try:\n",
        "        print(f\"üîÑ RAG Fusion retrieval for: {request.query[:100]}...\")\n",
        "        \n",
        "        # Use fusion retriever\n",
        "        documents = await fusion_retriever.retrieve_documents_fusion(\n",
        "            request.query, \n",
        "            request.top_k or 5\n",
        "        )\n",
        "        \n",
        "        return RetrieveResponse(\n",
        "            query=request.query,\n",
        "            documents=documents,\n",
        "            total_found=len(documents),\n",
        "            fusion_metadata={\n",
        "                \"method\": \"RRF\",\n",
        "                \"query_variations\": len(set(doc.get('fusion_metadata', {}).get('query_sources', []) for doc in documents)),\n",
        "                \"avg_appearance_count\": sum(doc.get('fusion_metadata', {}).get('appearance_count', 1) for doc in documents) / len(documents) if documents else 0\n",
        "            }\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Fusion retrieval error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Fusion retrieval failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/embed\", response_model=EmbedResponse)\n",
        "async def generate_embedding(request: EmbedRequest):\n",
        "    \"\"\"Generate embeddings for text with enhanced error handling\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üîç Generating embedding for text: {request.text[:100]}...\")\n",
        "        \n",
        "        # Use the embedder service\n",
        "        embedding = await embedder_service.embed_text(request.text)\n",
        "        \n",
        "        return EmbedResponse(\n",
        "            embedding=embedding,\n",
        "            dimensions=len(embedding),\n",
        "            model=\"PubMedBERT (CPU)\",\n",
        "            device=\"cpu\"\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in embed endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Embedding generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerateResponse)\n",
        "async def generate_text(request: GenerateRequest):\n",
        "    \"\"\"Enhanced generate endpoint using modular generator service\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üî¨ Generating response for query: {request.query[:100]}...\")\n",
        "        \n",
        "        # Create enhanced prompt with chat history\n",
        "        if request.history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in request.history[-3:]])\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Current question: {request.query}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Question: {request.query}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Generate response using generator service\n",
        "        response = await generator_service.generate_response(prompt, request.max_tokens)\n",
        "        \n",
        "        return GenerateResponse(\n",
        "            answer=response,\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            context_used=len(request.context) > 0,\n",
        "            history_used=len(request.history) > 0\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in generate endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Text generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/query\", response_model=QueryResponse)\n",
        "async def query_docs(request: QueryRequest):\n",
        "    \"\"\"Enhanced query endpoint using modular retriever service\"\"\"\n",
        "    try:\n",
        "        # Use base retriever service for single query\n",
        "        results = await base_retriever_service.retrieve_documents(request.query, request.top_k)\n",
        "        \n",
        "        return QueryResponse(\n",
        "            documents=results,\n",
        "            total_found=len(results),\n",
        "            query=request.query\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in query endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Document query failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/ask\", response_model=AskResponse)\n",
        "async def ask_question(request: AskRequest):\n",
        "    \"\"\"Enhanced RAG pipeline with fusion using RAGController\"\"\"\n",
        "    try:\n",
        "        print(f\"ü§ñ Processing question: {request.question[:100]}...\")\n",
        "        \n",
        "        # Use the RAG controller with fusion\n",
        "        result = await rag_controller.process_query(\n",
        "            question=request.question,\n",
        "            history=request.history,\n",
        "            use_fusion=FUSION_CONFIG[\"enabled\"]\n",
        "        )\n",
        "        \n",
        "        # Update chat history\n",
        "        session_id = request.session_id or \"default\"\n",
        "        if session_id not in chat_sessions:\n",
        "            chat_sessions[session_id] = []\n",
        "        \n",
        "        chat_sessions[session_id].append({\n",
        "            \"question\": request.question,\n",
        "            \"answer\": result['response'],\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "        \n",
        "        # Format response for WellnessGrid compatibility\n",
        "        return AskResponse(\n",
        "            response=result['response'],\n",
        "            sources=result['sources'],\n",
        "            chat_history=chat_sessions[session_id],\n",
        "            session_id=session_id,\n",
        "            mockMode=False,\n",
        "            metadata=result['metadata']\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Ask endpoint error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Question processing failed: {str(e)}\")\n",
        "\n",
        "@app.get(\"/chat/history/{session_id}\")\n",
        "async def get_chat_history(session_id: str):\n",
        "    \"\"\"Get chat history for a session\"\"\"\n",
        "    try:\n",
        "        history = chat_sessions.get(session_id, [])\n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"history\": history,\n",
        "            \"message_count\": len(history)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat/clear/{session_id}\")\n",
        "async def clear_chat_history(session_id: str):\n",
        "    \"\"\"Clear chat history for a session\"\"\"\n",
        "    try:\n",
        "        if session_id in chat_sessions:\n",
        "            del chat_sessions[session_id]\n",
        "        \n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"cleared\": True\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health_check():\n",
        "    \"\"\"Enhanced health check with memory monitoring\"\"\"\n",
        "    try:\n",
        "        # Test Supabase connection\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        \n",
        "        # Test embeddings\n",
        "        embed_result = supabase.table('document_embeddings').select('count').execute()\n",
        "        embed_count = len(embed_result.data) if embed_result.data else 0\n",
        "        \n",
        "        # Get GPU memory info\n",
        "        gpu_info = {\"available\": torch.cuda.is_available()}\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "            gpu_info.update({\n",
        "                \"allocated_gb\": f\"{allocated:.2f}\",\n",
        "                \"total_gb\": f\"{total:.2f}\",\n",
        "                \"usage_percent\": f\"{(allocated/total)*100:.1f}%\"\n",
        "            })\n",
        "        \n",
        "        return HealthResponse(\n",
        "            status=\"healthy\",\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            embedding_device=\"cpu\",\n",
        "            database=\"Supabase + pgvector\",\n",
        "            documents_in_db=doc_count,\n",
        "            embeddings_in_db=embed_count,\n",
        "            rag_system=\"fusion_enhanced\",\n",
        "            chat_support=True,\n",
        "            active_sessions=len(chat_sessions),\n",
        "            gpu_memory=gpu_info\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=503, detail=str(e))\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def status():\n",
        "    \"\"\"Detailed status endpoint\"\"\"\n",
        "    try:\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"models\": {\n",
        "                \"selected_model\": MODEL_INFO['name'],\n",
        "                \"model_loaded\": True,\n",
        "                \"embedding_model\": \"PubMedBERT\"\n",
        "            },\n",
        "            \"database\": {\n",
        "                \"connected\": True,\n",
        "                \"url\": supabase_url[:30] + \"...\" if supabase_url else \"not_set\"\n",
        "            },\n",
        "            \"rag_fusion\": {\n",
        "                \"enabled\": FUSION_CONFIG[\"enabled\"],\n",
        "                \"query_variations\": FUSION_CONFIG[\"max_query_variations\"],\n",
        "                \"rrf_k\": FUSION_CONFIG[\"rrf_k_parameter\"]\n",
        "            },\n",
        "            \"config\": CONFIG,\n",
        "            \"memory\": {\n",
        "                \"active_sessions\": len(chat_sessions),\n",
        "                \"session_ids\": list(chat_sessions.keys())\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "print(\"üåê Enhanced FastAPI endpoints configured:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']}\")\n",
        "print(\"  ‚úÖ POST /ask - Enhanced RAG endpoint with RAG Fusion\")\n",
        "print(\"  ‚úÖ POST /retrieve-fusion - RAG Fusion document retrieval\")\n",
        "print(\"  ‚úÖ POST /query - Single query document retrieval\")\n",
        "print(\"  ‚úÖ GET /health - Health check with fusion status\")\n",
        "print(\"  ‚úÖ GET /status - Detailed status with fusion configuration\")\n",
        "print(\"  ‚úÖ GET /chat/history/{session_id} - Get chat history\")\n",
        "print(\"  ‚úÖ POST /chat/clear/{session_id} - Clear chat history\")\n",
        "print(\"  ‚úÖ GET /docs - Auto-generated API documentation\")\n",
        "print(f\"‚úÖ Enhanced FastAPI server ready with RAG Fusion support!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test RAG Fusion\n",
        "print(\"üß™ Testing RAG Fusion...\")\n",
        "\n",
        "test_queries = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How to treat high blood pressure?\",\n",
        "    \"What causes heart attacks?\",\n",
        "    \"Symptoms of stroke\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nüîç Testing: {query}\")\n",
        "    \n",
        "    # Test single retrieval\n",
        "    single_docs = await base_retriever_service.retrieve_documents(query, 5)\n",
        "    print(f\"   Single retrieval: {len(single_docs)} docs\")\n",
        "    \n",
        "    # Test fusion retrieval\n",
        "    fusion_docs = await fusion_retriever.retrieve_documents_fusion(query, 5)\n",
        "    print(f\"   Fusion retrieval: {len(fusion_docs)} docs\")\n",
        "    \n",
        "    # Compare results\n",
        "    single_sources = set(doc.get('metadata', {}).get('source', '') for doc in single_docs)\n",
        "    fusion_sources = set(doc.get('metadata', {}).get('source', '') for doc in fusion_docs)\n",
        "    \n",
        "    new_sources = fusion_sources - single_sources\n",
        "    print(f\"   New sources found: {len(new_sources)}\")\n",
        "    \n",
        "    if new_sources:\n",
        "        print(f\"   New sources: {list(new_sources)[:3]}\")\n",
        "    \n",
        "    # Test full RAG pipeline\n",
        "    try:\n",
        "        rag_result = await rag_controller.process_query(query, use_fusion=True)\n",
        "        print(f\"   RAG response: {rag_result['response'][:100]}...\")\n",
        "        print(f\"   Sources used: {rag_result['metadata']['documentsUsed']}\")\n",
        "        print(f\"   Fusion enabled: {rag_result['metadata']['fusionUsed']}\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå RAG test failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n‚úÖ RAG Fusion testing complete!\")\n",
        "print(\"üéØ System is ready for deployment with enhanced retrieval capabilities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Memory Management Utilities\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Get detailed GPU memory information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"No GPU available\"\n",
        "    \n",
        "    device = torch.cuda.current_device()\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "    total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "    free = total - allocated\n",
        "    \n",
        "    return {\n",
        "        \"allocated\": f\"{allocated:.2f} GB\",\n",
        "        \"reserved\": f\"{reserved:.2f} GB\", \n",
        "        \"total\": f\"{total:.2f} GB\",\n",
        "        \"free\": f\"{free:.2f} GB\",\n",
        "        \"percentage_used\": f\"{(allocated/total)*100:.1f}%\"\n",
        "    }\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"üßπ GPU cache cleared\")\n",
        "    else:\n",
        "        print(\"üì± No GPU to clear\")\n",
        "\n",
        "# Show current GPU memory status\n",
        "print(\"üîç Current GPU Memory Status:\")\n",
        "memory_info = get_gpu_memory_info()\n",
        "if isinstance(memory_info, dict):\n",
        "    for key, value in memory_info.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "else:\n",
        "    print(f\"   {memory_info}\")\n",
        "\n",
        "# Add GPU memory management to Flask health endpoint\n",
        "print(\"‚úÖ GPU memory utilities ready!\")\n",
        "print(\"   Use get_gpu_memory_info() to check memory\")\n",
        "print(\"   Use clear_gpu_memory() to free cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Server startup with ngrok (optional for local development)\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Quick pre-check to give immediate feedback\n",
        "print(\"üîç Pre-flight check...\")\n",
        "required_vars = ['medical_model', 'tokenizer', 'supabase', 'rag_controller', 'CONFIG', 'MODEL_INFO']\n",
        "missing_vars = [var for var in globals() if var not in globals()]\n",
        "\n",
        "# Check if we have the essential components\n",
        "essential_components = ['MODEL_INFO', 'supabase', 'rag_controller']\n",
        "missing_essential = [comp for comp in essential_components if comp not in globals()]\n",
        "\n",
        "if missing_essential:\n",
        "    print(f\"‚ùå Missing essential components: {', '.join(missing_essential)}\")\n",
        "    print(f\"üîß Please run all previous cells in order first!\")\n",
        "    print(f\"   Then come back to this cell.\")\n",
        "    raise RuntimeError(f\"Required setup incomplete. Missing: {', '.join(missing_essential)}\")\n",
        "\n",
        "print(\"‚úÖ Pre-flight check passed!\")\n",
        "print(f\"üéØ Selected model: {MODEL_INFO['name']}\")\n",
        "\n",
        "# Setup ngrok if token is available\n",
        "if ngrok_token and COLAB_AVAILABLE:\n",
        "    print(\"üîë Using ngrok auth token from Colab secrets...\")\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "    print(\"‚úÖ Ngrok token set successfully!\")\n",
        "    \n",
        "    # Start ngrok tunnel\n",
        "    print(\"üåê Starting ngrok tunnel...\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"üåç Public URL: {public_url}\")\n",
        "    print(\"\udccb Copy this URL to your WellnessGrid app configuration!\")\n",
        "    \n",
        "elif ngrok_token and not COLAB_AVAILABLE:\n",
        "    print(\"üîë Using ngrok auth token from environment...\")\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "    print(\"‚úÖ Ngrok token set successfully!\")\n",
        "    \n",
        "    # Start ngrok tunnel\n",
        "    print(\"üåê Starting ngrok tunnel...\")\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(f\"üåç Public URL: {public_url}\")\n",
        "    print(\"üìã Copy this URL to your WellnessGrid app configuration!\")\n",
        "    \n",
        "else:\n",
        "    print(\"üì± No ngrok token found - server will run locally only\")\n",
        "    print(\"üåç Local URL: http://localhost:8000\")\n",
        "    print(\"üí° Set NGROK_AUTH_TOKEN environment variable to enable public access\")\n",
        "\n",
        "# Start FastAPI app with uvicorn\n",
        "print(\"üöÄ Starting FastAPI app with uvicorn...\")\n",
        "print(\"üì° Available endpoints:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings (required by WellnessGrid)\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} (required by WellnessGrid)\")\n",
        "print(\"  ‚úÖ POST /ask - Main RAG endpoint for WellnessGrid with RAG Fusion\")\n",
        "print(\"  ‚úÖ POST /retrieve-fusion - RAG Fusion document retrieval\")\n",
        "print(\"  ‚úÖ GET /health - Health check\")\n",
        "print(\"  ‚úÖ POST /query - Query documents from Supabase\")\n",
        "print(\"  ‚úÖ GET /docs - Interactive API documentation\")\n",
        "print(\"  ‚úÖ GET /redoc - Alternative API documentation\")\n",
        "\n",
        "if 'public_url' in locals():\n",
        "    print(f\"\\nüéØ IMPORTANT: Copy the ngrok URL above to your WellnessGrid .env.local:\")\n",
        "    print(f\"   FAST_API_URL={public_url}\")\n",
        "else:\n",
        "    print(f\"\\nüéØ For local development, use:\")\n",
        "    print(f\"   FAST_API_URL=http://localhost:8000\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
        "print(f\"\\nüöÄ Your WellnessGrid RAG system with {MODEL_INFO['name']} and RAG Fusion is now live!\")\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops in Jupyter\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the server\n",
        "try:\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚èπÔ∏è Server stopped by user\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Server error: {str(e)}\")\n",
        "    print(\"üîß Try restarting the notebook if you encounter issues\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
