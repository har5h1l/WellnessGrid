{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# WellnessGrid RAG System - Multi-Model Edition\n",
        "\n",
        "This notebook demonstrates a complete RAG (Retrieval-Augmented Generation) system for medical questions using:\n",
        "\n",
        "1. ü§ñ **Selectable Medical Models** for advanced medical text generation\n",
        "2. üóÑÔ∏è **Supabase + pgvector** for document retrieval  \n",
        "3. üåê **Flask API with ngrok** for external access\n",
        "4. üîç **Pre-embedded medical documents** from your database\n",
        "\n",
        "## Available Medical Models:\n",
        "- **OpenBioLLM-8B**: 8B parameter medical LLM optimized for biomedical tasks\n",
        "- **Med42-v2-8B**: 8B parameter medical model from M42 Health\n",
        "- **MedGemma-4B**: 4B parameter medical model based on Gemma architecture\n",
        "\n",
        "## Features:\n",
        "- **Model Selection**: Choose from multiple specialized medical LLMs\n",
        "- **Vector Search**: Supabase pgvector with existing embeddings\n",
        "- **Flask API**: Compatible with WellnessGrid frontend\n",
        "- **Google Colab**: GPU-accelerated inference\n",
        "- **ngrok**: Public URL for external access\n",
        "- **Fallback Support**: Automatic fallback to stable models if loading fails\n",
        "\n",
        "## Recent Updates (Multi-Model Support):\n",
        "- ‚úÖ **Model Selection**: Interactive choice between 3 medical models\n",
        "- ‚úÖ **Dynamic Loading**: Only loads the selected model to save memory\n",
        "- ‚úÖ **Fallback Support**: Automatic fallback if preferred model fails\n",
        "- ‚úÖ **Enhanced Error Handling**: Better validation and error messages\n",
        "- ‚úÖ **Updated API**: All endpoints reflect the selected model\n",
        "\n",
        "## Setup Instructions:\n",
        "1. Run this notebook in Google Colab with GPU enabled\n",
        "2. Execute cells in order and select your preferred medical model when prompted\n",
        "3. Enter your Supabase credentials and ngrok auth token when prompted  \n",
        "4. Use the generated ngrok URL in your WellnessGrid app\n",
        "\n",
        "## Prerequisites:\n",
        "- Supabase database with `medical_documents` and `document_embeddings` tables\n",
        "- Documents embedded using `embed_documents.py` script\n",
        "- RPC function `search_embeddings` deployed in Supabase\n",
        "- **IMPORTANT**: Use `service_role` API key (NOT `anon` key)\n",
        "\n",
        "## üîë How to Get Your Supabase Credentials:\n",
        "1. **Go to your Supabase project dashboard**\n",
        "2. **Click \"Settings\" ‚Üí \"API\"**\n",
        "3. **Copy \"Project URL\"** (starts with `https://`)\n",
        "4. **Copy \"service_role\" key** (NOT the anon key!)\n",
        "   - ‚úÖ `service_role`: Long key starting with `eyJ...` (200+ chars)\n",
        "   - ‚ùå `anon`: Shorter public key (WRONG for this notebook)\n",
        "\n",
        "**‚ö° RAG system using your existing Supabase embeddings**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Google Colab\n",
        "%pip install transformers torch sentence-transformers --quiet\n",
        "%pip install supabase python-dotenv --quiet\n",
        "%pip install sacremoses --quiet\n",
        "%pip install fastapi uvicorn pydantic python-multipart --quiet\n",
        "%pip install pyngrok --quiet\n",
        "%pip install psutil --quiet  # For memory management\n",
        "%pip install nest-asyncio --quiet  # Add this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import logging\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "import torch\n",
        "from getpass import getpass\n",
        "\n",
        "# AI Models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "# Supabase\n",
        "from supabase import create_client\n",
        "\n",
        "# FastAPI\n",
        "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from fastapi.responses import JSONResponse\n",
        "import uvicorn\n",
        "\n",
        "# Pydantic models for validation\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Colab secrets\n",
        "from google.colab import userdata\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"üì¶ All packages imported successfully!\")\n",
        "print(f\"üïê RAG session started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"üîß Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
        "\n",
        "# Available medical models\n",
        "MEDICAL_MODELS = {\n",
        "    \"1\": {\n",
        "        \"name\": \"OpenBioLLM-8B\",\n",
        "        \"path\": \"aaditya/OpenBioLLM-Llama3-8B\",\n",
        "        \"description\": \"8B parameter medical LLM optimized for biomedical tasks\",\n",
        "        \"type\": \"causal\"\n",
        "    },\n",
        "    \"2\": {\n",
        "        \"name\": \"Med42-v2-8B\",\n",
        "        \"path\": \"m42-health/Llama3-Med42-8B\",\n",
        "        \"description\": \"8B parameter medical model from M42 Health based on Llama3\",\n",
        "        \"type\": \"causal\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"\\nü§ñ Available Medical Models:\")\n",
        "for key, model in MEDICAL_MODELS.items():\n",
        "    print(f\"  {key}. {model['name']} - {model['description']}\")\n",
        "\n",
        "print(\"\\nPlease select a model by entering the number (1 or 2):\")\n",
        "print(\"Note: MedGemma-4B is available in a separate notebook (query_rag_medgemma.ipynb)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pydantic models for request/response validation\n",
        "class AskRequest(BaseModel):\n",
        "    question: str = Field(..., min_length=1, max_length=1000, description=\"The medical question to ask\")\n",
        "    session_id: Optional[str] = Field(default=\"default\", description=\"Chat session identifier\")\n",
        "    history: Optional[List[dict]] = Field(default=[], description=\"Previous chat history\")\n",
        "\n",
        "class AskResponse(BaseModel):\n",
        "    response: str = Field(..., description=\"AI generated response\")\n",
        "    sources: List[dict] = Field(..., description=\"Source documents used\")\n",
        "    chat_history: List[dict] = Field(..., description=\"Updated chat history\")\n",
        "    session_id: str = Field(..., description=\"Session identifier\")\n",
        "    mockMode: bool = Field(default=False, description=\"Whether response is from mock mode\")\n",
        "    metadata: dict = Field(..., description=\"Response metadata\")\n",
        "\n",
        "class EmbedRequest(BaseModel):\n",
        "    text: str = Field(..., min_length=1, max_length=5000, description=\"Text to embed\")\n",
        "\n",
        "class EmbedResponse(BaseModel):\n",
        "    embedding: List[float] = Field(..., description=\"Generated embedding vector\")\n",
        "    dimensions: int = Field(..., description=\"Embedding dimensions\")\n",
        "    model: str = Field(..., description=\"Model used for embedding\")\n",
        "    device: str = Field(..., description=\"Device used for embedding\")\n",
        "\n",
        "class GenerateRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Query text\")\n",
        "    context: str = Field(default=\"\", description=\"Context information\")\n",
        "    history: List[dict] = Field(default=[], description=\"Chat history\")\n",
        "    max_tokens: int = Field(default=200, ge=1, le=1000, description=\"Maximum tokens to generate\")\n",
        "    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description=\"Generation temperature\")\n",
        "\n",
        "class GenerateResponse(BaseModel):\n",
        "    answer: str = Field(..., description=\"Generated answer\")\n",
        "    model: str = Field(..., description=\"Model used for generation\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    context_used: bool = Field(..., description=\"Whether context was used\")\n",
        "    history_used: bool = Field(..., description=\"Whether history was used\")\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str = Field(..., min_length=1, max_length=1000, description=\"Search query\")\n",
        "    top_k: int = Field(default=5, ge=1, le=20, description=\"Number of documents to retrieve\")\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    documents: List[dict] = Field(..., description=\"Retrieved documents\")\n",
        "    total_found: int = Field(..., description=\"Total documents found\")\n",
        "    query: str = Field(..., description=\"Original query\")\n",
        "\n",
        "class HealthResponse(BaseModel):\n",
        "    status: str = Field(..., description=\"System status\")\n",
        "    model: str = Field(..., description=\"Active model name\")\n",
        "    model_path: str = Field(..., description=\"Model path\")\n",
        "    embedding_device: str = Field(..., description=\"Embedding device\")\n",
        "    database: str = Field(..., description=\"Database type\")\n",
        "    documents_in_db: int = Field(..., description=\"Number of documents in database\")\n",
        "    embeddings_in_db: int = Field(..., description=\"Number of embeddings in database\")\n",
        "    rag_system: str = Field(..., description=\"RAG system type\")\n",
        "    chat_support: bool = Field(..., description=\"Chat support status\")\n",
        "    active_sessions: int = Field(..., description=\"Number of active sessions\")\n",
        "    gpu_memory: dict = Field(..., description=\"GPU memory information\")\n",
        "\n",
        "print(\"‚úÖ Pydantic models defined for request/response validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory Management and Background Processing\n",
        "import gc\n",
        "import psutil\n",
        "from typing import Optional\n",
        "import asyncio  # Add this line\n",
        "\n",
        "class MemoryManager:\n",
        "    \"\"\"Manages GPU and CPU memory efficiently\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.gpu_memory_threshold = 0.8  # 80% GPU memory usage\n",
        "        self.cpu_memory_threshold = 0.9  # 90% CPU memory usage\n",
        "    \n",
        "    async def check_gpu_memory(self) -> bool:\n",
        "        \"\"\"Check if GPU memory is available\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                allocated = torch.cuda.memory_allocated()\n",
        "                total = torch.cuda.get_device_properties(0).total_memory\n",
        "                usage_ratio = allocated / total\n",
        "                return usage_ratio < self.gpu_memory_threshold\n",
        "            except:\n",
        "                return False\n",
        "        return True\n",
        "    \n",
        "    async def check_cpu_memory(self) -> bool:\n",
        "        \"\"\"Check if CPU memory is available\"\"\"\n",
        "        try:\n",
        "            memory = psutil.virtual_memory()\n",
        "            return memory.percent < (self.cpu_memory_threshold * 100)\n",
        "        except:\n",
        "            return True\n",
        "    \n",
        "    async def clear_gpu_cache(self):\n",
        "        \"\"\"Clear GPU memory cache\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            print(\"üßπ GPU cache cleared\")\n",
        "    \n",
        "    async def clear_cpu_cache(self):\n",
        "        \"\"\"Clear CPU memory cache\"\"\"\n",
        "        gc.collect()\n",
        "        print(\"üßπ CPU cache cleared\")\n",
        "    \n",
        "    async def get_memory_status(self) -> dict:\n",
        "        \"\"\"Get current memory status\"\"\"\n",
        "        status = {\n",
        "            \"gpu_available\": torch.cuda.is_available(),\n",
        "            \"cpu_memory_percent\": psutil.virtual_memory().percent\n",
        "        }\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            try:\n",
        "                allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "                total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "                status.update({\n",
        "                    \"gpu_allocated_gb\": f\"{allocated:.2f}\",\n",
        "                    \"gpu_total_gb\": f\"{total:.2f}\",\n",
        "                    \"gpu_usage_percent\": f\"{(allocated/total)*100:.1f}%\"\n",
        "                })\n",
        "            except:\n",
        "                status[\"gpu_info\"] = \"unavailable\"\n",
        "        \n",
        "        return status\n",
        "\n",
        "class BackgroundTaskManager:\n",
        "    \"\"\"Manages background tasks and queues\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.embedding_queue = asyncio.Queue(maxsize=100)\n",
        "        self.generation_queue = asyncio.Queue(maxsize=50)\n",
        "        self.memory_manager = MemoryManager()\n",
        "        self.running = False\n",
        "    \n",
        "    async def start_background_workers(self):\n",
        "        \"\"\"Start background workers for embedding and generation\"\"\"\n",
        "        self.running = True\n",
        "        \n",
        "        # Start embedding worker\n",
        "        asyncio.create_task(self._embedding_worker())\n",
        "        \n",
        "        # Start generation worker\n",
        "        asyncio.create_task(self._generation_worker())\n",
        "        \n",
        "        print(\"‚úÖ Background workers started\")\n",
        "    \n",
        "    async def stop_background_workers(self):\n",
        "        \"\"\"Stop background workers\"\"\"\n",
        "        self.running = False\n",
        "        print(\"ÔøΩÔøΩ Background workers stopped\")\n",
        "    \n",
        "    async def _embedding_worker(self):\n",
        "        \"\"\"Background worker for embedding tasks\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Check memory before processing\n",
        "                if not await self.memory_manager.check_cpu_memory():\n",
        "                    await asyncio.sleep(1)\n",
        "                    continue\n",
        "                \n",
        "                # Process embedding tasks\n",
        "                if not self.embedding_queue.empty():\n",
        "                    task = await self.embedding_queue.get()\n",
        "                    # Process task here\n",
        "                    self.embedding_queue.task_done()\n",
        "                \n",
        "                await asyncio.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Embedding worker error: {str(e)}\")\n",
        "                await asyncio.sleep(1)\n",
        "    \n",
        "    async def _generation_worker(self):\n",
        "        \"\"\"Background worker for generation tasks\"\"\"\n",
        "        while self.running:\n",
        "            try:\n",
        "                # Check GPU memory before processing\n",
        "                if not await self.memory_manager.check_gpu_memory():\n",
        "                    await asyncio.sleep(1)\n",
        "                    continue\n",
        "                \n",
        "                # Process generation tasks\n",
        "                if not self.generation_queue.empty():\n",
        "                    task = await self.generation_queue.get()\n",
        "                    # Process task here\n",
        "                    self.generation_queue.task_done()\n",
        "                \n",
        "                await asyncio.sleep(0.1)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Generation worker error: {str(e)}\")\n",
        "                await asyncio.sleep(1)\n",
        "\n",
        "# Initialize memory and background managers\n",
        "print(\"üóÉÔ∏è Initializing memory and background managers...\")\n",
        "memory_manager = MemoryManager()\n",
        "background_manager = BackgroundTaskManager()\n",
        "\n",
        "# Start background workers\n",
        "await background_manager.start_background_workers()\n",
        "\n",
        "print(\"‚úÖ Memory and background managers initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model selection and loading\n",
        "print(\"ü§ñ Select your medical model:\")\n",
        "model_choice = input(\"Enter model number (1 or 2): \").strip()\n",
        "\n",
        "if model_choice not in MEDICAL_MODELS:\n",
        "    print(f\"‚ùå Invalid choice '{model_choice}'. Defaulting to OpenBioLLM-8B (option 1)\")\n",
        "    model_choice = \"1\"\n",
        "\n",
        "selected_model = MEDICAL_MODELS[model_choice]\n",
        "model_name = selected_model[\"name\"]\n",
        "model_path = selected_model[\"path\"]\n",
        "model_type = selected_model[\"type\"]\n",
        "\n",
        "print(f\"üß† Loading {model_name} for medical text generation...\")\n",
        "print(f\"üì¶ Model path: {model_path}\")\n",
        "print(f\"üîß Model type: {model_type}\")\n",
        "\n",
        "# Check GPU memory before loading\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîç GPU Memory before loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "\n",
        "try:\n",
        "    # Standard causal LM handling (models 1 and 2 are both causal models)\n",
        "    print(\"üî§ Loading tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "    \n",
        "    print(\"üß† Loading model (this may take a few minutes)...\")\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Set pad token if not available\n",
        "    if hasattr(tokenizer, 'pad_token') and tokenizer.pad_token is None:\n",
        "        if tokenizer.eos_token is not None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        else:\n",
        "            tokenizer.pad_token = tokenizer.unk_token\n",
        "    \n",
        "    # Store model info globally\n",
        "    MODEL_INFO = {\n",
        "        \"name\": model_name,\n",
        "        \"path\": model_path,\n",
        "        \"choice\": model_choice,\n",
        "        \"type\": model_type\n",
        "    }\n",
        "    \n",
        "    # Check GPU memory after loading\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"üîç GPU Memory after loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB allocated, {torch.cuda.memory_reserved()/1024**3:.2f} GB reserved\")\n",
        "    \n",
        "    print(f\"‚úÖ {model_name} loaded and ready on {device}\")\n",
        "    print(f\"üéØ Selected model: {model_name}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading {model_name}: {str(e)}\")\n",
        "    print(\"üîÑ Falling back to a smaller model...\")\n",
        "    \n",
        "    # Fallback to a more reliable model\n",
        "    fallback_path = \"microsoft/DialoGPT-medium\"\n",
        "    print(f\"üîÑ Loading fallback model: {fallback_path}\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(fallback_path)\n",
        "    medical_model = AutoModelForCausalLM.from_pretrained(\n",
        "        fallback_path,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    \n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    MODEL_INFO = {\n",
        "        \"name\": \"DialoGPT-medium (Fallback)\",\n",
        "        \"path\": fallback_path,\n",
        "        \"choice\": \"fallback\"\n",
        "    }\n",
        "    \n",
        "    print(f\"‚úÖ Fallback model loaded successfully\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Pre-load the embedding model to avoid CUDA errors during Flask requests\n",
        "print(\"üîç Pre-loading embedding model to avoid CUDA memory conflicts...\")\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    \n",
        "    # Check if there's enough GPU memory for the embedding model\n",
        "    if torch.cuda.is_available():\n",
        "        total_memory = torch.cuda.get_device_properties(0).total_memory\n",
        "        allocated_memory = torch.cuda.memory_allocated()\n",
        "        free_memory = total_memory - allocated_memory\n",
        "        \n",
        "        print(f\"üîç Available GPU memory: {free_memory/1024**3:.2f} GB\")\n",
        "        \n",
        "        # If less than 2GB free, use CPU for embeddings\n",
        "        if free_memory < 2 * 1024**3:  # 2GB threshold\n",
        "            print(\"‚ö†Ô∏è Limited GPU memory - using CPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            EMBEDDING_DEVICE = 'cpu'\n",
        "        else:\n",
        "            print(\"‚úÖ Sufficient GPU memory - using GPU for embedding model\")\n",
        "            embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings')\n",
        "            EMBEDDING_DEVICE = 'cuda'\n",
        "    else:\n",
        "        print(\"üì± No GPU available - using CPU for embedding model\")\n",
        "        embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "        EMBEDDING_DEVICE = 'cpu'\n",
        "    \n",
        "    print(f\"‚úÖ Embedding model loaded on {EMBEDDING_DEVICE}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error pre-loading embedding model: {str(e)}\")\n",
        "    print(\"üîÑ Will load embedding model on-demand with CPU fallback\")\n",
        "    embedding_model = None\n",
        "    EMBEDDING_DEVICE = 'cpu'\n",
        "\n",
        "# Setup Supabase connection using Colab secrets\n",
        "print(\"üóÑÔ∏è Setting up Supabase connection using Colab secrets...\")\n",
        "print(\"üìã Required Colab secrets:\")\n",
        "print(\"   1. SUPABASE_URL - Your project URL (e.g., https://abc123.supabase.co)\")\n",
        "print(\"   2. SUPABASE_SERVICE_ROLE_KEY - Your service role key (NOT anon key)\")\n",
        "print(\"   3. NGROK_AUTH_TOKEN - Your ngrok authentication token\")\n",
        "print(\"\")\n",
        "print(\"üîë To set these secrets:\")\n",
        "print(\"   1. Click the üîë key icon in the left sidebar\")\n",
        "print(\"   2. Add the three secrets listed above\")\n",
        "print(\"   3. Re-run this cell\")\n",
        "print(\"\")\n",
        "\n",
        "try:\n",
        "    supabase_url = userdata.get('SUPABASE_URL')\n",
        "    supabase_key = userdata.get('SUPABASE_SERVICE_ROLE_KEY')\n",
        "    ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    \n",
        "    print(\"‚úÖ Successfully retrieved secrets from Colab\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error retrieving secrets: {str(e)}\")\n",
        "    print(\"üîß Make sure you've added the required secrets in Colab:\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_URL\")\n",
        "    print(\"   ‚Ä¢ SUPABASE_SERVICE_ROLE_KEY\") \n",
        "    print(\"   ‚Ä¢ NGROK_AUTH_TOKEN\")\n",
        "    raise\n",
        "\n",
        "# Validate the inputs\n",
        "if not supabase_url or not supabase_key:\n",
        "    raise ValueError(\"‚ùå Both Supabase URL and Service Role Key are required!\")\n",
        "\n",
        "if not supabase_url.startswith('https://'):\n",
        "    raise ValueError(\"‚ùå Supabase URL should start with 'https://'\")\n",
        "\n",
        "if not supabase_key.startswith('eyJ'):\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys typically start with 'eyJ'\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "    \n",
        "if len(supabase_key) < 100:\n",
        "    print(\"‚ö†Ô∏è WARNING: Service role keys are typically very long (200+ characters)\")\n",
        "    print(\"   You might be using the anon key instead of service_role key\")\n",
        "\n",
        "try:\n",
        "    supabase = create_client(supabase_url, supabase_key)\n",
        "    print(\"‚úÖ Supabase client initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to initialize Supabase client: {str(e)}\")\n",
        "    print(\"üîß Common issues:\")\n",
        "    print(\"   ‚Ä¢ Wrong API key type (use service_role, not anon)\")\n",
        "    print(\"   ‚Ä¢ Typo in URL or key\")\n",
        "    print(\"   ‚Ä¢ Key might be expired or regenerated\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"top_k\": 5,\n",
        "    \"similarity_threshold\": 0.5,\n",
        "    \"max_context_length\": 2000,\n",
        "    \"max_response_length\": 150,\n",
        "}\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è RAG Configuration:\")\n",
        "print(f\"   üéØ Retrieve top {CONFIG['top_k']} similar documents\")\n",
        "print(f\"   üìä Similarity threshold: {CONFIG['similarity_threshold']}\")\n",
        "print(f\"   üìè Max context length: {CONFIG['max_context_length']} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modular Service Architecture\n",
        "import asyncio\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class BaseService(ABC):\n",
        "    \"\"\"Base class for all services\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.executor = ThreadPoolExecutor(max_workers=2)\n",
        "    \n",
        "    async def run_in_executor(self, func, *args):\n",
        "        \"\"\"Run CPU-intensive tasks in thread pool\"\"\"\n",
        "        loop = asyncio.get_event_loop()\n",
        "        return await loop.run_in_executor(self.executor, func, *args)\n",
        "\n",
        "class EmbedderService(BaseService):\n",
        "    \"\"\"Handles text embedding generation on CPU\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = None\n",
        "        self.device = 'cpu'\n",
        "    \n",
        "    async def initialize(self):\n",
        "        \"\"\"Initialize embedding model on CPU\"\"\"\n",
        "        if self.model is None:\n",
        "            from sentence_transformers import SentenceTransformer\n",
        "            self.model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "            print(\"‚úÖ EmbedderService: Model loaded on CPU\")\n",
        "    \n",
        "    async def embed_text(self, text: str) -> List[float]:\n",
        "        \"\"\"Generate embeddings for text\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run embedding on CPU thread pool\n",
        "        embedding = await self.run_in_executor(self.model.encode, [text])\n",
        "        return embedding[0].tolist()\n",
        "    \n",
        "    async def embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Generate embeddings for multiple texts\"\"\"\n",
        "        await self.initialize()\n",
        "        \n",
        "        # Run batch embedding on CPU thread pool\n",
        "        embeddings = await self.run_in_executor(self.model.encode, texts)\n",
        "        return embeddings.tolist()\n",
        "\n",
        "class RetrieverService(BaseService):\n",
        "    \"\"\"Handles document retrieval from Supabase\"\"\"\n",
        "    \n",
        "    def __init__(self, supabase_client, embedder_service: EmbedderService):\n",
        "        super().__init__()\n",
        "        self.supabase = supabase_client\n",
        "        self.embedder = embedder_service\n",
        "        self.top_k = 5\n",
        "        self.similarity_threshold = 0.5\n",
        "    \n",
        "    async def retrieve_documents(self, query: str, top_k: int = None) -> List[dict]:\n",
        "        \"\"\"Retrieve relevant documents using vector search\"\"\"\n",
        "        top_k = top_k or self.top_k\n",
        "        \n",
        "        # Generate query embedding\n",
        "        query_embedding = await self.embedder.embed_text(query)\n",
        "        \n",
        "        # Search Supabase\n",
        "        try:\n",
        "            result = self.supabase.rpc('search_embeddings', {\n",
        "                'query_embedding': query_embedding,\n",
        "                'match_threshold': self.similarity_threshold,\n",
        "                'match_count': top_k\n",
        "            }).execute()\n",
        "            \n",
        "            if result.data:\n",
        "                documents = []\n",
        "                for i, doc in enumerate(result.data):\n",
        "                    documents.append({\n",
        "                        'content': doc.get('chunk_content', ''),\n",
        "                        'similarity_score': doc.get('similarity', 0.0),\n",
        "                        'metadata': {\n",
        "                            'title': doc.get('title', 'Medical Document'),\n",
        "                            'source': doc.get('source', 'unknown'),\n",
        "                            'topic': doc.get('topic', 'general'),\n",
        "                            'document_type': doc.get('document_type', 'unknown'),\n",
        "                            'document_id': doc.get('document_id', '')\n",
        "                        },\n",
        "                        'rank': i + 1,\n",
        "                        'doc_id': doc.get('document_id', '')\n",
        "                    })\n",
        "                return documents\n",
        "            else:\n",
        "                return []\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RetrieverService error: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "class GeneratorService(BaseService):\n",
        "    \"\"\"Handles text generation using medical LLMs on GPU\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str, tokenizer, medical_model):\n",
        "        super().__init__()\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = tokenizer\n",
        "        self.medical_model = medical_model\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    async def generate_response(self, prompt: str, max_tokens: int = 150) -> str:\n",
        "        \"\"\"Generate medical response using the selected model\"\"\"\n",
        "        try:\n",
        "            # Run generation on GPU thread pool\n",
        "            response = await self.run_in_executor(self._generate_sync, prompt, max_tokens)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå GeneratorService error: {str(e)}\")\n",
        "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
        "    \n",
        "    def _generate_sync(self, prompt: str, max_tokens: int) -> str:\n",
        "        \"\"\"Synchronous generation method for thread pool\"\"\"\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "        \n",
        "        input_len = inputs['input_ids'].shape[1]\n",
        "        \n",
        "        generation_params = {\n",
        "            \"max_new_tokens\": max_tokens,\n",
        "            \"temperature\": 0.7,\n",
        "            \"do_sample\": True,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"top_p\": 0.9\n",
        "        }\n",
        "        \n",
        "        if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:\n",
        "            generation_params[\"pad_token_id\"] = self.tokenizer.pad_token_id\n",
        "        if hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
        "            generation_params[\"eos_token_id\"] = self.tokenizer.eos_token_id\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.medical_model.generate(**inputs, **generation_params)\n",
        "        \n",
        "        response = self.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True)\n",
        "        return response.strip()\n",
        "\n",
        "class RAGController:\n",
        "    \"\"\"Orchestrates the complete RAG pipeline\"\"\"\n",
        "    \n",
        "    def __init__(self, embedder: EmbedderService, retriever: RetrieverService, generator: GeneratorService):\n",
        "        self.embedder = embedder\n",
        "        self.retriever = retriever\n",
        "        self.generator = generator\n",
        "        self.max_context_length = 2000\n",
        "    \n",
        "    async def process_query(self, question: str, history: List[dict] = None) -> dict:\n",
        "        \"\"\"Complete RAG pipeline: retrieve context and generate response\"\"\"\n",
        "        try:\n",
        "            print(f\"üîç Processing query: {question}\")\n",
        "            \n",
        "            # Step 1: Retrieve relevant documents\n",
        "            retrieved_docs = await self.retriever.retrieve_documents(question)\n",
        "            print(f\"üìä Found {len(retrieved_docs)} similar documents\")\n",
        "            \n",
        "            # Step 2: Build context from documents\n",
        "            context_parts = []\n",
        "            total_chars = 0\n",
        "            \n",
        "            for doc in retrieved_docs:\n",
        "                if total_chars + len(doc['content']) <= self.max_context_length:\n",
        "                    context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                    total_chars += len(doc['content'])\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            context = \"\\n\\n\".join(context_parts)\n",
        "            print(f\"ÔøΩÔøΩ Using {len(context_parts)} documents for context ({len(context)} chars)\")\n",
        "            \n",
        "            # Step 3: Create prompt with history\n",
        "            if history:\n",
        "                history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in history[-3:]])\n",
        "                prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {question}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "            else:\n",
        "                prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "            \n",
        "            # Step 4: Generate response\n",
        "            print(f\"ü§ñ Generating response using {self.generator.model_name}...\")\n",
        "            response = await self.generator.generate_response(prompt, 150)\n",
        "            \n",
        "            # Step 5: Format result\n",
        "            result = {\n",
        "                'query': question,\n",
        "                'response': response,\n",
        "                'sources': [\n",
        "                    {\n",
        "                        'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                        'source': doc['metadata']['source'],\n",
        "                        'topic': doc['metadata']['topic'],\n",
        "                        'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                        'rank': doc['rank'],\n",
        "                        'content_preview': doc['content'][:150] + \"...\"\n",
        "                    }\n",
        "                    for doc in retrieved_docs\n",
        "                ],\n",
        "                'metadata': {\n",
        "                    'documentsUsed': len(context_parts),\n",
        "                    'totalFound': len(retrieved_docs),\n",
        "                    'contextLength': len(context),\n",
        "                    'model': self.generator.model_name,\n",
        "                    'processingTime': datetime.now().isoformat()\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAGController error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Initialize RAG Fusion Services\n",
        "print(\"üîß Initializing RAG Fusion services...\")\n",
        "\n",
        "# Initialize base services\n",
        "embedder_service = EmbedderService()\n",
        "base_retriever_service = RetrieverService(supabase, embedder_service)\n",
        "generator_service = GeneratorService(MODEL_INFO['name'], tokenizer, medical_model)\n",
        "\n",
        "# Initialize RAG Fusion components\n",
        "query_reformulator = QueryReformulatorService()  # No API key for now - uses rule-based fallback\n",
        "fusion_retriever = FusionRetrieverService(base_retriever_service, query_reformulator)\n",
        "\n",
        "# Create enhanced RAG controller with fusion\n",
        "enhanced_rag_controller = RAGController(embedder_service, fusion_retriever, generator_service)\n",
        "\n",
        "# Update main references\n",
        "rag_controller = enhanced_rag_controller\n",
        "rag_system = enhanced_rag_controller\n",
        "\n",
        "print(\"‚úÖ RAG Fusion services initialized!\")\n",
        "print(f\"   üìç QueryReformulatorService: Multi-query generation\")\n",
        "print(f\"   üìç FusionRetrieverService: RRF-based document fusion\")\n",
        "print(f\"   üìç Enhanced RAGController: Fusion-enabled pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Fusion: Query Reformulator Service\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from typing import List, Dict, Any\n",
        "import json\n",
        "\n",
        "class QueryReformulatorService(BaseService):\n",
        "    \"\"\"Generates multiple query variations for RAG Fusion\"\"\"\n",
        "    \n",
        "    def __init__(self, openrouter_api_key: str = None):\n",
        "        super().__init__()\n",
        "        self.openrouter_api_key = openrouter_api_key\n",
        "        self.base_url = \"https://openrouter.ai/api/v1\"\n",
        "        self.model = \"anthropic/claude-3-haiku\"  # Fast and reliable\n",
        "        \n",
        "    async def reformulate_query(self, original_query: str, num_variations: int = 3) -> List[str]:\n",
        "        \"\"\"Generate multiple query variations using LLM\"\"\"\n",
        "        try:\n",
        "            # Simple prompt for query reformulation\n",
        "            prompt = f\"\"\"You are a medical query enhancement specialist. Create {num_variations} different ways to ask this medical question for better information retrieval.\n",
        "\n",
        "Original query: \"{original_query}\"\n",
        "\n",
        "Generate {num_variations} variations that:\n",
        "1. Use different medical terminology\n",
        "2. Include synonyms and related terms\n",
        "3. Focus on different aspects (symptoms, causes, treatments, etc.)\n",
        "4. Are concise and clear\n",
        "\n",
        "Return only the variations, one per line:\"\"\"\n",
        "\n",
        "            if self.openrouter_api_key:\n",
        "                # Use OpenRouter API\n",
        "                variations = await self._call_openrouter_api(prompt, num_variations)\n",
        "            else:\n",
        "                # Fallback to simple rule-based reformulation\n",
        "                variations = await self._rule_based_reformulation(original_query, num_variations)\n",
        "            \n",
        "            # Always include original query\n",
        "            all_queries = [original_query] + variations\n",
        "            return all_queries[:num_variations + 1]  # Limit total queries\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Query reformulation failed: {str(e)}\")\n",
        "            # Fallback to original query only\n",
        "            return [original_query]\n",
        "    \n",
        "    async def _call_openrouter_api(self, prompt: str, num_variations: int) -> List[str]:\n",
        "        \"\"\"Call OpenRouter API for query reformulation\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"Authorization\": f\"Bearer {self.openrouter_api_key}\",\n",
        "                \"Content-Type\": \"application/json\"\n",
        "            }\n",
        "            \n",
        "            payload = {\n",
        "                \"model\": self.model,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"max_tokens\": 200,\n",
        "                \"temperature\": 0.7\n",
        "            }\n",
        "            \n",
        "            async with aiohttp.ClientSession() as session:\n",
        "                async with session.post(\n",
        "                    f\"{self.base_url}/chat/completions\",\n",
        "                    headers=headers,\n",
        "                    json=payload,\n",
        "                    timeout=10\n",
        "                ) as response:\n",
        "                    if response.status == 200:\n",
        "                        result = await response.json()\n",
        "                        content = result['choices'][0]['message']['content']\n",
        "                        \n",
        "                        # Parse variations from response\n",
        "                        variations = []\n",
        "                        lines = content.strip().split('\\n')\n",
        "                        for line in lines:\n",
        "                            line = line.strip()\n",
        "                            if line and not line.startswith(('#', '-', '*', '1.', '2.', '3.')):\n",
        "                                variations.append(line)\n",
        "                        \n",
        "                        return variations[:num_variations]\n",
        "                    else:\n",
        "                        raise Exception(f\"API call failed: {response.status}\")\n",
        "                        \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå OpenRouter API call failed: {str(e)}\")\n",
        "            raise\n",
        "    \n",
        "    async def _rule_based_reformulation(self, query: str, num_variations: int) -> List[str]:\n",
        "        \"\"\"Simple rule-based query reformulation as fallback\"\"\"\n",
        "        variations = []\n",
        "        \n",
        "        # Medical synonyms mapping\n",
        "        medical_synonyms = {\n",
        "            'diabetes': ['diabetes mellitus', 'blood sugar', 'glucose', 'type 2 diabetes'],\n",
        "            'hypertension': ['high blood pressure', 'elevated blood pressure', 'HTN'],\n",
        "            'heart attack': ['myocardial infarction', 'MI', 'cardiac arrest', 'heart failure'],\n",
        "            'stroke': ['cerebrovascular accident', 'CVA', 'brain attack'],\n",
        "            'cancer': ['malignancy', 'tumor', 'neoplasm', 'carcinoma'],\n",
        "            'pain': ['discomfort', 'ache', 'soreness', 'tenderness'],\n",
        "            'fever': ['elevated temperature', 'pyrexia', 'hyperthermia'],\n",
        "            'fatigue': ['tiredness', 'exhaustion', 'weakness', 'lethargy'],\n",
        "            'nausea': ['sick to stomach', 'queasiness', 'upset stomach'],\n",
        "            'headache': ['head pain', 'migraine', 'cephalalgia']\n",
        "        }\n",
        "        \n",
        "        # Generate variations using synonyms\n",
        "        for term, synonyms in medical_synonyms.items():\n",
        "            if term.lower() in query.lower() and len(variations) < num_variations:\n",
        "                for synonym in synonyms[:2]:  # Use max 2 synonyms per term\n",
        "                    variation = query.lower().replace(term.lower(), synonym)\n",
        "                    if variation != query.lower():\n",
        "                        variations.append(variation.capitalize())\n",
        "                        if len(variations) >= num_variations:\n",
        "                            break\n",
        "        \n",
        "        # Add question format variations\n",
        "        if len(variations) < num_variations:\n",
        "            if '?' not in query:\n",
        "                variations.append(f\"{query}?\")\n",
        "            if 'what' not in query.lower():\n",
        "                variations.append(f\"What are {query.lower()}?\")\n",
        "            if 'how' not in query.lower():\n",
        "                variations.append(f\"How to {query.lower()}?\")\n",
        "        \n",
        "        return variations[:num_variations]\n",
        "\n",
        "print(\"‚úÖ QueryReformulatorService ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Fusion: Fusion Retriever Service\n",
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "class FusionRetrieverService(BaseService):\n",
        "    \"\"\"Combines multiple query retrievals using Reciprocal Rank Fusion (RRF)\"\"\"\n",
        "    \n",
        "    def __init__(self, base_retriever: RetrieverService, reformulator: QueryReformulatorService):\n",
        "        super().__init__()\n",
        "        self.base_retriever = base_retriever\n",
        "        self.reformulator = reformulator\n",
        "        self.k = 60  # RRF parameter (typically 60)\n",
        "        self.max_queries = 4  # Maximum number of query variations\n",
        "        \n",
        "    async def retrieve_documents_fusion(self, query: str, top_k: int = 5) -> List[dict]:\n",
        "        \"\"\"Retrieve documents using RAG Fusion with multiple query variations\"\"\"\n",
        "        try:\n",
        "            print(f\"üîÑ RAG Fusion: Generating query variations...\")\n",
        "            \n",
        "            # Step 1: Generate query variations\n",
        "            query_variations = await self.reformulator.reformulate_query(query, self.max_queries - 1)\n",
        "            print(f\"üìù Generated {len(query_variations)} query variations\")\n",
        "            \n",
        "            # Step 2: Retrieve documents for each variation\n",
        "            all_documents = []\n",
        "            for i, variation in enumerate(query_variations):\n",
        "                print(f\"üîç Retrieving for variation {i+1}: '{variation[:50]}...'\")\n",
        "                docs = await self.base_retriever.retrieve_documents(variation, top_k * 2)  # Get more docs for fusion\n",
        "                \n",
        "                # Add query source info\n",
        "                for doc in docs:\n",
        "                    doc['query_source'] = i\n",
        "                    doc['query_variation'] = variation\n",
        "                \n",
        "                all_documents.extend(docs)\n",
        "            \n",
        "            # Step 3: Apply Reciprocal Rank Fusion\n",
        "            print(f\"üîó Fusing {len(all_documents)} documents from {len(query_variations)} queries...\")\n",
        "            fused_docs = await self._apply_rrf(all_documents, top_k)\n",
        "            \n",
        "            # Step 4: Deduplicate and format results\n",
        "            final_docs = await self._deduplicate_and_format(fused_docs, top_k)\n",
        "            \n",
        "            print(f\"‚úÖ RAG Fusion complete: {len(final_docs)} unique documents retrieved\")\n",
        "            return final_docs\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå RAG Fusion failed: {str(e)}\")\n",
        "            # Fallback to single query retrieval\n",
        "            print(\"üîÑ Falling back to single query retrieval...\")\n",
        "            return await self.base_retriever.retrieve_documents(query, top_k)\n",
        "    \n",
        "    async def _apply_rrf(self, documents: List[dict], top_k: int) -> List[dict]:\n",
        "        \"\"\"Apply Reciprocal Rank Fusion to combine document scores\"\"\"\n",
        "        # Group documents by document ID\n",
        "        doc_groups = defaultdict(list)\n",
        "        for doc in documents:\n",
        "            doc_id = doc.get('doc_id', doc.get('metadata', {}).get('document_id', ''))\n",
        "            if doc_id:\n",
        "                doc_groups[doc_id].append(doc)\n",
        "        \n",
        "        # Calculate RRF scores\n",
        "        rrf_scores = {}\n",
        "        for doc_id, doc_list in doc_groups.items():\n",
        "            rrf_score = 0.0\n",
        "            \n",
        "            for doc in doc_list:\n",
        "                rank = doc.get('rank', 1)\n",
        "                rrf_score += 1.0 / (self.k + rank)\n",
        "            \n",
        "            # Store best document with RRF score\n",
        "            best_doc = max(doc_list, key=lambda x: x.get('similarity_score', 0))\n",
        "            best_doc['rrf_score'] = rrf_score\n",
        "            best_doc['appearance_count'] = len(doc_list)\n",
        "            best_doc['query_sources'] = [d.get('query_source', 0) for d in doc_list]\n",
        "            \n",
        "            rrf_scores[doc_id] = best_doc\n",
        "        \n",
        "        # Sort by RRF score\n",
        "        sorted_docs = sorted(rrf_scores.values(), key=lambda x: x['rrf_score'], reverse=True)\n",
        "        return sorted_docs[:top_k * 2]  # Return more for deduplication\n",
        "    \n",
        "    async def _deduplicate_and_format(self, documents: List[dict], top_k: int) -> List[dict]:\n",
        "        \"\"\"Remove duplicates and format final results\"\"\"\n",
        "        seen_content = set()\n",
        "        final_docs = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            # Create content hash for deduplication\n",
        "            content_preview = doc.get('content', '')[:100]\n",
        "            content_hash = hash(content_preview)\n",
        "            \n",
        "            if content_hash not in seen_content:\n",
        "                seen_content.add(content_hash)\n",
        "                \n",
        "                # Format final document\n",
        "                final_doc = {\n",
        "                    'content': doc.get('content', ''),\n",
        "                    'similarity_score': doc.get('rrf_score', doc.get('similarity_score', 0.0)),\n",
        "                    'metadata': doc.get('metadata', {}),\n",
        "                    'rank': len(final_docs) + 1,\n",
        "                    'doc_id': doc.get('doc_id', ''),\n",
        "                    'fusion_metadata': {\n",
        "                        'rrf_score': doc.get('rrf_score', 0.0),\n",
        "                        'appearance_count': doc.get('appearance_count', 1),\n",
        "                        'query_sources': doc.get('query_sources', []),\n",
        "                        'fusion_method': 'RRF'\n",
        "                    }\n",
        "                }\n",
        "                \n",
        "                final_docs.append(final_doc)\n",
        "                \n",
        "                if len(final_docs) >= top_k:\n",
        "                    break\n",
        "        \n",
        "        return final_docs\n",
        "\n",
        "print(\"‚úÖ FusionRetrieverService ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Fusion Configuration\n",
        "FUSION_CONFIG = {\n",
        "    \"enabled\": True,\n",
        "    \"max_query_variations\": 4,\n",
        "    \"rrf_k_parameter\": 60,\n",
        "    \"retrieval_multiplier\": 2,  # Get 2x docs for fusion\n",
        "    \"use_openrouter\": False,  # Set to True if you have API key\n",
        "    \"openrouter_api_key\": None,  # Add your key here\n",
        "    \"fallback_to_single\": True  # Fallback if fusion fails\n",
        "}\n",
        "\n",
        "print(\"‚öôÔ∏è RAG Fusion Configuration:\")\n",
        "for key, value in FUSION_CONFIG.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Update services with configuration\n",
        "if FUSION_CONFIG[\"use_openrouter\"] and FUSION_CONFIG[\"openrouter_api_key\"]:\n",
        "    query_reformulator.openrouter_api_key = FUSION_CONFIG[\"openrouter_api_key\"]\n",
        "    print(\"üîë OpenRouter API configured for query reformulation\")\n",
        "else:\n",
        "    print(\"üìù Using rule-based query reformulation (fallback mode)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Supabase connection and RPC functions\n",
        "print(\"üß™ Testing Supabase connection...\")\n",
        "try:\n",
        "    # Test basic connection\n",
        "    test_result = supabase.table('medical_documents').select('count').execute()\n",
        "    doc_count = len(test_result.data) if test_result.data else 0\n",
        "    print(f\"‚úÖ Supabase connected - Found {doc_count} documents in database\")\n",
        "    \n",
        "    # Test RPC function availability\n",
        "    print(\"üß™ Testing RPC functions...\")\n",
        "    try:\n",
        "        stats_result = supabase.rpc('get_document_stats').execute()\n",
        "        if stats_result.data:\n",
        "            print(\"‚úÖ RPC functions working\")\n",
        "            for stat in stats_result.data[:3]:  # Show first 3 document sources\n",
        "                print(f\"   üìÑ {stat['source']}: {stat['count']} documents\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è RPC function exists but returned no data\")\n",
        "    except Exception as rpc_error:\n",
        "        print(f\"‚ö†Ô∏è RPC function test failed: {str(rpc_error)}\")\n",
        "        print(\"   Vector search will use fallback method\")\n",
        "        \n",
        "except Exception as e:\n",
        "    error_str = str(e)\n",
        "    print(f\"‚ö†Ô∏è Supabase connection test failed: {error_str}\")\n",
        "    \n",
        "    # Provide specific guidance based on error type\n",
        "    if '401' in error_str or 'Invalid API key' in error_str:\n",
        "        print(\"üîß AUTHENTICATION ERROR - Invalid API Key:\")\n",
        "        print(\"   ‚ùå You're using the wrong API key!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Go to your Supabase project dashboard\")\n",
        "        print(\"   2. Settings ‚Üí API\")\n",
        "        print(\"   3. Copy the 'service_role' key (NOT anon key)\")\n",
        "        print(\"   4. The service_role key is much longer and starts with 'eyJ'\")\n",
        "        print(\"   5. Re-run Cell 3 with the correct key\")\n",
        "        print(\"\")\n",
        "        print(\"   üîç Key differences:\")\n",
        "        print(\"   ‚Ä¢ anon key: Used for client-side apps (WRONG for this notebook)\")\n",
        "        print(\"   ‚Ä¢ service_role key: Used for server-side/admin access (CORRECT)\")\n",
        "    elif '404' in error_str:\n",
        "        print(\"üîß TABLE NOT FOUND:\")\n",
        "        print(\"   ‚ùå The 'medical_documents' table doesn't exist!\")\n",
        "        print(\"   üìã To fix this:\")\n",
        "        print(\"   1. Run the schema.sql in your Supabase SQL editor\")\n",
        "        print(\"   2. Or run the embed_documents.py script to create tables\")\n",
        "    elif 'timeout' in error_str.lower():\n",
        "        print(\"üîß CONNECTION TIMEOUT:\")\n",
        "        print(\"   ‚ùå Can't reach Supabase servers\")\n",
        "        print(\"   üìã Check your internet connection and Supabase URL\")\n",
        "    else:\n",
        "        print(\"üîß GENERAL CONNECTION ERROR:\")\n",
        "        print(\"   üìã Common fixes:\")\n",
        "        print(\"   ‚Ä¢ Double-check your Supabase URL\")\n",
        "        print(\"   ‚Ä¢ Verify you're using service_role key (not anon)\")\n",
        "        print(\"   ‚Ä¢ Check if your project is paused/suspended\")\n",
        "        print(\"   ‚Ä¢ Ensure database tables exist\")\n",
        "    \n",
        "    print(\"\\n   ‚ö†Ô∏è The system will continue but may have limited document retrieval\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced FastAPI Setup with Chat History Support\n",
        "app = FastAPI(\n",
        "    title=\"WellnessGrid RAG API\",\n",
        "    description=\"Medical AI Assistant with RAG capabilities using FastAPI\",\n",
        "    version=\"2.0.0\",\n",
        "    docs_url=\"/docs\",\n",
        "    redoc_url=\"/redoc\"\n",
        ")\n",
        "\n",
        "# Add CORS middleware\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Store chat sessions in memory (in production, use Redis or database)\n",
        "chat_sessions = {}\n",
        "\n",
        "@app.post(\"/retrieve-fusion\", response_model=RetrieveResponse)\n",
        "async def retrieve_with_fusion(request: RetrieveRequest):\n",
        "    \"\"\"Retrieve documents using RAG Fusion\"\"\"\n",
        "    try:\n",
        "        print(f\"üîÑ RAG Fusion retrieval for: {request.query[:100]}...\")\n",
        "        \n",
        "        # Use fusion retriever\n",
        "        documents = await fusion_retriever.retrieve_documents_fusion(\n",
        "            request.query, \n",
        "            request.top_k or 5\n",
        "        )\n",
        "        \n",
        "        return RetrieveResponse(\n",
        "            query=request.query,\n",
        "            documents=documents,\n",
        "            total_found=len(documents),\n",
        "            fusion_metadata={\n",
        "                \"method\": \"RRF\",\n",
        "                \"query_variations\": len(set(doc.get('fusion_metadata', {}).get('query_sources', []) for doc in documents)),\n",
        "                \"avg_appearance_count\": sum(doc.get('fusion_metadata', {}).get('appearance_count', 1) for doc in documents) / len(documents) if documents else 0\n",
        "            }\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Fusion retrieval error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Fusion retrieval failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/embed\", response_model=EmbedResponse)\n",
        "async def generate_embedding(request: EmbedRequest):\n",
        "    \"\"\"Generate embeddings for text with enhanced error handling and GPU memory management\"\"\"\n",
        "    try:\n",
        "        global embedding_model, EMBEDDING_DEVICE\n",
        "        \n",
        "        logger.info(f\"üîç Generating embedding for text: {request.text[:100]}...\")\n",
        "        \n",
        "        # Use pre-loaded embedding model or load with CPU fallback\n",
        "        if embedding_model is None:\n",
        "            logger.info(\"üì• Loading embedding model on-demand with CPU fallback...\")\n",
        "            try:\n",
        "                from sentence_transformers import SentenceTransformer\n",
        "                \n",
        "                # Clear GPU cache first to free up memory\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "                    logger.info(\"üßπ GPU cache cleared\")\n",
        "                \n",
        "                # Always use CPU for FastAPI requests to avoid CUDA conflicts\n",
        "                embedding_model = SentenceTransformer('NeuML/pubmedbert-base-embeddings', device='cpu')\n",
        "                EMBEDDING_DEVICE = 'cpu'\n",
        "                logger.info(\"‚úÖ Embedding model loaded on CPU (safer for FastAPI)\")\n",
        "                \n",
        "            except Exception as load_error:\n",
        "                logger.error(f\"‚ùå Failed to load embedding model: {str(load_error)}\")\n",
        "                raise HTTPException(status_code=500, detail=f\"Failed to load embedding model: {str(load_error)}\")\n",
        "        \n",
        "        # Generate embedding\n",
        "        logger.info(f\"üß† Generating embedding on {EMBEDDING_DEVICE}...\")\n",
        "        embedding = embedding_model.encode([request.text])[0].tolist()\n",
        "        \n",
        "        logger.info(f\"‚úÖ Generated embedding with {len(embedding)} dimensions\")\n",
        "        \n",
        "        return EmbedResponse(\n",
        "            embedding=embedding,\n",
        "            dimensions=len(embedding),\n",
        "            model=f\"PubMedBERT ({EMBEDDING_DEVICE})\",\n",
        "            device=EMBEDDING_DEVICE\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in embed endpoint: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "        raise HTTPException(status_code=500, detail=f\"Embedding generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/generate\", response_model=GenerateResponse)\n",
        "async def generate_text(request: GenerateRequest):\n",
        "    \"\"\"Enhanced generate endpoint using modular generator service\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üî¨ Generating response for query: {request.query[:100]}...\")\n",
        "        logger.info(f\"üìö Context length: {len(request.context)} characters\")\n",
        "        logger.info(f\"ÔøΩÔøΩ Chat history: {len(request.history)} messages\")\n",
        "        \n",
        "        # Create enhanced prompt with chat history\n",
        "        if request.history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in request.history[-3:]])\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Current question: {request.query}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{request.context}\n",
        "\n",
        "Question: {request.query}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Generate response using NEW generator service\n",
        "        response = await generator_service.generate_response(prompt, request.max_tokens)\n",
        "        \n",
        "        return GenerateResponse(\n",
        "            answer=response,\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            context_used=len(request.context) > 0,\n",
        "            history_used=len(request.history) > 0\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in generate endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Text generation failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/query\", response_model=QueryResponse)\n",
        "async def query_docs(request: QueryRequest):\n",
        "    \"\"\"Enhanced query endpoint using modular retriever service\"\"\"\n",
        "    try:\n",
        "        # Use NEW retriever service\n",
        "        results = await retriever_service.retrieve_documents(request.query, request.top_k)\n",
        "        \n",
        "        return QueryResponse(\n",
        "            documents=results,\n",
        "            total_found=len(results),\n",
        "            query=request.query\n",
        "        )\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in query endpoint: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Document query failed: {str(e)}\")\n",
        "\n",
        "@app.post(\"/ask\", response_model=AskResponse)\n",
        "async def ask_question(request: AskRequest):\n",
        "    \"\"\"Enhanced RAG pipeline with fusion\"\"\"\n",
        "    try:\n",
        "        print(f\"ü§ñ Processing question: {request.question[:100]}...\")\n",
        "        \n",
        "        # Step 1: Retrieve documents using fusion\n",
        "        if FUSION_CONFIG[\"enabled\"]:\n",
        "            print(\"üîÑ Using RAG Fusion for enhanced retrieval...\")\n",
        "            retrieved_docs = await fusion_retriever.retrieve_documents_fusion(\n",
        "                request.question, \n",
        "                CONFIG['top_k']\n",
        "            )\n",
        "        else:\n",
        "            print(\" Using single query retrieval...\")\n",
        "            retrieved_docs = await base_retriever_service.retrieve_documents(\n",
        "                request.question, \n",
        "                CONFIG['top_k']\n",
        "            )\n",
        "        \n",
        "        print(f\"üìä Found {len(retrieved_docs)} similar documents\")\n",
        "        \n",
        "        # Step 2: Build context from documents\n",
        "        context_parts = []\n",
        "        total_chars = 0\n",
        "        max_context_length = 2000\n",
        "        \n",
        "        for doc in retrieved_docs:\n",
        "            if total_chars + len(doc['content']) <= max_context_length:\n",
        "                context_parts.append(f\"Source: {doc['metadata']['source']}\\n{doc['content']}\")\n",
        "                total_chars += len(doc['content'])\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "        print(f\" Using {len(context_parts)} documents for context ({len(context)} chars)\")\n",
        "        \n",
        "        # Step 3: Create prompt with history\n",
        "        if request.history:\n",
        "            history_context = \"\\n\".join([f\"Human: {h.get('question', '')}\\nAssistant: {h.get('answer', '')}\" for h in request.history[-3:]])\n",
        "            prompt = f\"\"\"Previous conversation:\n",
        "{history_context}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Current question: {request.question}\n",
        "\n",
        "Answer based on the context and conversation history:\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"You are a helpful medical assistant. Use the following context to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {request.question}\n",
        "\n",
        "Answer (based only on the context):\"\"\"\n",
        "        \n",
        "        # Step 4: Generate response\n",
        "        print(f\"ü§ñ Generating response using {generator_service.model_name}...\")\n",
        "        response = await generator_service.generate_response(prompt, 150)\n",
        "        \n",
        "        # Step 5: Update chat history\n",
        "        session_id = request.session_id or \"default\"\n",
        "        if session_id not in chat_sessions:\n",
        "            chat_sessions[session_id] = []\n",
        "        \n",
        "        chat_sessions[session_id].append({\n",
        "            \"question\": request.question,\n",
        "            \"answer\": response,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "        \n",
        "        # Step 6: Format result\n",
        "        result = {\n",
        "            'query': request.question,\n",
        "            'response': response,\n",
        "            'sources': [\n",
        "                {\n",
        "                    'title': doc['metadata'].get('title', 'Medical Document'),\n",
        "                    'source': doc['metadata']['source'],\n",
        "                    'topic': doc['metadata']['topic'],\n",
        "                    'similarity': f\"{doc['similarity_score']:.3f}\",\n",
        "                    'rank': doc['rank'],\n",
        "                    'content_preview': doc['content'][:150] + \"...\"\n",
        "                }\n",
        "                for doc in retrieved_docs\n",
        "            ],\n",
        "            'metadata': {\n",
        "                'documentsUsed': len(context_parts),\n",
        "                'totalFound': len(retrieved_docs),\n",
        "                'contextLength': len(context),\n",
        "                'model': generator_service.model_name,\n",
        "                'processingTime': datetime.now().isoformat(),\n",
        "                'fusionEnabled': FUSION_CONFIG[\"enabled\"],\n",
        "                'sessionId': session_id\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Ask endpoint error: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=f\"Question processing failed: {str(e)}\")\n",
        "\n",
        "@app.get(\"/chat/history/{session_id}\")\n",
        "async def get_chat_history(session_id: str):\n",
        "    \"\"\"Get chat history for a session\"\"\"\n",
        "    try:\n",
        "        history = chat_sessions.get(session_id, [])\n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"history\": history,\n",
        "            \"message_count\": len(history)\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/chat/clear/{session_id}\")\n",
        "async def clear_chat_history(session_id: str):\n",
        "    \"\"\"Clear chat history for a session\"\"\"\n",
        "    try:\n",
        "        if session_id in chat_sessions:\n",
        "            del chat_sessions[session_id]\n",
        "        \n",
        "        return {\n",
        "            \"session_id\": session_id,\n",
        "            \"cleared\": True\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\", response_model=HealthResponse)\n",
        "async def health_check():\n",
        "    \"\"\"Enhanced health check with memory monitoring\"\"\"\n",
        "    try:\n",
        "        # Get memory status\n",
        "        memory_status = await memory_manager.get_memory_status()\n",
        "        \n",
        "        # Test Supabase connection\n",
        "        test_result = supabase.table('medical_documents').select('count').execute()\n",
        "        doc_count = len(test_result.data) if test_result.data else 0\n",
        "        \n",
        "        # Test embeddings\n",
        "        embed_result = supabase.table('document_embeddings').select('count').execute()\n",
        "        embed_count = len(embed_result.data) if embed_result.data else 0\n",
        "        \n",
        "        return HealthResponse(\n",
        "            status=\"healthy\",\n",
        "            model=MODEL_INFO['name'],\n",
        "            model_path=MODEL_INFO['path'],\n",
        "            embedding_device=EMBEDDING_DEVICE if 'EMBEDDING_DEVICE' in globals() else 'unknown',\n",
        "            database=\"Supabase + pgvector\",\n",
        "            documents_in_db=doc_count,\n",
        "            embeddings_in_db=embed_count,\n",
        "            rag_system=\"enhanced_modular\",\n",
        "            chat_support=True,\n",
        "            active_sessions=len(chat_sessions),\n",
        "            gpu_memory=memory_status\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=503, detail=str(e))\n",
        "\n",
        "@app.get(\"/status\")\n",
        "async def status():\n",
        "    \"\"\"Detailed status endpoint\"\"\"\n",
        "    try:\n",
        "        return {\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"models\": {\n",
        "                \"selected_model\": MODEL_INFO['name'] if 'MODEL_INFO' in globals() else \"not_selected\",\n",
        "                \"medical_model\": \"loaded\" if 'medical_model' in globals() else \"not_loaded\",\n",
        "                \"pubmedbert\": \"available\"\n",
        "            },\n",
        "            \"database\": {\n",
        "                \"connected\": True,\n",
        "                \"url\": supabase_url[:30] + \"...\" if supabase_url else \"not_set\"\n",
        "            },\n",
        "            \"config\": CONFIG,\n",
        "            \"memory\": {\n",
        "                \"active_sessions\": len(chat_sessions),\n",
        "                \"session_ids\": list(chat_sessions.keys())\n",
        "            }\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "print(\"üåê Enhanced FastAPI endpoints configured:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings with enhanced error handling\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} + chat history\")\n",
        "print(\"  ‚úÖ POST /ask - Enhanced RAG endpoint with chat history\")\n",
        "print(\"  ‚úÖ GET /health - Enhanced health check\")\n",
        "print(\"  ‚úÖ POST /query - Enhanced document query\")\n",
        "print(\"  ‚úÖ GET /chat/history/{session_id} - Get chat history\")\n",
        "print(\"  ‚úÖ POST /chat/clear/{session_id} - Clear chat history\")\n",
        "print(\"  ‚úÖ GET /status - Detailed status information\")\n",
        "print(\"  ‚úÖ GET /docs - Auto-generated API documentation\")\n",
        "print(\"  ‚úÖ GET /redoc - Alternative API documentation\")\n",
        "print(f\"‚úÖ Enhanced FastAPI server ready with {MODEL_INFO['name']} and chat history support!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test RAG Fusion\n",
        "print(\"ÔøΩÔøΩ Testing RAG Fusion...\")\n",
        "\n",
        "test_queries = [\n",
        "    \"What are the symptoms of diabetes?\",\n",
        "    \"How to treat high blood pressure?\",\n",
        "    \"What causes heart attacks?\",\n",
        "    \"Symptoms of stroke\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\nüîç Testing: {query}\")\n",
        "    \n",
        "    # Test single retrieval\n",
        "    single_docs = await base_retriever_service.retrieve_documents(query, 5)\n",
        "    print(f\"   Single retrieval: {len(single_docs)} docs\")\n",
        "    \n",
        "    # Test fusion retrieval\n",
        "    fusion_docs = await fusion_retriever.retrieve_documents_fusion(query, 5)\n",
        "    print(f\"   Fusion retrieval: {len(fusion_docs)} docs\")\n",
        "    \n",
        "    # Compare results\n",
        "    single_sources = set(doc.get('metadata', {}).get('source', '') for doc in single_docs)\n",
        "    fusion_sources = set(doc.get('metadata', {}).get('source', '') for doc in fusion_docs)\n",
        "    \n",
        "    new_sources = fusion_sources - single_sources\n",
        "    print(f\"   New sources found: {len(new_sources)}\")\n",
        "    \n",
        "    if new_sources:\n",
        "        print(f\"   New sources: {list(new_sources)[:3]}\")\n",
        "\n",
        "print(\"\\n‚úÖ RAG Fusion testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Memory Management Utilities\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Get detailed GPU memory information\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"No GPU available\"\n",
        "    \n",
        "    device = torch.cuda.current_device()\n",
        "    allocated = torch.cuda.memory_allocated(device) / 1024**3\n",
        "    reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "    total = torch.cuda.get_device_properties(device).total_memory / 1024**3\n",
        "    free = total - allocated\n",
        "    \n",
        "    return {\n",
        "        \"allocated\": f\"{allocated:.2f} GB\",\n",
        "        \"reserved\": f\"{reserved:.2f} GB\", \n",
        "        \"total\": f\"{total:.2f} GB\",\n",
        "        \"free\": f\"{free:.2f} GB\",\n",
        "        \"percentage_used\": f\"{(allocated/total)*100:.1f}%\"\n",
        "    }\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"üßπ GPU cache cleared\")\n",
        "    else:\n",
        "        print(\"üì± No GPU to clear\")\n",
        "\n",
        "# Show current GPU memory status\n",
        "print(\"üîç Current GPU Memory Status:\")\n",
        "memory_info = get_gpu_memory_info()\n",
        "if isinstance(memory_info, dict):\n",
        "    for key, value in memory_info.items():\n",
        "        print(f\"   {key}: {value}\")\n",
        "else:\n",
        "    print(f\"   {memory_info}\")\n",
        "\n",
        "# Add GPU memory management to Flask health endpoint\n",
        "print(\"‚úÖ GPU memory utilities ready!\")\n",
        "print(\"   Use get_gpu_memory_info() to check memory\")\n",
        "print(\"   Use clear_gpu_memory() to free cache\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ngrok setup and FastAPI server startup\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Quick pre-check to give immediate feedback\n",
        "print(\"üîç Pre-flight check...\")\n",
        "required_vars = ['medical_model', 'tokenizer', 'supabase', 'rag_controller', 'CONFIG', 'MODEL_INFO']\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"‚ùå Missing required variables: {', '.join(missing_vars)}\")\n",
        "    print(f\"üîß Please run all previous cells (1-7) in order first!\")\n",
        "    print(f\"   Then come back to this cell.\")\n",
        "    raise RuntimeError(f\"Required setup incomplete. Missing: {', '.join(missing_vars)}\")\n",
        "\n",
        "print(\"‚úÖ Pre-flight check passed!\")\n",
        "print(f\"üéØ Selected model: {MODEL_INFO['name']}\")\n",
        "\n",
        "print(\"üîë Using ngrok auth token from Colab secrets...\")\n",
        "\n",
        "# Use the token we already retrieved in Cell 3\n",
        "if 'ngrok_token' not in globals() or not ngrok_token:\n",
        "    print(\"‚ùå Ngrok token not found in secrets!\")\n",
        "    print(\"üîß Make sure you've added NGROK_AUTH_TOKEN to Colab secrets\")\n",
        "    raise ValueError(\"Missing NGROK_AUTH_TOKEN in Colab secrets\")\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "print(\"‚úÖ Ngrok token set successfully!\")\n",
        "\n",
        "# Start ngrok tunnel\n",
        "print(\"üåê Starting ngrok tunnel...\")\n",
        "public_url = ngrok.connect(8000)  # Changed from 5000 to 8000 for FastAPI\n",
        "print(f\"üåç Public URL: {public_url}\")\n",
        "print(\"üìã Copy this URL to your WellnessGrid app configuration!\")\n",
        "\n",
        "# Start FastAPI app with uvicorn\n",
        "print(\"üöÄ Starting FastAPI app with uvicorn...\")\n",
        "print(\"üì° Available endpoints:\")\n",
        "print(\"  ‚úÖ POST /embed - Generate embeddings (required by WellnessGrid)\")\n",
        "print(f\"  ‚úÖ POST /generate - Generate text with {MODEL_INFO['name']} (required by WellnessGrid)\")\n",
        "print(\"  ‚úÖ POST /ask - Main RAG endpoint for WellnessGrid\")\n",
        "print(\"  ‚úÖ GET /health - Health check\")\n",
        "print(\"  ‚úÖ POST /query - Query documents from Supabase\")\n",
        "print(\"  ‚úÖ GET /docs - Interactive API documentation\")\n",
        "print(\"  ‚úÖ GET /redoc - Alternative API documentation\")\n",
        "print(\"\\nüéØ IMPORTANT: Copy the ngrok URL above to your WellnessGrid .env.local:\")\n",
        "print(\"   FAST_API_URL=https://your-ngrok-id.ngrok.io\")\n",
        "print(\"\\n‚ö†Ô∏è  Keep this cell running to maintain the server!\")\n",
        "print(f\"\\nüöÄ Your WellnessGrid RAG system with {MODEL_INFO['name']} is now live!\")\n",
        "\n",
        "# Run the FastAPI app with uvicorn - FIXED for Jupyter\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Run the server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
